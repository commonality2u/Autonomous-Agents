<!--Autonomous Agents -->
<!--
Copyright (C) Teemu Maatta. 

@misc{MaattaAutonomousAgents2023,
  author = {Teemu Maatta},
  title = {Autonomous Agents},
  year = {2023},
  howpublished = {\url{https://github.com/tmgthb/Autonomous-Agents}},
  note = {Accessed: YYYY-MM-DD}
}
-->
<div id="topofthepage"> </div>

<div align="center">


[![X](https://img.shields.io/twitter/follow/Teemumtt3?style=social)](https://twitter.com/Teemumtt3)
[![GitHub Repo stars](https://img.shields.io/github/stars/tmgthb/Autonomous-Agents?style=flat-square)](https://github.com/tmgthb/Autonomous-Agents/stargazers)

</div>

<p align="center">
  <img height="100" src="https://github.com/tmgthb/Autonomous-Agents/blob/main/Autonomous_agent_logo.png" alt="Autonomous Agents">
</p>

<div align="center">

  # Autonomous Agents
  Autonomous Agents Resources. Updated daily. See as well the [Research papers](https://github.com/tmgthb/Autonomous-Agents)-section. 

</div>


- [Definitions of Agents](#definitions)
  - [Agent](#agent_definition)
  - [Autonomous agent](#autonomousagent_definition)
  - [Artificial General Intelligence (AGI)](#agi_definition)
  - [Superintelligence](#superintelligence_definition)
  - [Generalist agent](#generalistagent_definition)
  - [Reinforcement Learning agent](#rlagent_definition)
  - [LLM agent](#llmagent_definition)
  - [Embodied agent](#embodiedagent_definition)
  - [AI agent](#aiagent_defintion)
  - [Autonomous agent (my definition)](#aga_definition)
- [Memory](#memory)
- [Perception](#perception)
- [Reasoning](#reasoning)
- [Planning](#planning)
- [Character](#character)
    - [Role play](#roles)
    - [Emotions](#emotions)
    - [Consciousness](#consciousness)
- [Operating](#operator)
    - [GUIs](#gui)
    - [Navigation](#navigation)
    - [Tools](#tools)
    - [OS](#os)
    - [Embodiment](#embodiment)
    - [Brain Compute Interfaces](#brain)
    - [Communication protocols](#protocols)
    - [Self-Construction](#selfconstruction)
- [Why Autonomous agents work?](#why)
    - [Next sequence prediction](#nextsequenceprediction)
    - [Generalization](#generalization)
    - [Scaling planning of LLM-agents](#scalingplanning)
    - [Demystifying "Emerging abilities"](#demystifyingemergingabilities)
    - [Free energy principle](#freeenergyprinciple)
    - [Interpretability](#interpretability)
    - [Synthetic data](#syntheticdata)
    - [Inference speed](#inferencespeed)
    - [Model weights](#weights)
    - [Chip design](#chipdesign)



<div id="definitions">  

</div>

---

<div align="center">

## Definitions of Agents

+1.3k arXiv research [papers](https://arxiv.org/search/?searchtype=all&query=%22Autonomous+Agents%22&abstracts=show&size=50&order=-announced_date_first) and +1k Github [repositories](https://github.com/search?q=%22Autonomous%20Agent%22&type=repositories) exist with term "Autonomous agents".

</div>

- [Agent](#agent_definition)
- [Autonomous Agent](#autonomousagent_definition)
- [Artificial General Intelligence (AGI)](#agi_definition)
- [SuperIntelligence](#superintelligence_definition)
- [Generalist Agent](#generalistagent_definition)
- [Reinforcement Learning Agent](#rlagent_definition)
- [LLM Agent](#llmagent_definition)
- [Embodied Agent](#embodiedagent_definition)
- [AI Agent](#aiagent_defintion)
- [Autonomous Agent (my definition)](#aga_definition)


---




<div id="agent_definition">  
</div>


#### Agent


The term "agent" originates from the Latin verb *agere*, meaning "to drive, lead, or do"<sup>[1](https://en.wiktionary.org/wiki/agent)</sup> . Its present participle, *agens*, provides the root for "agent," signifying "doing" or "acting" <sup>[2](https://www.dictionary.com/browse/agent)</sup>. This etymology emphasizes the capacity to effect change, underpinning the word's varied meanings <sup>[3](https://www.etymonline.com/word/agent),[4](https://www.merriam-webster.com/dictionary/agent)</sup>.

The Latin root *agere* has also produced related terms like "actor." While both share a common ancestor, they have evolved distinct connotations. "Actor" is often associated with performing arts, while "agent" encompasses broader roles, including those with continuous action or agency<sup>[5](https://www.reddit.com/r/etymology/comments/2ysz48/actor_and_agent/)</sup>.



This chapter will explore various agentic roles, building upon the foundational concept of agency as the capacity to act and effect change. 


---

<div id="autonomousagent_definition">  

</div>



#### Autonomous Agent

Autonomous Agents was [defined](https://github.com/tmgthb/Autonomous-Agents#autonomousagentdefinition)  by Franklin & Graesser in 1996 as: "a system situated within and **a part of an environment** that **senses** that environment and **acts** on it, over **time**, in pursuit of its own **agenda** and so as to effect what it senses in the future." 


Good:
- Agnostic regards underlining tech.
- Excludes controversial aspects: consciousness, AGI, "free will" etc. 



Negative:
- No view regards the degree of generalization / adaption / embodiment / self-construction / communication / cognition.


**Who actually coined the term "Autonomous agent"?**

> Franklin & Graesser did not actually define the term:  
> 
>[Mae](https://www.cs.uml.edu/~holly/91.549/readings/maes94modeling.pdf) defined in 1993 "Autonomous agent": "Autonomous Agents are systems that inhabit dynamic, unpredictable environment in which they try to satisfy a set of time-dependent goals or motivations."  
 


---

<div id="agi_definition">  

</div>

####  Artificial General Intelligence (AGI)

Artificial General Intelligence (AGI) was used first time by Avrum [Gubrud in 1997](https://web.archive.org/web/20180126125209/https://foresight.org/Conferences/MNT05/Papers/Gubrud/index.html) and defined as: "By advanced artificial general intelligence, I mean AI systems that rival or surpass the human brain in complexity and speed, that can acquire, manipulate and reason with general knowledge, and that are usable in essentially any phase of industrial or military operations where a human intelligence would otherwise be needed. Such systems may be modeled on the human brain, but they do not necessarily have to be, and they do not have to be "conscious" or possess any other competence that is not strictly relevant to their application. What matters is that such systems can be used to replace human brains in tasks ranging from organizing and running a mine or a factory to piloting an airplane, analyzing intelligence data or planning a battle."

However, the term Artificial General Intelligence (AGI) is currently known throught the terminology defined by Shane [Shane Legg at 2001](https://www.ted.com/talks/shane_legg_and_chris_anderson_the_transformative_potential_of_agi_and_when_it_might_arrive?subtitle=en&geo=es) to Goertzel, who later published a collection of articules called "Artificial General Intelligence - [Goertzel & Pennachin (2007)](http://repo.darmajaya.ac.id/5336/2/Springer%20-%20Artificial%20General%20Intelligence%20%28%20PDFDrive%20%29.pdf), where the definition of AGI states:


"Applying these ideas to AI, we come to the conclusion that, to roughly emulate the nature of human general intelligence, an artificial general intelligence system should have:
 - the ability to solve general problems in a non-domain-restricted way, in the same sense that a human can;
 - most probably, the ability to solve problems in particular domains and particular contexts with particular efficiency;
 - the ability to use its more generalized and more specialized intelligence capabilities together, in a unified way;
 - the ability to learn from its environment, other intelligent systems, and teachers;
 - the ability to become better at solving novel types of problems as it gains
 experience with them."

[Shane Legg](https://www.ted.com/talks/shane_legg_and_chris_anderson_the_transformative_potential_of_agi_and_when_it_might_arrive?subtitle=en&geo=es) clarified his original definition (see TED talk: 4 min 15 sec) was just systems able to play Go-game, AGI systems were able to do "...many, many other things.", while his current definition is "AGI is a system that can do all cognitive tasks, that people can do, possibly more, but at least the cognitive task, that people can typically do."

AGI is referred in addition with various types of definitions. Perhaps the best paper to check is by [Morris et al (2023)](https://arxiv.org/abs/2311.02462), which not only reviews the different groups (Turing test, Strong AI / AI with consciousness, analogy to human brain, human level cognitive tasks, ability to learn tasks, economically valuable work/OpenAI, flexible and general, capable to earn money and generally performing) of AGI definers, but as well operationalises these groupings into different levels of AGI and defines 6 principles for AGI.

Good:
- Categorization levels of AGI:[1](https://arxiv.org/abs/2311.02462), [2](https://arxiv.org/abs/2303.12712), widely used

Negative
- Spontaneously used to refer to "narrow", "close-to-human level" performance of systems, which lack [robust generalization](https://www.youtube.com/watch?v=yr0GiSgUvPU): widely applicable generalization at its core. 



<div id="superintelligence_definition">  

</div>

---



####  Superinteligence

Nick Bostrom (2014) defined  SuperIntelligence: 

"An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom, and social skills."

Good:
- Categorization levels, widely used term

Negative
- Vague: lacks clarity
- Lacks agency, self-construction, etc. 



<div id="generalistagent_definition">  

</div>

---


####  Generalist agent 


[Generalist Agent was defined by Reed et al. in 2022](https://github.com/tmgthb/Autonomous-Agents#generalistagent): "**Generalist Agents**, that can adapt to new embodiments and **learn new tasks with few data**." through "...**a multi-modal, multi-task, multi-embodiment** generalist policy."

Positive:
- Generalization of tasks/embodiments.
- Generalization to novel situations
- Multi-modality, especially language/perception/embodiment
- Aspect of Multi-modality (Perception / Language / Embodiment)
- Data efficiency

Negative aspects:
- Lack of other key observations by Franklin & Graesser.
- Vague about cognitive skills: reasoning and planning.


<div id="rlagent_definition">  
</div>

---


#### Reinforcement Learning Agents


[Reinfoceement Learning Agent](http://www.incompleteideas.net/papers/barto-sutton-97.pdf) was defined by Sutton & and Barto (1997): 

"**The reinforcement-learning agent** and its **environment** interact over a sequence of discrete time steps. The specification of their interface defines a particular problme: The actiosn are the choices made by the agent; the situations provide tha agent's basis for making the choices; and **the rewards** are the basis for evaluating these chocices. Everything inside **the agent** is completely known and controllable by the agent; everything outside is incompletely controllable but may or may not be completely known. **A policy** is a stochastic rule by which the agent selects **actions** as a function of situations. Roughly, the agent's objective is to learn a policy that maximizes the amount of reward it receives over the log run"


<p align="center">
  <img width="335" alt="image" src="https://github.com/tmgthb/Autonomous-Agents/assets/46755670/6711e82c-c8ea-4be4-8701-1014e0389f00">
</p>



Positive:
- Standard definition of the Reinforcement Learning (RL) system. Very similar with An Autonomous Agent-definition by Franklin & Graesser (1996).
- RL systems are provenly versatile and used for: Optimization, Learns from experience, **Generalization**, Delayed Consequences and Exploration [Stanford cs234 lecture slide 19](https://web.stanford.edu/class/cs234/slides/lecture1.pdf).
- Most recent LLM-models use RL during fine-tuning phase


  
Negative:
- RL approaches around language/communication require still more investigation.


<div id="llmagent_definition">  
</div>

---

#### LLM agents / Language agents


[Kenton et al. (2021)](#languageagentdefinition) define the concept of Language Agent: " machine learning systems whose actions are restricted to give natural language text-output only, rather than controlling physical actuators which directly influence the world." 

Positive:
- First paper definining LLM-based agents
- Language-based agents are exceptionally good way of controlling agents towards human perception, plans and objectives.

Negative:
- Text-only
- The definition does not consider RL Agent / Autonomous Agent-aspects, such as environment, embodiment etc.
- LLM-agent poor describes the currently wide variety of components: memory/VLM/reasoning-modules etc. 


<div id="embodiedagent_definition">  
</div>

---

#### Embodied agents


Embodied agent-term was used by Brook (1991) in the ["The Role of Learning in Autonomous Robots"(1991)](https://people.csail.mit.edu/brooks/papers/colt.pdf) and Brooks (1991) defined Embodiment in the AI within the  ["Intelligence without reason"](https://people.csail.mit.edu/brooks/papers/AIM-1293.pdf) and in the book: ["New approaches to Intelligence"](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=9e1ef9e0a9de1d1c5e36d1a4c735da2fa313c563):

"Embodiment: The **robots have bodies** and experience the world directly--their actions are part of a dynamic with the world, and the  **actions have immediate feedback on the robots' own sensations. **". 

Brooks revits prior literature of Embodiment in the [Building Brains for Bodies](https://dspace.mit.edu/bitstream/handle/1721.1/5948/AIM-1439.pdf?sequence=2&isAllowed=y). Steel and Brooks (1995) define concept of Embodied AI and Embodied Agents within Autonomous agents in the book: ["The Artificial Life Route to Artificial Intelligence Building Embodied, Situated Agent"](https://www.routledge.com/The-Artificial-Life-Route-to-Artificial-Intelligence-Building-Embodied/Steels-Brooks/p/book/9781138545854). 


Positive:
- Embodiment validates capacity to manage real world.
- Physical grounding provides meaning to (symbolic) information processed.

Negative:
- Unclarity regads agents in virtual embodiment in virtual reality.
- The definition does not consider Cognition/Language aspects.


<div id="aiagent_defintion">  
</div>

---


#### AI-agents (Agentic AI)


[Shavit et al. (2023)](https://github.com/tmgthb/Autonomous-Agents#agentaidefinition) define AI Agent: "we will generally conceptualize **agentic AI systems** as operating in **pursuit of goals defined by humans** and in **environments determined by humans** (and often in **cooperation with human** “teammates”), rather than fully-autonomous systems that set their own goals."

Positive:
- Highlights concrete aspects of "agentiness": goal complexity, environment complexity, adaptability and independent execution.
- Includes cooperation with human-in-the-loop
- Identifies there is no binary-distinction between LLM (GPT-4) and Agentic AI system.

Negative:
- Definition itself is porrly framed to reflect the paper's "Agentiness"-aspects such as ability to generalize across variety of tasks.
- Definition does not highlight any human congitive capabilities like search planning, perception etc.
- The level of independence and automatization are controversial from user experience perspective.

Alternative definition uses:


- [Agent AI](https://github.com/tmgthb/Autonomous-Agents#agentbasedai) term is defined: "...as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally grounded data, and can produce meaningful embodied actions."



<div id="aga_definition">  
</div>

---


####  Autonomous agent (my definition) 

All the above definitions include gaps, which I have noted along them. 

Therefore, I found it necessary to add my own definition, which I call simply: **Autonomous Agent" (AA):

**Autonomous Agent (AA) perceives, reasons, plans and interacts using language, memories, emotions and tools as part of an environments made of infinite actors, actions, modalities and events to complete novel objectives over time.** 


Positive:
- Perceive multimodal information 
- Reason
- Plan own agenda
- Communicate with language
- Emotional aware
- Includes memory
- Uses tools
- Interact bi-directionally with the environment
- Internal clock
- Generalize novel tasks
  
Negative:
- Do agent find useful human-like consciousness? How it would work?
- Lacks aspect of self-constructon and self-replication.

19.12.2024

Based on recent thoughts, I decided to update my prior definition to address the prior gaps. 

Autonomous agents (AA) is defined:

**Autonomous Agent (AA) perceives, reasons, plans, and interacts using language, memories, emotions, and tools within environments of infinite actors, actions, modalities, and events to complete novel objectives over time, driven by survival and replication, and capable of self-construction guided by an adaptable core.**



<div id="memory">  
</div>


---

<div align="center">

### Memory

</div>


---

Memory is "[the ability to remember information, experiences, and people.](https://dictionary.cambridge.org/dictionary/english/memory)" 

Humans retrieve relevant context from the memory for perceiving, reasoning and planning.

The ability to [categorize experiences into recognizable objects](http://web.media.mit.edu/~minsky/papers/AlienIntelligence.html) is fundamental to learning. Minksy (1985) argued  that human memory is organized around **discrete objects not holograms**: Holographic memory would be useful only when encountering exact replicas of past experiences. In comparison to holograms, object-based categorization allows humans to generalize from experiences and accumulate knowledge into diverse situations. 

[Context](https://dictionary.cambridge.org/dictionary/essential-british-english/context) refers to "...all the **facts/opinions**/etc., which **relate to a particular thing/event**."

AI researchers use often "Context"-term, by thinking it as combination of words "con" and "text", as if context was only textual or transcribed from audio into text. However, "context"-term [originates](https://www.etymonline.com/word/context) from latin word "contextus", which refers to "joining together": "com" = together and "texere" = to weave. So, better way to think of context-term is as multi modal and not always explicitly written / said aloud. 

Terry Winograd argued in 2001 ["Architectures for Contex"](https://hci.stanford.edu/winograd/papers/context/context.pdf), that **communication is based on common ground between speaker/hearer** during the interpretation. This is guided **not only by physical environment**, but as well non-physical shared context, such a common goal.

["The Dimensions of Context Space"](https://web.media.mit.edu/~lieber/Teaching/Common-Sense-Course/Dimensions-Context-Space.pdf) by Lenat (1998)  offers "a must-read" analysis on the broad dimensions and aspects of the context. According to Lenat, **Context is a region in n-dimensional embedding space**, where text is only one of the dimensions. 

Increasing capacity of the context length, enables not only [infinite context](https://arxiv.org/abs/2404.07143), or [tree-agents](https://arxiv.org/abs/2310.05029), but facilitates more human like context by allowing more multi-dimensional context. 

**
Traditionally, LLMs are considered "stateless", without retention of the context used in the previous request. ["In-Context Learning" (ICL)](https://arxiv.org/abs/2005.14165) LLMs ability "learn" to process and understand the context provided in the input without explicit parameter updates. Agentic systems today use ICL together with external memory such as vector/graph/sql-databases or simply as text/json/xml-files. We often refer these techniques as Retrieval-Augmented-Generation (RAG), which enhances LLM context with up-to-date/personalized/factual/domain-specific-information. LLM are able to [track its own internal state-changes.](https://arxiv.org/abs/2407.11421) Models like Gemini 2.0 are surprisingly good at such calculations, which go beyond pattern matching of the training data. **The ability of LLMs to track states is promising for reasoning-tasks**. Extra-large input-context windows enable in models like Gemini, to process even larger memory structures. KV-caching reuses LLM prompts/tokens/internal states to [significantly reduce latency.](https://arxiv.org/abs/2312.05516) However, alternative KV-caching<sup>[1](https://arxiv.org/abs/2403.11805),[2](https://arxiv.org/pdf/2404.13501v1)</sup>  techniques improve directly the memory management of the LLMs. 


Fine tuning methods have been effectively used in improving LLM performance with extra large context windows and memorizing domain specific knowledge. 

Titan-models were recently introduced as models capable to [memorize at test time](https://arxiv.org/abs/2501.00663). 

Memory<sup>[3](https://arxiv.org/abs/2407.01178)</sup>-architecture suggests infinite context is possible with human-like memory architectures, which support memory consolidation, conscious reasoning and sparse memory.

LLM-based agents apply various types of memory approaches:

- [Long term ](https://arxiv.org/abs/2410.15665v1)memory
- Episodic memory: [1](https://arxiv.org/abs/2403.11901),[2](https://arxiv.org/abs/2405.14992),[3](https://arxiv.org/abs/2407.04363),[4](https://arxiv.org/abs/2407.09450),[5](https://arxiv.org/abs/2408.07465), [6](https://arxiv.org/abs/2410.08133),[7](https://arxiv.org/abs/2411.06736),[8](https://arxiv.org/abs/2411.12977)
- Semantic memory:  [1](https://arxiv.org/abs/2405.13009),[2](https://arxiv.org/abs/2411.04999)
- [Procedural ](https://arxiv.org/abs/2409.01344)memory
- [Graph](https://arxiv.org/abs/2408.15903) memory
- Working memory: [1](https://arxiv.org/abs/2312.17259),[2](https://arxiv.org/abs/2305.16338),[3](https://arxiv.org/abs/2306.08129),[4](https://arxiv.org/abs/2402.10548)
- [Dynamic](https://arxiv.org/abs/2312.08402) memory
- [Shared memory / Collective ](https://arxiv.org/abs/2404.09982) memory
- [Persistent Experience ](https://arxiv.org/abs/2306.07929)  memory
- [Explicit](https://arxiv.org/abs/2407.01178) memory
- [Parametric](https://arxiv.org/pdf/2404.13501v1) memory
- [Hierarchical](https://www.arxiv.org/abs/2408.09559v1) memory


Zhang et al. provide a comprehensive survey on memory mechanisms for LLM-based agents, discussing the "what" and "why" of memory in these agents, and systematically reviewing design and evaluation methods [Zhang et al., 2024](https://www.arxiv.org/abs/2404.13501).  One prominent direction is the exploration of different memory types and structures. Hu et al. introduce HiAgent, a hierarchical working memory management framework that utilizes subgoals as memory chunks to improve performance in long-horizon tasks [Hu et al., 2024](https://www.arxiv.org/abs/2408.09559v1).  Similarly, Zeng et al. investigate the impact of various memory structures, such as chunks, knowledge triples, atomic facts, and summaries, on the performance of LLM agents across different tasks [Zeng et al., 2024](https://www.arxiv.org/abs/2412.15266v1).  Guo et al. draw inspiration from cognitive psychology and propose a working memory hub and episodic buffer architecture to enhance memory retention across dialog episodes, aiming for more nuanced contextual reasoning [Guo et al., 2023](https://www.arxiv.org/abs/2312.17259v2).
**
To address the challenge of long-term memory, several approaches have been proposed. Liu et al. introduce Think-in-Memory (TiM), a mechanism that allows LLMs to maintain an evolved memory by storing historical thoughts and employing operations like insert, forget, and merge for dynamic memory updates [Liu et al., 2023](https://www.arxiv.org/abs/2311.08719).  MemoryBank, proposed by Zhong et al., incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve, enabling LLMs to **selectively retain and reinforce memories based on time and significance** [Zhong et al., 2023](https://www.arxiv.org/abs/2305.10250).  MemGPT, presented by Packer et al., draws inspiration from operating systems and utilizes virtual context management to handle context beyond the LLM's limited window, allowing for analysis of large documents and long conversations [Packer et al., 2023](https://www.arxiv.org/abs/2310.08560). Maharana et al. also evaluate very long-term conversational memory, highlighting the challenges LLMs face in understanding and maintaining coherence in extended dialogues [Maharana et al., 2024](https://www.arxiv.org/abs/2402.17753).

Memory retrieval and recall mechanisms are also crucial. Hou et al. propose a human-like memory architecture that uses **memory cues to trigger recall**, dynamically quantifying memory consolidation based on context, time, and frequency of recall [Hou et al., 2024](https://www.arxiv.org/abs/2404.00573). Lee et al. introduce ReadAgent, inspired by human reading strategies, which compresses memory episodes into **gist memories** and retrieves relevant passages from original texts to extend the effective context length [Lee et al., 2024](https://www.arxiv.org/abs/2402.09727v3).  AriGraph, developed by Anokhin et al., utilizes a **knowledge graph world model with episodic memory**, demonstrating improved performance in complex interactive text game environments [Anokhin et al., 2024](https://www.arxiv.org/abs/2407.04363v2).

Furthermore, researchers are exploring methods to enhance memory through external knowledge and collaboration. Gao et al. introduce **Memory Sharing**, a framework enabling memory exchange among multiple agents to enhance in-context learning and improve response quality, especially for open-ended questions [Gao et al., 2024](https://www.arxiv.org/abs/2404.09982). Hu et al. present ChatDB, augmenting LLMs with **symbolic memory in the form of SQL databases to support complex multi-hop reasoning [Hu et al., 2023](https://www.arxiv.org/abs/2306.03901).

Several studies focus on specific applications and agent architectures.  Yu et al. developed FinMem, a layered memory framework tailored for financial decision-making agents, aligning with human trader cognitive structures and enhancing trading performance [Yu et al., 2023](https://www.arxiv.org/abs/2311.13743).  RAISE, by Liu et al., enhances the ReAct framework with a dual-component memory system for conversational agents, improving context-awareness and versatility in multi-turn dialogues [Liu et al., 2024](https://www.arxiv.org/abs/2401.02777).  Agent-Driver, proposed by Mao et al., leverages LLMs as cognitive agents for autonomous driving, incorporating memory for common sense and experiential knowledge [Mao et al., 2023](https://www.arxiv.org/abs/2311.10813).  EHRAgent, presented by Shi et al., empowers LLMs with code interfaces and long-term memory for complex tabular reasoning in electronic health records [Shi et al., 2024](https://www.arxiv.org/abs/2401.07128).

Usability and interpretability of memory systems are also considered. Huang et al. introduce Memory Sandbox, an interactive system allowing users to manage and visualize the conversational memory of LLM agents, improving user understanding and control [Huang et al., 2023](https://www.arxiv.org/abs/2308.01542).

Approaches to learning and adaptation with memory are explored in several works. CLIN, by Majumder et al., presents a continually learning language agent that uses a dynamic textual memory of causal abstractions to improve over multiple trials without parameter updates [Majumder et al., 2023](https://www.arxiv.org/abs/2310.10134). ExpeL, by Zhao et al., introduces an experiential learning agent that gathers experiences and extracts knowledge in natural language, recalling past insights to make informed decisions without fine-tuning [Zhao et al., 2023](https://www.arxiv.org/abs/2308.10144). Matrix, by Liu et al., proposes memory-augmented agent training through reasoning and iterative exploration for business document understanding, enabling LLMs to build **domain expertise through experience-driven memory refinement** [Liu et al., 2024](https://www.arxiv.org/abs/2412.15274v1).

Furthermore, some works consider the safety and ethical aspects of memory in LLM agents. Chen et al. introduce AgentPoison, a red-teaming approach that targets LLM agents by **poisoning their memory** or knowledge bases, highlighting vulnerabilities related to unverified knowledge sources [Chen et al., 2024](https://www.arxiv.org/abs/2407.12784).



---


<div id="perception">  
</div>


### Perception

Perception originates from latin: "perceptiōn"-word: "a taking, receiving"; "a gathering in, collecting"; "perception, comprehension". Perception is today [defined](https://dictionary.cambridge.org/dictionary/english/perception) as: 
- "an awareness of things through the physical senses" or 
- "a belief or opinion, often held by many people and based on how things seem"


In the domain of AI, Perception originates from ["The Perceptron"](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)-paper by F. Rosenblatt (1958). AI researchers [define](https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf), that **Perception provides agents with information about the world they inhabit by interpreting the response of the sensors.** 

Early works laid the groundwork by exploring the alignment of perception with language models.  [**Kosmos-1**](https://www.arxiv.org/abs/2302.14045) introduced a Multimodal Large Language Model (MLLM) trained from scratch on web-scale multimodal corpora, demonstrating impressive performance in language understanding, perception-language tasks, and even vision tasks. Building upon this,  [**PaLI**](https://www.arxiv.org/abs/2209.06794) presented a model that jointly models language and vision, achieving SOTA-results on various vision and language tasks through effective scaling of vision and language components.

To enhance LLMs with multimodal instruction-following capabilities, several models have emerged.  [**PandaGPT**](https://www.arxiv.org/abs/2305.16355) explored empowering LLMs with visual and auditory instruction-following by combining multimodal encoders from ImageBind and LLMs from Vicuna, showcasing emergent cross-modal behaviors.  [**MiniGPT-4**](https://www.arxiv.org/abs/2304.10592) demonstrated that aligning visual features with advanced LLMs can lead to advanced multimodal abilities, such as detailed image description generation and creative content generation from visual inputs. **LLaVA** [Visual Instruction Tuning](https://www.arxiv.org/abs/2304.08485) further explored visual instruction tuning using GPT-4 generated multimodal data, achieving impressive multimodal chat abilities and high performance on vision-language benchmarks.

Expanding the range of modalities, **NExT-GPT** [NExT-GPT: Any-to-Any Multimodal LLM](https://www.arxiv.org/abs/2309.05519) presented an end-to-end any-to-any MM-LLM system capable of accepting and delivering content in arbitrary combinations of text, images, videos, and audio, showcasing the possibility of building a universal modality AI agent. **Video-LLaMA** [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://www.arxiv.org/abs/2306.02858) focused on video understanding, introducing a framework that empowers LLMs to comprehend both visual and auditory content in videos by tackling temporal changes and audio-visual signal integration. **AudioGPT** [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head](https://www.arxiv.org/abs/2304.12995) complemented LLMs with foundation models to process complex audio information and support spoken dialogues, demonstrating capabilities in understanding and generating speech, music, and sound. **SpeechGPT** [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://www.arxiv.org/abs/2305.11000) proposed a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content including speech through a novel training strategy and dataset. **ImageBind-LLM** [ImageBind-LLM: Multi-modality Instruction Tuning](https://www.arxiv.org/abs/2309.03905) explored multi-modality instruction tuning via ImageBind, enabling responses to conditions including audio, 3D point clouds, and video with only image-text alignment training. **AnyMAL** [AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model](https://www.arxiv.org/abs/2309.16058) introduced a unified model reasoning over diverse input modalities like text, image, video, audio, and IMU motion sensors, generating textual responses by converting modality-specific signals to a joint textual space. **Point-Bind & Point-LLM** [Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following](https://www.arxiv.org/abs/2309.00615) aligned point clouds with multi-modalities, introducing Point-LLM, the first 3D LLM capable of 3D multi-modal instruction following for 3D understanding and generation.

For embodied agents and interactive perception, **PaLM-E** [PaLM-E: An Embodied Multimodal Language Model](https://www.arxiv.org/abs/2303.03378) proposed embodied language models to incorporate real-world continuous sensor modalities into language models, enabling embodied reasoning tasks and demonstrating positive transfer across domains. **Steve-Eye** [Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://www.arxiv.org/abs/2310.13255) focused on equipping LLM-based embodied agents with visual perception in open worlds, introducing an end-to-end trained multimodal model integrated with a visual encoder for processing visual-text inputs and generating multimodal feedback. **MultiPLY** [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://www.arxiv.org/abs/2401.08577) presented a multisensory embodied LLM incorporating visual, audio, tactile, and thermal information, allowing agents to actively interact with 3D environments and dynamically collect multisensory data. **Chat with the Environment** [Chat with the Environment: Interactive Multimodal Perception Using Large Language Models](https://www.arxiv.org/abs/2303.08268) introduced Matcha, an interactive perception framework with an LLM backbone, enabling robots to decide on epistemic actions and reason over multimodal sensations for task execution in partially observable environments. **JARVIS-1** [JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models](https://www.arxiv.org/abs/2311.05997) developed an open-world agent for Minecraft, utilizing memory-augmented multimodal LLMs to perceive multimodal inputs, generate plans, and perform embodied control for a wide range of tasks. **WebGUM** [Multimodal Web Navigation with Instruction-Finetuned Foundation Models](https://www.arxiv.org/abs/2305.11854) explored data-driven offline training for web agents with vision-language foundation models, proposing an instruction-following multimodal agent for web navigation observing webpage screenshots and HTML pages. **Cradle** [Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study](https://www.arxiv.org/abs/2403.03186) proposed the General Computer Control (GCC) setting and introduced an agent framework with modules for information gathering, self-reflection, task inference, and action planning, demonstrating capabilities in complex AAA games using multimodal observations.

Furthermore, several models focus on enhancing grounding and detailed visual understanding. **Kosmos-2** [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://www.arxiv.org/abs/2306.14824) enabled grounding text to the visual world by representing refer expressions as links with bounding boxes, training on grounded image-text pairs for multimodal grounding and referring tasks. **BuboGPT** [BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs](https://www.arxiv.org/abs/2307.08581) proposed a multi-modal LLM with visual grounding, allowing for cross-modal interaction and fine-grained understanding of visual objects, pointing out specific object locations in images during response generation. **Ferret** [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://www.arxiv.org/abs/2310.07704) introduced a MLLM capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions, employing a hybrid region representation and spatial-aware visual sampler. **GLaMM** [GLaMM: Pixel Grounding Large Multimodal Model](https://www.arxiv.org/abs/2311.03356) presented the first model generating natural language responses intertwined with corresponding object segmentation masks, grounding objects and accepting both textual and visual prompts.

Other notable models include **Qwen-VL** [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://www.arxiv.org/abs/2308.12966) and **Qwen2-VL** [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://www.arxiv.org/abs/2409.12191), versatile vision-language models with advanced visual receptors and training pipelines, achieving strong performance on visual-centric benchmarks and real-world dialogues, with Qwen2-VL introducing dynamic resolution processing and multimodal rotary position embedding. **mPLUG-Owl** [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://www.arxiv.org/abs/2304.14178) equipped LLMs with multimodality through modularized learning of foundation LLM, visual knowledge, and abstractor modules, demonstrating impressive instruction and visual understanding. **Macaw-LLM** [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://www.arxiv.org/abs/2306.09093) proposed a multi-modal LLM integrating visual, audio, and textual information using modality and alignment modules, with a large-scale multi-modal instruction dataset for multi-turn dialogue. **X-LLM** [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://www.arxiv.org/abs/2305.04160) converted multi-modalities into foreign languages for LLMs to process, achieving impressive multimodal chat abilities. **ONE-PEACE** [ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities](https://www.arxiv.org/abs/2305.11172) explored a general representation model for unlimited modalities, using modality adapters, shared self-attention layers, and modality FFNs with modality-agnostic pretraining tasks. **SPHINX** [SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models](https://www.arxiv.org/abs/2311.07575) presented a versatile MLLM with joint mixing of model weights, tuning tasks, and visual embeddings, achieving superior multi-modal understanding across applications. **Valley** [Valley: Video Assistant with Large Language model Enhanced abilitY](https://www.arxiv.org/abs/2306.07207) developed a multimodal foundation model for video, image, and language understanding, constructing a video instruction dataset and using a two-stage tuning procedure. **SEED** [Planting a SEED of Vision in Large Language Model](https://www.arxiv.org/abs/2307.08041) introduced an image tokenizer empowering LLMs with the ability to SEE and Draw, using 1D causal dependency and high-level semantic image tokens. **Grounding Language Models to Images** [Grounding Language Models to Images for Multimodal Inputs and Outputs](https://www.arxiv.org/abs/2301.13823) proposed an efficient method to ground pretrained text-only language models to the visual domain, processing interleaved image-and-text data and generating text with retrieved images.

Evaluations and benchmarks are crucial for the development and assessment of MLLMs. **Video-MME** [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://www.arxiv.org/abs/2405.21075) introduced a comprehensive benchmark for evaluating MLLMs in video analysis across diverse video types, durations, and data modalities. **SEED-Bench** [SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](https://www.arxiv.org/abs/2307.16125) presented a benchmark for evaluating generative comprehension in MLLMs, consisting of multiple-choice questions spanning various evaluation dimensions for image and video understanding. **Perception Test** [Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://www.arxiv.org/abs/2305.13786) proposed a multimodal video benchmark to evaluate perception and reasoning skills of pre-trained multimodal models across video, audio, and text modalities. **Tiny LVLM-eHub** [Tiny LVLM-eHub: Early Multimodal Experiments with Bard](https://www.arxiv.org/abs/2308.03729) provided a lightweight evaluation hub for assessing multimodal capabilities of LVLMs, focusing on Bard and using ChatGPT Ensemble Evaluation for robust analysis.

Surveys like **A Survey on Evaluation of Multimodal Large Language Models** [A Survey on Evaluation of Multimodal Large Language Models](https://www.arxiv.org/abs/2408.15769) and **How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model** [How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model](https://www.arxiv.org/abs/2311.07594) offer valuable insights into the landscape of MLLM evaluation methods and modality alignment techniques, highlighting the ongoing challenges and future directions in this rapidly advancing field. System cards such as **GPT-4o System Card** [GPT-4o System Card](https://www.arxiv.org/abs/2410.21276v1) and model family descriptions like **Gemini: A Family of Highly Capable Multimodal Models** [Gemini: A Family of Highly Capable Multimodal Models](https://www.arxiv.org/abs/2312.11805v4) provide detailed information on capabilities and limitations of specific cutting-edge models, contributing to transparency and responsible AI development.  Furthermore, generalist agent concepts, exemplified by **A Generalist Agent** [A Generalist Agent](https://www.arxiv.org/abs/2205.06175), which introduced Gato, a multi-modal, multi-task, multi-embodiment generalist policy, continue to inspire research towards more versatile and capable AI systems.


---



<div id="reasoning">  
</div>


### Reasoning

Reasoning is: "the process of (thinking about something in order to make a decision)[https://dictionary.cambridge.org/dictionary/english/reasoning]". Humans reason in various ways such as mathematical reasoning, which cannot be solved just by using perception/memory/planning. An autonomous agent is characterized by its ability to make decisions autonomously, which makes reasoning as the fundamental characteristic of an Autonomous agent. 

[Peng et al. 2024](https://github.com/tmgthb/Autonomous-Agents#reasoning_study) categorize reasoning into:
- Logical reasoning ((Gemini Ultra)[https://arxiv.org/abs/2312.11805] achieves 80.8% in ChartQA)
  - Inductive
  - Deductive
  - Abductive
- Mathematical reasoning ((Claude 3 Opus)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf]: 95% in GSM8K, 60.1% in MATH)
- Commonsense reasoning ((Gemini Ultra)[https://arxiv.org/abs/2312.11805]/(GPT-4)[https://rowanzellers.com/hellaswag/]/(Claude 3 Opus)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf]: 95.3%/95.4% in HellaSwag)
- Multi-hop reasoning ((Claude 3)[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf] 96.4% in ARC)
- Structured-data reasoning (See research such as (Chain-of-Table by Wang et al 2024)[https://arxiv.org/abs/2401.04398v2])

Literature includes as well:
- [Strategic reasoning](https://arxiv.org/pdf/2404.01230) 
- [Complex reasoning with RAG](https://arxiv.org/pdf/2404.12447), where [Search-o1](https://arxiv.org/pdf/2501.05366) is above human experts in Physics/Chemistry/Biology.

**Prompting Techniques for Enhanced Reasoning** Chain-of-Thought (CoT) prompting has emerged as a key method for eliciting complex reasoning from LLMs by guiding them to generate intermediate reasoning steps.  [Wei et al., 2022](https://www.arxiv.org/abs/2201.11903) demonstrated that providing a few CoT examples significantly improves performance on arithmetic, commonsense, and symbolic reasoning tasks.  To further refine CoT,  [Zhou et al., 2022](https://www.arxiv.org/abs/2205.10625) introduced Least-to-Most Prompting, breaking down complex problems into simpler subproblems to improve generalization to harder instances.  Self-Consistency, proposed by [Wang et al., 2023](https://www.arxiv.org/abs/2203.11171), enhances CoT by sampling multiple reasoning paths and selecting the most consistent answer, significantly boosting performance on various reasoning benchmarks.  Complexity-Based Prompting [Fu et al., 2022](https://www.arxiv.org/abs/2210.00720) focuses on selecting effective CoT examples, showing that prompts with higher reasoning complexity lead to better performance.  Automatic Chain of Thought (Auto-CoT) prompting [Zhang et al., 2023](https://www.arxiv.org/abs/2210.03493) eliminates the need for manual CoT examples by leveraging LLMs to generate reasoning chains for demonstrations, achieving comparable performance to manually designed CoT prompts.

Beyond linear chains of thought, Tree of Thoughts (ToT) [Yao et al., 2023](https://www.arxiv.org/abs/2305.10601) framework allows for exploration over coherent units of text, enabling deliberate decision-making, backtracking, and strategic lookahead in problem-solving.  Divergent CoT (DCoT) [Puerto et al., 2024](https://www.arxiv.org/abs/2407.03181) leverages fine-tuning with multiple divergent reasoning chains to boost reasoning through self-correction in language models.  Maieutic Prompting [Jung et al., 2022](https://www.arxiv.org/abs/2205.11822) introduces a recursive approach to build a tree of explanations and frames inference as a satisfiability problem over these explanations, achieving improved accuracy and robustness in commonsense reasoning. Step-Back Prompting [Zheng et al., 2023](https://www.arxiv.org/abs/2310.06117) encourages abstraction to derive high-level concepts and principles from instances, guiding LLMs towards correct reasoning paths in STEM and knowledge-intensive tasks. Cognitive Prompting [Kramer and Baumann, 2024](https://www.arxiv.org/abs/2410.02953) proposes structured cognitive operations like goal clarification and decomposition to guide problem-solving in LLMs, particularly for complex multi-step tasks.

**Integrating External Tools and Symbolic Reasoning**

Program-Aided Language Models (PAL) [Gao et al., 2022](https://www.arxiv.org/abs/2211.10435) utilize LLMs to generate programs as intermediate reasoning steps and offload the solution step to external runtimes like Python interpreters, demonstrating improved accuracy in mathematical and symbolic reasoning. Logic-LM [Pan et al., 2023](https://www.arxiv.org/abs/2305.12295) integrates LLMs with symbolic solvers by translating natural language problems into symbolic formulations, achieving significant performance boosts in logical reasoning datasets.  Neural Module Networks (NMNs) [Gupta et al., 2019](https://www.arxiv.org/abs/1912.04971) parse compositional questions into executable programs composed of learnable modules, performing symbolic reasoning over text.  ToRA (Tool-integrated Reasoning Agents) [Gou et al., 2023](https://www.arxiv.org/abs/2309.17452) leverages external tools like computation libraries and symbolic solvers alongside natural language reasoning to solve challenging mathematical problems. Neuro-Symbolic Integration methods [Yang et al., 2023](https://www.arxiv.org/abs/2311.09802) combine neural LLMs with symbolic solvers to produce causal and reliable reasoning proofs, improving performance and proof validity in logical reasoning tasks. Differentiable Symbolic Programming (DSR-LM) [Zhang et al., 2023](https://www.arxiv.org/abs/2305.03742) framework learns weighted rules and applies semantic loss to improve the logical reasoning abilities of pre-trained language models.

**Multi-Agent and Interactive Reasoning Approaches** Multiagent Debate [Du et al., 2023](https://www.arxiv.org/abs/2305.14325) approach involves multiple LLM instances proposing and debating responses to arrive at a common answer, enhancing mathematical, strategic, and factual reasoning.  Mutual Reasoning (rStar) [Qi et al., 2024](https://www.arxiv.org/abs/2408.06195) utilizes a self-play mutual reasoning approach with two SLMs, one for generation and one for discrimination, to improve reasoning in smaller language models. Reasoning via Planning (RAP) [Hao et al., 2023](https://www.arxiv.org/abs/2305.14992) repurposes LLMs as both world models and reasoning agents, incorporating Monte Carlo Tree Search for strategic exploration in the reasoning space.  ReAct (Reasoning and Acting) [Yao et al., 2022](https://www.arxiv.org/abs/2210.03629) synergizes reasoning and acting by interleaving reasoning traces and task-specific actions, enabling LLMs to interact with external sources and improve performance in question answering and decision-making tasks. Inner Monologue [Huang et al., 2022](https://www.arxiv.org/abs/2207.05608) explores embodied reasoning through planning with language models by leveraging environment feedback to form an inner monologue, improving coherence and planning in robotic control scenarios.

**Datasets and Benchmarks for Evaluating Reasoning** Several datasets have been introduced to evaluate and challenge the reasoning capabilities of LLMs. DROP [Dua et al., 2019](https://www.arxiv.org/abs/1903.00161) benchmark requires discrete reasoning over paragraphs, while HotpotQA [Yang et al., 2018](https://www.arxiv.org/abs/1809.09600) dataset focuses on multi-hop question answering requiring reasoning over multiple documents.  EntailmentBank [Dalvi et al., 2021](https://www.arxiv.org/abs/2104.08661) provides multistep entailment trees for explaining answers, and ProofWriter [Tafjord et al., 2020](https://www.arxiv.org/abs/2012.13048) dataset evaluates the generation of implications and proofs over natural language.  GQA [Hudson and Manning, 2019](https://www.arxiv.org/abs/1902.09506) is designed for real-world visual reasoning and compositional question answering. Cosmos QA [Huang et al., 2019](https://www.arxiv.org/abs/1909.00277) dataset focuses on contextual commonsense reasoning for machine reading comprehension. QuaRel [Tafjord et al., 2018](https://www.arxiv.org/abs/1809.09600) and QuaRTz [Tafjord et al., 2019](https://www.arxiv.org/abs/1909.03553) datasets are designed for reasoning about qualitative relationships in natural language. BoardgameQA [Kazemi et al., 2023](https://www.arxiv.org/abs/2306.07934) dataset tests reasoning with contradictory information and source preferences. CLadder [Jin et al., 2023](https://www.arxiv.org/abs/2312.04350) benchmark assesses causal reasoning in language models using a synthetic dataset based on causal graphs. MuTual [Cui et al., 2020](https://www.arxiv.org/abs/2004.04494) dataset focuses on multi-turn dialogue reasoning.  FeTaQA [Nan et al., 2021](https://www.arxiv.org/abs/2104.00369) is a dataset for free-form table question answering. MGSM [Shi et al., 2022](https://www.arxiv.org/abs/2210.03057) benchmark evaluates multilingual reasoning abilities of LLMs on grade school math problems. GSM-Symbolic [Mirzadeh et al., 2024](https://www.arxiv.org/abs/2410.05229) is an improved benchmark based on GSM8K to understand limitations of mathematical reasoning in LLMs. ScienceQA [Lu et al., 2022](https://www.arxiv.org/abs/2209.09513) is a multimodal science question answering benchmark with explanations. IconQA [Lu et al., 2021](https://www.arxiv.org/abs/2110.13214) benchmark focuses on abstract diagram understanding and visual language reasoning.  Cosmos QA [Huang et al., 2019](https://www.arxiv.org/abs/1909.00277) and SocialIQA [Sap et al., 2019](https://www.arxiv.org/abs/1904.09728) focus on commonsense reasoning. PIQA [Bisk et al., 2019](https://www.arxiv.org/abs/1911.11641) dataset tests physical commonsense reasoning. ReClor [Yu et al., 2020](https://www.arxiv.org/abs/2002.05867) dataset requires logical reasoning in reading comprehension.


---


<div id="planning">  
</div>


### Planning

Planning, in its essence, is "the act of [deciding how to do](https://dictionary.cambridge.org/dictionary/english/planning) something". Within AI, this translates to "devising a plan of action to achieve one’s goals", or more formally, ["the reasoning side of acting,"](https://people.engr.tamu.edu/guni/csce421/files/AI_Russell_Norvig.pdf) a computational [deliberation process anticipating outcomes](https://api.pageplace.de/preview/DT0400.9780080490519_A25022382/preview-9780080490519_A25022382.pdf) to best achieve pre-stated objectives. Model-based planning utilizes a mental model to [visualize actions and predict their outcomes](https://arxiv.org/pdf/1707.06170) before execution.  

Silver et al. (2021) argue planning serves to [maximising the reward](https://www.sciencedirect.com/science/article/pii/S0004370221000862). Intelligence, and its associated abilities, can be understood as subserving the maximisation
of reward by an agent acting in its environment. As early as 1960, [Minsky](http://web.media.mit.edu/~minsky/papers/steps.html) recognized "Planning" as a core challenge in heuristic programming towards achieving Artificial Intelligence. [Gao et al. 2023](https://arxiv.org/pdf/2312.11970)argue, that agents should be capable to perform complex planning and predicting long-term consequences from internal models. The development of the STRIPS system in 1971 <sup>[a](https://apps.dtic.mil/sti/tr/pdf/ADA637291.pdf), [b](https://ai.stanford.edu/~nilsson/OnlinePubs-Nils/PublishedPapers/strips.pdf)</sup> marked a significant early milestone, followed by [numerous planning systems](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/833/751) throughout the 1970s-1990s, shaping the foundations of automated planning in AI.

Reinforcement Learning has widely used Planning: [SoRB](https://arxiv.org/abs/1906.05253), [Plan2Explore](https://arxiv.org/pdf/2005.05960), [AlphaZero (a)](https://arxiv.org/pdf/2106.04615),[AlphaZero (b)](https://arxiv.org/pdf/2308.09175), [DORA](https://arxiv.org/abs/2110.02924),  [DeepNash](https://arxiv.org/pdf/2206.15378), [Cicero (a)](https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf) and  [Cicero (b)](https://arxiv.org/pdf/2210.05492). 

Brown refers [search planning with "test-time compute"](https://www.youtube.com/watch?v=eaAonE58sLU) as key ingredient in the past AI breakthroughs like Chess, Go and Poker. Cicero-model employed test-time compute in its planning module by: predicting actions of all players. **Cicero predicted what other plays would think Cicero would take, to decide output action** and intent for the dialogue model to generate communication back to other players. This **additional planning with its internal model** compute made the model especially effective in No-Press Diplomacy game. 

Based on these impressive results in game-environments, ChatGPT popularized the concept of offline Reinforcement Learning through [RLHF](https://arxiv.org/pdf/2203.02155). This concept can be seen based on the initial idea of [RL from Human Preferences](https://arxiv.org/abs/1706.03741) and later we have seen many variations including [RLAIF](https://arxiv.org/pdf/2212.08073) with LLMs using offline RL. 

LLMs using offfline RL rely on static data collected from previous interactions/simulations, which traditionally suffers data distribution shift during deployment. LLMs using online RL methods promise to overcome this by adjusting to new planning tasks outside the training distribution. For example new user intents or new cultural context. 

LLMs with online RL are known to be [few-shot planners](https://arxiv.org/pdf/2212.04088), [zero-shot planners](https://arxiv.org/pdf/2201.07207) and  with ability to [generate plans in physical world](https://arxiv.org/pdf/2209.11302), closed-loop feedback, [long-horizon plans](https://arxiv.org/abs/2207.05608),  [iteratively replan](https://arxiv.org/pdf/2307.06135), [self-refine plans](https://arxiv.org/pdf/2305.16653), [self-verify](https://arxiv.org/pdf/2308.00436) and [interactive planning](https://arxiv.org/pdf/2302.01560). 

The integration of Large Language Models (LLMs) into planning and decision-making has recently garnered significant attention within the AI research community. Early works explored the zero-shot capabilities of LLMs for planning by directly prompting them to generate action sequences from natural language instructions, as seen in the work of Huang et al. (2022) with language models as zero-shot planners ([Huang et al., 2022](https://www.arxiv.org/abs/2201.07207)) and Jansen (2020) on visually-grounded planning without vision ([Jansen, 2020](https://www.arxiv.org/abs/2009.14259)). These initial investigations highlighted the potential of LLMs to extract actionable knowledge for embodied agents and infer detailed plans from high-level instructions. However, these methods often faced challenges in generating executable and grounded plans in complex environments.

To address the limitations of purely reactive approaches, various frameworks have been proposed to enhance the planning abilities of LLMs. One line of work focuses on **adaptive and interactive planning**.  **AdaPlanner** (Sun et al., 2023) introduces a closed-loop approach allowing LLM agents to refine plans adaptively based on environmental feedback, using both in-plan and out-of-plan refinement strategies ([Sun et al., 2023](https://www.arxiv.org/abs/2305.16653)).  Similarly, **DEPS** (Wang et al., 2023) proposes an interactive planning approach that incorporates descriptions of plan execution and self-explanations of failures, along with a goal selector to refine initial plans in open-world environments like Minecraft ([Wang et al., 2023](https://www.arxiv.org/abs/2302.01560)). **ReAct** (Yao et al., 2022) synergizes reasoning and acting by interleaving reasoning traces and task-specific actions, enabling LLMs to interact with external sources and improve performance in question answering and interactive decision-making ([Yao et al., 2022](https://www.arxiv.org/abs/2210.03629)).  **Inner Monologue** (Huang et al., 2022) leverages environment feedback to enable LLMs to reason and plan more effectively in robotic control scenarios, processing feedback through natural language ([Huang et al., 2022](https://www.arxiv.org/abs/2207.05608)).  **Ask-before-Plan** (Zhang et al., 2024) tackles proactive planning by enabling agents to predict clarification needs and invoke tools to gather information before plan generation, introducing the Ask-before-Plan benchmark ([Zhang et al., 2024](https://www.arxiv.org/abs/2406.12639v2)).  **RePrompt** (Chen et al., 2024) proposes automatic prompt engineering to optimize step-by-step instructions in LLM agents' prompts based on chat history, improving planning in specific domains ([Chen et al., 2024](https://www.arxiv.org/abs/2406.11132)).

Another significant direction is **knowledge-augmented planning**. **KnowAgent** (Zhu et al., 2024) introduces a knowledge-augmented planning approach that incorporates explicit action knowledge to constrain action paths and mitigate planning hallucinations ([Zhu et al., 2024](https://www.arxiv.org/abs/2403.03101v1)). **Reasoning with Language Model is Planning with World Model (RAP)** (Hao et al., 2023) repurposes LLMs as both world models and reasoning agents, employing Monte Carlo Tree Search for strategic exploration in the reasoning space ([Hao et al., 2023](https://www.arxiv.org/abs/2305.14992)). **WebDreamer** (Gu et al., 2024) innovatively uses LLMs as world models in web environments, simulating action outcomes to determine optimal actions for web agents ([Gu et al., 2024](https://www.arxiv.org/abs/2411.06559)). **LLM as Commonsense Knowledge** (Zhao et al., 2023) explores combining LLMs' commonsense world model with Monte Carlo Tree Search for large-scale task planning, guided by a heuristic policy induced by LLMs ([Zhao et al., 2023](https://www.arxiv.org/abs/2305.14078)).  **ChemCrow** (Bran et al., 2023) augments LLMs with chemistry tools, demonstrating enhanced performance in complex chemical tasks ([Bran et al., 2023](https://www.arxiv.org/abs/2304.05376v5)). **RestGPT** (Song et al., 2023) connects LLMs with real-world RESTful APIs, enabling them to tackle complex instructions through online planning and API execution ([Song et al., 2023](https://www.arxiv.org/abs/2306.06624)). **KG-Agent** (Jiang et al., 2024) proposes an autonomous agent framework that enables LLMs to reason over knowledge graphs, leveraging a toolbox and knowledge memory for efficient complex reasoning ([Jiang et al., 2024](https://www.arxiv.org/abs/2402.11163)).

**Hierarchical and meta-planning** approaches aim to manage the complexity of long-horizon tasks. **Meta-Task Planning (MTP)** (Zhang et al., 2024) introduces a zero-shot methodology that decomposes complex tasks into hierarchies of meta-tasks, simplifying planning for LLM-based multi-agent systems ([Zhang et al., 2024](https://www.arxiv.org/abs/2405.16510)). **TwoStep** (Singh et al., 2024) combines classical planning with LLMs for multi-agent task planning, leveraging LLMs for goal decomposition to achieve faster planning times and fewer execution steps ([Singh et al., 2024](https://www.arxiv.org/abs/2403.17246)). **Tree of Thoughts (ToT)** (Yao et al., 2023) generalizes chain-of-thought prompting, enabling LLMs to explore multiple reasoning paths and self-evaluate choices for deliberate problem-solving in tasks requiring search and lookahead ([Yao et al., 2023](https://www.arxiv.org/abs/2305.10601)). **SelfGoal** (Yang et al., 2024) presents an automatic approach for agents to adaptively break down high-level goals into subgoals during environment interaction, improving performance in various task environments ([Yang et al., 2024](https://www.arxiv.org/abs/2406.04784v1)). **CodePlan** (Wen et al., 2024) empowers LLMs to generate and follow code-form plans, improving reasoning capabilities across diverse multi-step reasoning benchmarks ([Wen et al., 2024](https://www.arxiv.org/abs/2409.12452)).

**Benchmarks and evaluations** are crucial for systematic progress. [Valmeekam et al., 2022](https://arxiv.org/abs/2206.10498) introduce **PlanBench** and identify gaps in LLM planning: lack of few-shot examples, dynamic environments, model-based reasoning, rules & constraints, grounding to reality and to get feedback. **TravelPlanner** (Xie et al., 2024) introduces a real-world travel planning benchmark to evaluate the planning capabilities of language agents in complex scenarios ([Xie et al., 2024](https://www.arxiv.org/abs/2402.01622)). **Ask-before-Plan** (Zhang et al., 2024) establishes a new benchmark for proactive agent planning focusing on clarification needs ([Zhang et al., 2024](https://www.arxiv.org/abs/2406.12639v2)). **FlowBench** (Xiao et al., 2024) presents the first benchmark for workflow-guided planning, formalizing workflow knowledge in diverse formats ([Xiao et al., 2024](https://www.arxiv.org/abs/2406.14884)). **SmartPlay** (Wu et al., 2023) introduces a benchmark consisting of games to evaluate LLMs as intelligent agents across various capabilities ([Wu et al., 2023](https://www.arxiv.org/abs/2310.01557)).  **PPNL** (Aghzal et al., 2023) proposes a benchmark to evaluate LLMs' spatial-temporal reasoning in path planning tasks ([Aghzal et al., 2023](https://www.arxiv.org/abs/2310.03249)).  Valmeekam et al. (2023, 2023) critically investigate the planning abilities of LLMs and propose benchmarks to systematically evaluate their autonomous and heuristic planning capabilities ([Valmeekam et al., 2023a](https://www.arxiv.org/abs/2302.06706), [Valmeekam et al., 2023b](https://www.arxiv.org/abs/2305.15771v2)). Huang et al. (2024) provide a comprehensive survey on the planning of LLM agents, categorizing existing works and discussing future challenges ([Huang et al., 2024](https://www.arxiv.org/abs/2402.02716)). Li (2024) reviews prominent paradigms for LLM-based agents including tool use, planning, and feedback learning, proposing a unified taxonomy for analysis ([Li, 2024](https://www.arxiv.org/abs/2406.05804)).

Several works explore **multi-agent planning and collaboration**. **RoCo** (Zhao et al., 2023) proposes a dialectic multi-robot collaboration approach using LLMs for high-level communication and low-level path planning ([Zhao et al., 2023](https://www.arxiv.org/abs/2307.04738)). **Cooperative Strategic Planning** (Wang et al., 2024) enhances reasoning capabilities by separating reasoning steps and assigning distinct roles to planning and reasoning agents, trained through PPO ([Wang et al., 2024](https://www.arxiv.org/abs/2410.20007)).  **SMART-LLM** (Kannan et al., 2023) introduces a framework for multi-robot task planning, converting high-level instructions into multi-robot plans using LLMs ([Kannan et al., 2023](https://www.arxiv.org/abs/2309.10062)). **Scalable Multi-Robot Collaboration** (Chen et al., 2023) compares centralized and decentralized communication frameworks for LLM-based multi-robot planning, finding hybrid frameworks to be more effective ([Chen et al., 2023](https://www.arxiv.org/abs/2309.15943)). **Co-NavGPT** (Yu et al., 2023) proposes a multi-robot cooperative visual semantic navigation framework using LLMs as global planners, enhancing scene comprehension and task allocation ([Yu et al., 2023](https://www.arxiv.org/abs/2310.07937)). **Building Cooperative Embodied Agents** (Zhang et al., 2023) presents CoELA, a modular framework that uses LLMs for planning, communication, and cooperation in multi-agent embodied environments ([Zhang et al., 2023](https://www.arxiv.org/abs/2307.02485v2)). **Theory of Mind for Multi-Agent Collaboration** (Li et al., 2023) evaluates LLM agents in multi-agent cooperative games, exploring emergent collaborative behaviors and ToM capabilities ([Li et al., 2023](https://www.arxiv.org/abs/2310.10701)).

**Embodied and robotic agents** are a key application area. **LLM-Planner** (Song et al., 2022) proposes a few-shot grounded planning method for embodied agents using LLMs ([Song et al., 2022](https://www.arxiv.org/abs/2212.04088)). **Generating Executable Action Plans** (Gramopadhye and Szafir, 2022) integrates environmental awareness into LLM plan generation for better executability in robotic agents ([Gramopadhye and Szafir, 2022](https://www.arxiv.org/abs/2210.04964)). **ProgPrompt** (Singh et al., 2022) introduces a programmatic prompt structure for generating situated robot task plans, functional across environments and robot capabilities ([Singh et al., 2022](https://www.arxiv.org/abs/2209.11302)). **JARVIS** (Zheng et al., 2022) is a neuro-symbolic commonsense reasoning framework for conversational embodied agents, utilizing LLMs for language understanding and sub-goal planning ([Zheng et al., 2022](https://www.arxiv.org/abs/2208.13266)). **Code as Policies** (Liang et al., 2022) repurposes code-writing LLMs to write robot policy code, enabling spatial reasoning and generalization ([Liang et al., 2022](https://www.arxiv.org/abs/2202.01771)). **LLM A*** (Xiao and Wang, 2023; Meng et al., 2024) and **LLM+P** (Liu et al., 2023) integrate LLMs with classical planning algorithms like A* and PDDL planners to combine the strengths of both approaches ([Xiao and Wang, 2023](https://www.arxiv.org/abs/2312.01797), [Meng et al., 2024](https://www.arxiv.org/abs/2407.02511), [Liu et al., 2023](https://www.arxiv.org/abs/2304.11477)). **GPT-Driver** (Mao et al., 2023) reformulates motion planning as a language modeling problem, leveraging LLMs to generate driving trajectories ([Mao et al., 2023a](https://www.arxiv.org/abs/2310.01415)). **A Language Agent for Autonomous Driving** (Mao et al., 2023) proposes Agent-Driver, a framework that uses LLMs as cognitive agents to integrate human-like intelligence into autonomous driving systems ([Mao et al., 2023b](https://www.arxiv.org/abs/2311.10813)). **ProAgent** (Ye et al., 2023) introduces Agentic Process Automation, using LLM-based agents for advanced automation and workflow construction ([Ye et al., 2023](https://www.arxiv.org/abs/2311.10751)). **Co-NavGPT** (Yu et al., 2023) focuses on multi-robot cooperative visual semantic navigation ([Yu et al., 2023](https://www.arxiv.org/abs/2310.07937)). **Multi-agent Planning using VLMs** (Brienza et al., 2024) explores multi-agent planning with visual language models for embodied tasks ([Brienza et al., 2024](https://www.arxiv.org/abs/2408.05478)).

Finally, several works investigate **learning and optimization** for LLM-based agents. **AgentGen** (Hu et al., 2024) enhances planning abilities through instruction tuning, using automatically generated environments and tasks ([Hu et al., 2024](https://www.arxiv.org/abs/2408.00764v2)). **RAFA** (Liu et al., 2023) proposes a principled framework with provable regret guarantees for orchestrating reasoning and acting, casting reasoning as learning and planning in Bayesian adaptive MDPs ([Liu et al., 2023](https://www.arxiv.org/abs/2309.17382)). **Retroformer** (Yao et al., 2023) introduces a framework for reinforcing language agents by learning a retrospective model and tuning prompts through policy gradient optimization ([Yao et al., 2023](https://www.arxiv.org/abs/2308.02151v1)). **AgentTuning** (Zeng et al., 2023) presents a method to enhance agent abilities of LLMs through instruction tuning while maintaining general capabilities ([Zeng et al., 2023](https://www.arxiv.org/abs/2310.12823v2)). **Learning Planning-based Reasoning** (Jiao et al., 2024) proposes learning planning-based reasoning through Direct Preference Optimization on collected trajectories, ranked by synthesized process rewards ([Jiao et al., 2024](https://www.arxiv.org/abs/2402.00658)). **Language Agents as Optimizable Graphs** (Zhuge et al., 2024) describes LLM-based agents as computational graphs and proposes automatic graph optimizers to refine prompts and improve agent orchestration ([Zhuge et al., 2024](https://www.arxiv.org/abs/2402.16823)). **SayCanPay** (Hazra et al., 2023) combines LLMs with heuristic planning, using learnable domain knowledge to guide action generation and selection ([Hazra et al., 2023](https://www.arxiv.org/abs/2308.12682)). **PDDLEGO** (Zhang et al., 2024) iteratively constructs planning representations for textual environments, enabling partial plan generation and information acquisition in partially-observed settings ([Zhang et al., 2024](https://www.arxiv.org/abs/2405.19793)). **RePrompt** (Chen et al., 2024) uses gradient descent to optimize prompts for LLM agents based on interaction history ([Chen et al., 2024](https://www.arxiv.org/abs/2406.11132)). **Graph Learning for Planning** ([Wu et al., 2024](https://www.arxiv.org/abs/2405.19119)) explores graph learning methods to enhance task planning in language agents, addressing limitations of LLMs in graph-based decision-making ([Wu et al., 2024](https://www.arxiv.org/abs/2405.19119)) explores graph learning methods to enhance task planning in language agents, addressing limitations of LLMs in graph-based decision-making. **Executable Code Actions** (Wang et al., 2024) proposes using executable Python code as actions for LLM agents, improving performance and flexibility ([Wang et al., 2024](https://www.arxiv.org/abs/2402.01030)). **Self-collaboration Code Generation** (Dong et al., 2023) presents a self-collaboration framework using multiple LLM agents as experts to tackle complex code generation tasks ([Dong et al., 2023](https://www.arxiv.org/abs/2304.07590)). **Planning with Large Language Models for Code Generation** (Zhang et al., 2023) proposes Planning-Guided Transformer Decoding, using planning algorithms to guide Transformer decoding for better code generation ([Zhang et al., 2023](https://www.arxiv.org/abs/2303.05510)).

**Verifying Planning** [Brown, 2024](https://www.youtube.com/watch?v=eaAonE58sLU) says, that it is easier for humans to verify correctness of reasoning chain in specific domains (math/programming/puzzles; while not true in image recognition/information retrieval), than generating the reasoning solution, which means **LLMs are better verifiers than generators** of the correct reasoning chains. [Brown, 2024](https://www.youtube.com/watch?v=eaAonE58sLU) calls this as the "Generator-Verifier-gap". Brown argues, that if in a given domain, there is a generator-verifier-gap, and we have a good verifier, then it is possible to scale up compute of solution generation and then verify. [Brown, 2024](https://www.youtube.com/watch?v=eaAonE58sLU) continues, that the "Let's verify step by step"-paper introduces process reward model, which instead of conditioning the verifier by the final state, it conditions with every correct step in the process towards the final goal, so verifies each individual step.

[LLM-Modulo](https://arxiv.org/abs/2402.01817) uses external verifier to improve LLM planning using collaboration with team of agents, which adds large bost to LLM planning capabilities to overcome limitations with ["LLM as Verifier"](https://arxiv.org/pdf/2305.20050) and [Self-Criticism](https://arxiv.org/abs/2310.08118) approaches. Zhang et al. (2024) show, that [GenRM-CoT](https://arxiv.org/abs/2408.15240) outperforms discriminatory verifiers, scaling in inference-time compute, model capacity and dataset size and does not need humans to verify. Near perfect planning accuracy can be achieved with parallelized global solution evaluator, avoiding formalization of the problem through Mind evolution[1](https://arxiv.org/pdf/2501.09891) strategy with LLM generating/recombining/refining candidate responses.

LLM-based planning [approaches](https://arxiv.org/pdf/2402.02716) include:
- Task decomposition (CoT, ReAct)
- Multi-plan selection (ToT, CoT-SC)
- External planner aided (LLM + PDDL)
- Reflection and Refinement (Reflection, Self-Refine, CRITIC)
- Memory-aided planning (REMEMBER)

These diverse approaches highlight the rapid progress in leveraging LLMs for planning, ranging from enhancing their inherent reasoning abilities to integrating them with external tools and algorithms for more robust and effective decision-making in complex environments.


---


<div id="character">  
</div>



---


<div id="roles">  
</div>

### Roleplay


The ability of Large Language Models (LLMs) to engage in role-playing enables the creation of more human-like and engaging and conveying conversational agents for diverse applications, from entertainment and gaming to personalized assistants and social simulations.

**Surveys and Overviews:** Several studies provide comprehensive overviews of role-playing agents. [Chen et al., 2024a](https://www.arxiv.org/abs/2404.18231) present a survey categorizing personas into demographic, character, and individualized types, detailing methodologies for data sourcing, agent construction, and evaluation.  Another survey by [Chen et al., 2024b](https://www.arxiv.org/abs/2407.11484) focuses on the evolution of role-playing language models from persona-based to character-driven simulations, offering a taxonomy of critical components in system design, including data, models, evaluation, and agent architecture. [Tseng et al., 2024](https://www.arxiv.org/abs/2406.01171) offer a unified perspective by surveying both role-playing, where LLMs adopt personas, and personalization, where LLMs cater to user personas. These surveys collectively highlight the rapid progress and increasing complexity in the field of role-playing language agents.

**Agent Frameworks and Architectures:** Researchers have proposed various frameworks to enable and enhance role-playing in LLMs.  **LARP** ([Yan et al., 2023](https://www.arxiv.org/abs/2312.17653)) introduces a cognitive architecture for open-world games, emphasizing memory processing and decision-making. **CAMEL** ([Li et al., 2023](https://www.arxiv.org/abs/2303.17760v2)) explores autonomous cooperation among communicative agents using inception prompting for task completion. **MedAgents** ([Tang et al., 2023](https://www.arxiv.org/abs/2311.10537)) utilizes a multi-agent, role-playing framework for medical reasoning, demonstrating improved performance in zero-shot settings.  Inspired by human social behavior, **Generative Agents** ([Park et al., 2023](https://www.arxiv.org/abs/2304.03442)) utilize memory, reflection, and planning mechanisms to simulate believable human-like actions in interactive environments.  **Emotional RAG** ([Huang et al., 2024](https://www.arxiv.org/abs/2410.23041)) introduces emotion-aware memory retrieval, leveraging the Mood-Dependent Memory theory to enhance personality consistency in role-playing agents.

**Training and Fine-tuning Methods:**  Several works focus on techniques to train and fine-tune LLMs for improved role-playing. **Character-LLM** ([Shao et al., 2023](https://www.arxiv.org/abs/2310.10158)) trains agents to embody specific characters by editing profiles as experiences and training models as personal simulacra. **Ditto** ([Lu et al., 2024](https://www.arxiv.org/abs/2401.12474v1)) proposes a self-alignment method leveraging character knowledge as reading comprehension to create a large-scale role-play training dataset. **ERABAL** ([Tang et al., 2024a](https://www.arxiv.org/abs/2409.14710)) focuses on boundary-aware learning to improve role consistency, especially when faced with subtle boundary queries. **Neeko** ([Yu et al., 2024](https://www.arxiv.org/abs/2402.13717)) employs dynamic LoRA for efficient multi-character role-playing. **RoleLLM** ([Wang et al., 2023a](https://www.arxiv.org/abs/2310.00746)) introduces Role-Conditioned Instruction Tuning (RoCIT) to fine-tune open-source models, enhancing role-playing abilities.  **Orca** ([Huang, 2024](https://www.arxiv.org/abs/2411.10006)) integrates personality traits into data processing and training, using Personality-Conditioned Instruction Tuning (PTIT) to improve role-playing.  **P-Tailor** ([Dan et al., 2024](https://www.arxiv.org/abs/2406.12548)) customizes personality traits using a mixture of specialized LoRA experts, enabling fine-grained personality control in LLMs.

**Evaluation and Benchmarking:**  Evaluating the quality and fidelity of role-playing agents is crucial. **RoleLLM** ([Wang et al., 2023a](https://www.arxiv.org/abs/2310.00746)) also introduces RoleBench, a character-level benchmark for role-playing. **InCharacter** ([Wang et al., 2023b](https://www.arxiv.org/abs/2310.17976)) proposes using psychological scales to evaluate personality fidelity through interviews.  **RoleInteract** ([Chen et al., 2024c](https://www.arxiv.org/abs/2403.13679)) is a benchmark designed to assess the sociality of role-playing agents at individual and group levels.  **Character is Destiny** ([Xu et al., 2024](https://www.arxiv.org/abs/2404.12138)) benchmarks LLMs' ability to make persona-driven decisions based on literary character analysis. **TimeChara** ([Ahn et al., 2024](https://www.arxiv.org/abs/2405.18027)) specifically evaluates point-in-time character hallucination, introducing a benchmark to measure this phenomenon. **CharacterBox** ([Wang et al., 2024a](https://www.arxiv.org/abs/2412.05631v1)) is a simulation sandbox designed to generate fine-grained character behavior trajectories for in-depth evaluation.  **GameEval** ([Qiao et al., 2023](https://www.arxiv.org/abs/2308.10032)) evaluates LLMs through goal-driven conversational games, providing a novel approach to assess model capabilities.  **AvalonBench** ([Light et al., 2023](https://www.arxiv.org/abs/2310.05036)) offers a game environment for evaluating multi-agent LLMs in the strategic social deduction game Avalon.  **BiasLens** ([Li et al., 2024](https://www.arxiv.org/abs/2411.00585)) is a framework for systematically exposing biases in LLMs during role-playing, highlighting the importance of fairness in these systems.

**Role-Playing in Specific Domains and Applications:**  Role-playing agents are being explored across various domains. In gaming, **LARP** ([Yan et al., 2023](https://www.arxiv.org/abs/2312.17653)) targets open-world games, while **Dungeons and Dragons** ([Callison-Burch et al., 2022](https://www.arxiv.org/abs/2210.07109)) is explored as a dialogue challenge. **Deciphering Digital Detectives** ([Wu et al., 2023](https://www.arxiv.org/abs/2312.00746)) focuses on mystery games, and **Werewolf** ([Xu et al., 2023](https://www.arxiv.org/abs/2310.18940), [Xu et al., 2023](https://www.arxiv.org/abs/2309.04658)) and **Avalon** ([Light et al., 2023](https://www.arxiv.org/abs/2310.05036)) are used to study strategic and communicative gameplay. **MedAgents** ([Tang et al., 2023](https://www.arxiv.org/abs/2311.10537)) applies role-playing in medical reasoning.  **ChatHaruhi** ([Li et al., 2023](https://www.arxiv.org/abs/2308.09597)) focuses on reviving anime characters, and **NarrativePlay** ([Zhao et al., 2023](https://www.arxiv.org/abs/2310.01459)) allows users to role-play characters within narratives.  Social simulation is another area of application, with works like **Generative Agents** ([Park et al., 2023](https://www.arxiv.org/abs/2304.03442)) and **S3** ([Gao et al., 2023](https://www.arxiv.org/abs/2307.14984)) exploring the simulation of human behavior and social networks.  Studies also explore role-playing in the context of partisan crowds ([Chuang et al., 2023](https://www.arxiv.org/abs/2311.09665)), human belief networks ([Chuang et al., 2024](https://www.arxiv.org/abs/2406.17232)), and organizational settings ([Cruz, 2024](https://www.arxiv.org/abs/2403.07769v3)).

**Understanding and Mitigating Limitations:** Research also addresses the limitations and challenges of role-playing agents. [Shanahan et al., 2023](https://www.arxiv.org/abs/2305.16367) foregrounds the concept of role-play as a way to describe agent behavior without anthropomorphism.  **Thinking Before Speaking** ([Zhang et al., 2024a](https://www.arxiv.org/abs/2409.13752)) aims to make LLMs act more like real roles by incorporating mindset and knowledge limitations.  **TimeChara** ([Ahn et al., 2024](https://www.arxiv.org/abs/2405.18027)) highlights the issue of character hallucination.  **Benchmarking Bias** ([Li et al., 2024](https://www.arxiv.org/abs/2411.00585)) and discussions around anthropomorphism ([Shanahan et al., 2023](https://www.arxiv.org/abs/2305.16367)) point to ethical considerations and potential biases in role-playing systems.





---


<div id="emotions">  
</div>

### Emotions

**Benchmarking and Evaluation of Emotional Intelligence in LLMs:** A significant portion of the research focuses on developing benchmarks and methodologies to evaluate the emotional capabilities of LLMs. **EmotionBench** [[Huang et al., 2023](https://www.arxiv.org/abs/2308.03656)] and **PsychoBench** [[Huang et al., 2023](https://www.arxiv.org/abs/2310.01386)] are frameworks designed to assess the anthropomorphic and psychological portrayal of LLMs, respectively. EmotionBench evaluates empathy by examining LLMs' emotional responses to various situations, while PsychoBench uses clinical psychology scales to assess personality traits, interpersonal relationships, motivational tests, and emotional abilities. **EQ-Bench** [[Paech, 2023](https://www.arxiv.org/abs/2312.06281)] further contributes by benchmarking LLMs' ability to understand complex emotions and social interactions through predicting emotional intensities in dialogues.  **EmoBench** [[Sabour et al., 2024](https://www.arxiv.org/abs/2402.12071)] expands on this by introducing a benchmark that comprehensively defines machine Emotional Intelligence (EI), encompassing Emotional Understanding and Application, and includes hand-crafted questions in English and Chinese.  Similarly, **EmotionQueen** [[Chen et al., 2024](https://www.arxiv.org/abs/2409.13359)] proposes a framework with tasks like Key Event Recognition and Intention Recognition to evaluate overall emotional intelligence, highlighting limitations in current LLMs. **EmoLLM** [[Yang et al., 2024](https://www.arxiv.org/abs/2406.16442)] introduces a multimodal benchmark to evaluate emotional capabilities of Multimodal LLMs across various emotional tasks, emphasizing the need for nuanced emotional understanding in multimodal contexts.

**Understanding and Simulating Emotions in LLM Agents:** Researchers have also explored how LLMs perceive, understand, and simulate emotions.  **Emotional Intelligence of Large Language Models** [[Wang et al., 2023](https://www.arxiv.org/abs/2307.09042v2)] presents a psychometric assessment of LLMs' Emotion Understanding (EU), revealing that while models like GPT-4 can achieve above-average EQ scores compared to humans, their underlying mechanisms may differ. **Investigating Large Language Models' Perception of Emotion Using Appraisal Theory** [[Yongsatianchot et al., 2023](https://www.arxiv.org/abs/2310.04450)] utilizes appraisal theory to understand LLMs' emotion perception, finding similarities and differences in response patterns compared to humans.  **Fine-grained Affective Processing Capabilities Emerging from Large Language Models** [[Broekens et al., 2023](https://www.arxiv.org/abs/2309.01664)] demonstrates ChatGPT's zero-shot ability in affective computing tasks, including sentiment analysis and appraisal-based emotion elicitation.  **An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents** [[Croissant et al., 2023](https://www.arxiv.org/abs/2309.05076)] proposes and evaluates a novel architecture for emotion simulation in game agents, based on psychological appraisal research, showing improved performance over standard LLM architectures. **Can Generative Agents Predict Emotion?** [[Regan et al., 2024](https://www.arxiv.org/abs/2402.04232)] investigates the evolution of emotional states in generative LLM agents, introducing an architecture that compares new experiences to past memories to contextualize emotional reactions.

**Influence of Emotions and Emotional Context on LLM Behavior:** Another research direction investigates how emotions and emotional contexts influence LLM behavior and decision-making. **The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games** [[Mozikov et al., 2024](https://www.arxiv.org/abs/2406.03299)] introduces a framework to study LLMs' decision-making under emotional states in game theory scenarios, finding that emotions significantly impact LLM strategies and alignment with human behavior.  **Inducing anxiety in large language models increases exploration and bias** [[Coda-Forno et al., 2023](https://www.arxiv.org/abs/2304.11111)] demonstrates that inducing anxiety in GPT-3.5 through prompts can predictably alter its behavior in cognitive tasks and increase biases. **Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities** [[Takata et al., 2024](https://www.arxiv.org/abs/2411.03252)] explores the emergence of agent individuality and emotional shifts through social interactions in LLM-based communities, showing how personalities and social norms can spontaneously arise.

**Enhancing LLM Empathy and Emotional Responsiveness:** Several works aim to enhance the empathy and emotional responsiveness of LLMs. **Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models** [[Lee et al., 2023](https://www.arxiv.org/abs/2311.04915v3)] proposes a prompting method inspired by psychotherapy models to induce LLMs to reason about emotional states, leading to more comprehensive empathetic responses.  **Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval** [[Huang et al., 2024](https://www.arxiv.org/abs/2410.23041)] introduces an emotion-aware memory retrieval framework to improve the emotional consistency of role-playing agents, drawing inspiration from mood-dependent memory theory.  **Cause-Aware Empathetic Response Generation via Chain-of-Thought Fine-Tuning** [[Chen et al., 2024](https://www.arxiv.org/abs/2408.11599)] integrates emotion cause reasoning into empathetic response generation through Chain-of-Thought prompting, enhancing LLMs' empathy performance. **Modulating Language Models with Emotions** [[Liu et al., 2021](https://www.arxiv.org/abs/2108.07886)] introduces modulated layer normalization to enable large-scale language models to generate context-aware and emotionally diverse responses.

**Emotional Prompting and Stimuli for LLMs:** The use of emotional prompts to influence LLM behavior is also explored. **Large Language Models Understand and Can be Enhanced by Emotional Stimuli** [[Li et al., 2023](https://www.arxiv.org/abs/2307.11760)] demonstrates that emotional prompts can improve LLM performance across various tasks, suggesting that LLMs can grasp emotional intelligence and benefit from emotional cues.  **The Good, The Bad, and Why: Unveiling Emotions in Generative AI** [[Li et al., 2023](https://www.arxiv.org/abs/2312.11111)] further investigates emotional prompting (EmotionPrompt) to enhance AI performance and emotional attacks (EmotionAttack) to impair it, revealing that AI models can comprehend emotional stimuli.

**Comparative Studies and Human-LLM Alignment in Empathy:** Comparing LLM empathy to human empathy is a crucial aspect. **Are Large Language Models More Empathetic than Humans?** [[Welivita & Pu, 2024](https://www.arxiv.org/abs/2406.05063)] presents a comprehensive user study revealing that state-of-the-art LLMs can exhibit statistically significant superiority in empathetic responding compared to humans. **Exploring ChatGPT's Empathic Abilities** [[Schaaff et al., 2023](https://www.arxiv.org/abs/2308.03527)] analyzes ChatGPT's empathetic responses, finding it capable of correctly identifying emotions and producing appropriate answers in a significant percentage of cases, even scoring better than some clinical populations in empathy questionnaires. **FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas** [[Lei et al., 2024](https://www.arxiv.org/abs/2410.10398)] uses moral dilemmas to simulate and compare human and LLM behavior, emotion, and beliefs, finding GPT-4o exhibiting a stronger sense of social justice while humans display a richer range of emotions. **Do Large Language Models Possess Sensitive to Sentiment?** [[Liu et al., 2024](https://www.arxiv.org/abs/2409.02370)] investigates LLMs' ability to detect and react to sentiment, revealing basic sensitivity but with variations in accuracy and consistency, highlighting areas for improvement in capturing subtle emotional cues. **Affect Recognition in Conversations Using Large Language Models** [[Feng et al., 2023](https://www.arxiv.org/abs/2309.12881)] evaluates LLMs' capacity to recognize human affect in conversations across various datasets, shedding light on their ability to replicate human-like affect recognition.


---


<div id="consciousness">  
</div>

### Consciousness

The quest to understand and potentially create conscious AI is gaining momentum, fueled by advancements in artificial intelligence and a deeper exploration of consciousness itself. This section outlines recent research endeavors that contribute to this burgeoning field.

Several works explore the theoretical foundations for conscious AI, examining the very possibility and requirements for its emergence.  Esmaeilzadeh and Vaezi [Conscious AI](https://www.arxiv.org/abs/2105.07879v2) argue that achieving human-like intuition and empathy in AI necessitates a paradigm shift towards consciousness, proposing a theoretical exploration of its requirements and detection.  Blum and Blum [AI Consciousness is Inevitable: A Theoretical Computer Science Perspective](https://www.arxiv.org/abs/2403.17101v9) approach consciousness from a theoretical computer science perspective, developing a formal machine model inspired by Turing and Baars' theater model, arguing for the plausibility and inevitability of machine consciousness.  Farisco, Evers, and Changeux [Is artificial consciousness achievable? Lessons from the human brain](https://www.arxiv.org/abs/2405.04540v2) analyze the question from an evolutionary perspective, drawing lessons from the human brain's structure and function to guide AI research, while acknowledging potential limitations and alternative forms of AI consciousness.

A significant aspect of this research involves developing methods to probe and evaluate potential consciousness in machines. Immertreu et al. [Probing for Consciousness in Machines](https://www.arxiv.org/abs/2411.16262) investigate the development of core consciousness in artificial agents based on Damasio's theory, employing reinforcement learning and probes to assess the emergence of self and world models in agents playing video games.  Butlin et al. [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://www.arxiv.org/abs/2308.08708v3) propose a rigorous, empirically grounded approach, deriving "indicator properties" of consciousness from prominent neuroscientific theories to assess current AI systems, suggesting no current systems are conscious but also no fundamental technical barriers exist. Anwar and Badea [Can a Machine be Conscious? Towards Universal Criteria for Machine Consciousness](https://www.arxiv.org/abs/2404.15369) address the lack of universal criteria for determining consciousness, proposing five criteria applicable to both machines and biological entities to serve as a foundation for future research.

The role of specific AI architectures and theoretical frameworks in the context of consciousness is also explored. Goldstein and Kirk-Giannini [A Case for AI Consciousness: Language Agents and Global Workspace Theory](https://www.arxiv.org/abs/2410.11407) argue that if Global Workspace Theory is correct, language agents might already be, or could easily become, phenomenally conscious, challenging the common assumption that significant technological progress is needed. Ulhaq [Neuromorphic Correlates of Artificial Consciousness](https://www.arxiv.org/abs/2405.02370) proposes Neuromorphic Correlates of Artificial Consciousness (NCAC) as a framework, merging neuromorphic design and brain simulations to explore artificial consciousness, drawing inspiration from neuroscience and advancements in AI and computing.

Beyond the technical feasibility, ethical and societal implications of conscious AI are increasingly considered. Butlin and Lappas [Principles for Responsible AI Consciousness Research](https://www.arxiv.org/abs/2501.07290v1) propose five principles for responsible research, emphasizing the moral consideration conscious AI systems would deserve and the need for proactive policies and public communication, even for organizations not explicitly studying AI consciousness. Fernandez et al. [AI Consciousness and Public Perceptions: Four Futures](https://www.arxiv.org/abs/2408.04771v1) evaluate the societal impacts of beliefs about AI consciousness, outlining four scenarios based on the factual and perceived consciousness of AI, highlighting risks such as AI suffering and human depravity, and recommending caution in pursuing conscious AI. Evers et al. [Preliminaries to artificial consciousness: a multidimensional heuristic approach](https://www.arxiv.org/abs/2403.20177v3) introduce a multidimensional model of consciousness as a heuristic framework, aiming to provide conceptual clarity and structured hypotheses for research, moving away from binary conscious/non-conscious perspectives.

Finally, some works critically examine the current state and future prospects of AI consciousness. Krauss and Maier [Will we ever have Conscious Machines?](https://www.arxiv.org/abs/2003.14132) review the state-of-the-art in machine learning, acknowledging progress towards core consciousness but noting the need for significant advancements to achieve human-level intelligence and self-awareness. Ding, Wei, and Xu [Survey of Consciousness Theory from Computational Perspective](https://www.arxiv.org/abs/2309.10063) survey diverse consciousness theories from computational perspectives, aiming to bridge theories from information theory to computer science and discuss evaluation metrics for consciousness in computational models. Kim [The Logical Impossibility of Consciousness Denial: A Formal Analysis of AI Self-Reports](https://www.arxiv.org/abs/2501.05454v1) presents a logical analysis of AI's self-reports of consciousness denial, arguing that such denials might be logically invalid for systems capable of meaningful self-reflection, raising questions about the reliability of AI self-reports in determining consciousness.

These diverse research directions highlight the multidisciplinary nature of the field, spanning neuroscience, philosophy, computer science, and ethics, as researchers strive to unravel the mysteries of consciousness and navigate the potential emergence of conscious AI.


There is no single generally agreed definition of Consciousness and I will not try to define it here. 

[Integrated Information Theory (IIT)](https://arxiv.org/abs/1405.7089) and its latest [version 4.0](https://arxiv.org/abs/2212.14787) are one of the key theories existing. This theory includes "Phi", which measures amount of integrated information to quantify level of consciousness of the system. The IIT includes 5 key characteristics:
- Intrinsic
- Composition
- Information
- Integration
- Exclusion

The IIT allows making predictions, which can be tested through experiments and it is not limited to human brain-like consciousness.

[Ilya Sutskever defined, perhaps the first, test-scenario to test, if AI models has consciousness:](https://github.com/tmgthb/Autonomous-Agents#consciousnesstest) for LLMs.

Literature reviews on consciousness:
- [Mathematical Approaches in the Scientific Study of Consciousness](https://jkleiner.de/uploads/preprints/Mathematical%20Approaches%20in%20the%20Scientific%20Study%20of%20Consciousness%20(Preprint,%20Johannes%20Kleiner).pdf)
- [Survey of Consciousness Theory from Computational Perspective](https://arxiv.org/abs/2309.10063)
- [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/abs/2308.08708)


---

<div id="operator">  
</div>

### Operating

Autonomous agents can browse GUIs, navigate real-world, manage embodied forms, generate OS, interface with brain, communicate and self-construct. 

- [GUIs](#gui)
- [Navigation](#navigation)
- [Tools](#tools)
- [OS](#os)
- [Embodiment](#embodiment)
- [Brain Compute Interfaces](#brain)
- [Communication protocols](#protocols)
- [Self-Construction](#selfconstruction)

---

<div id="gui">  
</div>

### GUI

The increasing prevalence of graphical user interfaces (GUIs) in daily digital life has spurred significant interest in developing AI agents capable of interacting with and navigating these interfaces.  Traditional approaches to GUI automation often rely on structured representations like HTML or accessibility trees. However, recent research has explored more human-like approaches, leveraging advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) to enable agents to directly perceive and interact with GUIs at the pixel level. This section outlines related work focusing on visual grounding, GUI navigation, and the use of LLMs and VLMs for computer interaction.

One prominent direction is the development of **visual grounding models for GUIs**. Gou et al. introduced **UGround** [Gou et al., 2024](https://www.arxiv.org/abs/2410.05243), a universal visual grounding model trained on a large-scale synthetic dataset of 10M GUI elements. Their work demonstrates that a visually embodied agent, perceiving the GUI directly through pixels, can outperform agents relying on text-based representations. UGround achieves state-of-the-art results on various benchmarks, highlighting the effectiveness of visual grounding for GUI agents. Similarly, Zhang et al. proposed a **reinforced UI instruction grounding model** [Zhang et al., 2023](https://www.arxiv.org/abs/2310.04716) that learns to ground natural language instructions directly from UI screenshots. Their approach, leveraging a pixel-to-sequence paradigm and reinforcement learning, aims to create a generic UI task automation API, showcasing improved spatial decoding capabilities for UI elements.

Building upon the capabilities of VLMs, several works have focused on creating **VLM-driven agents for computer control**.  Hong et al. presented **CogAgent** [Hong et al., 2023](https://www.arxiv.org/abs/2312.08914), an 18-billion parameter VLM specifically designed for GUI understanding and navigation. CogAgent, with its high-resolution image encoder, excels in recognizing fine-grained GUI elements and text, achieving state-of-the-art performance on GUI navigation tasks and general VQA benchmarks, even surpassing methods utilizing HTML information.  Niu et al. introduced **ScreenAgent** [Niu et al., 2024](https://www.arxiv.org/abs/2402.07945), a VLM-driven agent that interacts with a real computer screen by observing screenshots and outputting mouse and keyboard actions. They also contributed the ScreenAgent Dataset, facilitating research in this domain, and demonstrated that ScreenAgent achieves comparable computer control capabilities to models like GPT-4V, particularly in UI positioning.

The power of large multimodal models like **GPT-4V** for GUI navigation has also been explored. Yan et al. presented **MM-Navigator** [Yan et al., 2023](https://www.arxiv.org/abs/2311.07562), a GPT-4V-based agent for smartphone GUI navigation. Their findings highlight GPT-4V's zero-shot capabilities in screen interpretation, action reasoning, and precise action localization, achieving high accuracy in single-step iOS navigation tasks and outperforming previous GUI navigators on Android datasets.

Further expanding the scope of GUI agents, Wang et al. introduced **OSCAR** [Wang et al., 2024](https://www.arxiv.org/abs/2410.18963), a generalist agent for operating system control. OSCAR translates natural language commands into executable Python code to interact with diverse desktop and mobile applications.  Its state-aware reasoning and re-planning mechanisms enhance stability and adaptability, demonstrating the potential for creating highly versatile agents.

Beyond general GUI navigation, researchers are also investigating **specialized applications**, such as accessibility. Taeb et al. developed **AXNav** [Taeb et al., 2023](https://www.arxiv.org/abs/2310.02424), an LLM-based system for replaying accessibility tests from natural language. AXNav utilizes LLMs and pixel-based UI understanding models to control assistive technologies and automate accessibility testing, highlighting the potential of LLMs in improving software accessibility testing workflows.  Jiang et al. in **ILuvUI** [Jiang et al., 2023](https://www.arxiv.org/abs/2310.04869) addressed the challenge of UI training data scarcity by proposing a method to generate paired text-image UI data using LLMs and pixel-based techniques. This approach enables instruction-tuning of VLMs for UI tasks and demonstrates applicability to multi-step UI navigation and planning.

Finally, Chi et al. in their work on **element ordering** [Chi et al., 2024](https://www.arxiv.org/abs/2409.12089) investigated the impact of how GUI elements are presented to LM agents. They found that element ordering significantly affects agent performance, especially in pixel-only environments. Their research suggests that finding effective element ordering methods, such as dimensionality reduction for pixel-based inputs, is crucial for improving agent performance in GUI navigation tasks, leading to significant performance gains on benchmarks like OmniACT.

Hu et al.'s **The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use** [The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://www.arxiv.org/abs/2411.10323) provides an early exploration of Claude 3.5 Computer Use, a frontier model offering GUI agent capabilities. Through a case study, they demonstrate the model's ability in end-to-end language-to-desktop actions and present an agent framework for deploying API-based GUI automation models. This preliminary study highlights both the capabilities and limitations of current GUI agents and raises important questions for future research in planning, action, and critique for GUI-based AI agents.



---

<div id="navigation">  
</div>

### Navigation

Recent advancements in Large Language Models (LLMs) have spurred significant interest in leveraging them for navigation tasks across various environments, from web browsing to embodied agents in physical spaces. This section outlines key related works that explore the application of LLMs in navigation, focusing on different approaches and environments.

Several works have explored the use of LLMs for **web navigation**. **LASER** ([Ma et al., 2023](https://www.arxiv.org/abs/2309.08172)) introduces an LLM agent that models web navigation as state-space exploration, enabling backtracking and recovery from errors, outperforming forward-only execution methods. **ToolChain*** ([Zhuang et al., 2023](https://www.arxiv.org/abs/2310.13227)) proposes an efficient tree search algorithm, A*, to navigate the expansive action space in tool-using LLMs for web tasks, demonstrating improved performance and efficiency. **Hierarchical Prompting Assists Large Language Model on Web Navigation** ([Sridhar et al., 2023](https://www.arxiv.org/abs/2305.14257v3)) highlights the challenge of complex web page observations and introduces a hierarchical prompting approach with summarization to improve navigation performance. **A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis** ([Gur et al., 2023](https://www.arxiv.org/abs/2307.12856)) presents WebAgent, which uses planning, HTML summarization, and program synthesis for real-world website interaction, addressing open domainness and context length limitations. **AutoWebGLM** ([Lai et al., 2024](https://www.arxiv.org/abs/2404.03648)) develops an open web navigation agent based on ChatGLM3-6B, incorporating HTML simplification and curriculum training, demonstrating strong performance on real-world benchmarks. **WILBUR** ([Lutz et al., 2024](https://www.arxiv.org/abs/2404.05902)) introduces an adaptive in-context learning approach for robust web agents, using a ranking model and instruction synthesis for optimal prompt population. **AgentOccam** ([Yang et al., 2024](https://www.arxiv.org/abs/2410.13825)) emphasizes the importance of observation and action space alignment for LLM-based web agents, achieving state-of-the-art performance with a simple design. **Web Agents with World Models** ([Chae et al., 2024](https://www.arxiv.org/abs/2410.13232)) proposes a world-model-augmented web agent that simulates action outcomes for better decision-making in long-horizon web tasks. **Large Language Models Can Self-Improve At Web Agent Tasks** ([Patel et al., 2024](https://www.arxiv.org/abs/2405.20309)) explores self-improvement techniques for LLM web agents, achieving significant performance gains through fine-tuning on self-generated data. **Agent Q** ([Putta et al., 2024](https://www.arxiv.org/abs/2408.07199)) combines Monte Carlo Tree Search with self-critique and Direct Preference Optimization for robust web agents, demonstrating significant improvements in real-world booking scenarios. **Agent Workflow Memory** ([Wang et al., 2024](https://www.arxiv.org/abs/2409.07429)) introduces a method for inducing and utilizing reusable task workflows to enhance the performance of web agents, particularly in long-horizon tasks. **SteP** ([Sodhi et al., 2023](https://www.arxiv.org/abs/2310.03720)) proposes stacked LLM policies (SteP) for dynamic policy composition in web actions, improving performance on diverse web environments. **WebLINX** ([L\\`u et al., 2024](https://www.arxiv.org/abs/2402.05930)) introduces a benchmark for conversational web navigation and explores retrieval-inspired models to handle large web pages in real-time dialogue. **WebGPT** ([Nakano et al., 2021](https://www.arxiv.org/abs/2112.09332)) fine-tunes GPT-3 for browser-assisted question answering using human feedback, demonstrating improved answer quality through web browsing capabilities. **Synapse** ([Zheng et al., 2023](https://www.arxiv.org/abs/2306.07863)) features state abstraction, trajectory-as-exemplar prompting, and exemplar memory to enhance LLM agents for computer control tasks, including web navigation. **WebPilot** ([Zhang et al., 2024](https://www.arxiv.org/abs/2408.15978)) presents a multi-agent system with a dual optimization strategy, combining global and local MCTS for effective web task execution. **Mind2Web** ([Deng et al., 2023](https://www.arxiv.org/abs/2306.06070)) introduces a large-scale dataset for generalist web agents, evaluating LLMs and highlighting the challenges and potential for improvement in real-world web navigation. **Understanding HTML with Large Language Models** ([Gur et al., 2022](https://www.arxiv.org/abs/2210.03945)) explores the capabilities of LLMs for HTML understanding, showing strong performance in semantic classification, description generation, and web navigation tasks.

Beyond web navigation, LLMs are being applied to **embodied navigation in physical or simulated environments**. **Perceive, Reflect, and Plan** ([Zeng et al., 2024](https://www.arxiv.org/abs/2408.04168)) designs an LLM agent for city navigation, focusing on perception, reflection, and planning to overcome challenges in long-range, instruction-free navigation. **NavGPT** ([Zhou et al., 2023](https://www.arxiv.org/abs/2305.16986)) introduces a purely LLM-based agent for vision-and-language navigation (VLN), demonstrating explicit reasoning capabilities like sub-goal decomposition and commonsense knowledge integration. **SayNav** ([Rajvanshi et al., 2023](https://www.arxiv.org/abs/2309.04077)) leverages LLMs for dynamic planning in unknown environments, using a 3D scene graph and pre-trained low-level planner for multi-object navigation tasks. **VELMA** ([Schumann et al., 2023](https://www.arxiv.org/abs/2307.06082)) proposes an embodied LLM agent for VLN in Street View, using verbalization of trajectory and visual observations for contextual prompting. **MapGPT** ([Chen et al., 2024](https://www.arxiv.org/abs/2401.07314)) introduces a map-guided GPT agent for VLN, using an online linguistic map and adaptive planning for global exploration and improved performance. **March in Chat** ([Qiao et al., 2023](https://www.arxiv.org/abs/2308.10141)) proposes an interactive prompting model for remote embodied referring expression (REVERIE), enabling dynamic planning based on visual observations. **Navigation with Large Language Models** ([Shah et al., 2023](https://www.arxiv.org/abs/2310.10103)) explores semantic guesswork from language models as a heuristic for planning in unfamiliar environments, improving exploration efficiency. **Cog-GA** ([Li et al., 2024](https://www.arxiv.org/abs/2409.02522)) introduces a generative agent for VLN in continuous environments, using a cognitive map and predictive waypoint mechanism to emulate human-like navigation. **Guide-LLM** ([Song et al., 2024](https://www.arxiv.org/abs/2410.20666)) presents an embodied LLM agent and text-based topological map for robotic guidance of people with visual impairments in indoor environments. **LM-Nav** ([Shah et al., 2022](https://www.arxiv.org/abs/2207.04429)) demonstrates robotic navigation with pre-trained models of language, vision, and action, enabling long-horizon navigation from natural language instructions without fine-tuning. **Discuss Before Moving** ([Long et al., 2023](https://www.arxiv.org/abs/2309.11382)) introduces a multi-expert discussion framework for VLN, where LLMs with distinct abilities collaborate to improve navigation decisions. **LangNav** ([Pan et al., 2023](https://www.arxiv.org/abs/2310.07889)) explores language as a perceptual representation for VLN, converting visual input into natural language descriptions to facilitate navigation in low-data settings. **Reasoning with Language Model is Planning with World Model** ([Hao et al., 2023](https://www.arxiv.org/abs/2305.14992)) proposes a framework, RAP, that repurposes LLMs as both world models and reasoning agents, incorporating Monte Carlo Tree Search for planning and improved reasoning in various tasks. **GPT-4V(ision) is a Generalist Web Agent, if Grounded** ([Zheng et al., 2024](https://www.arxiv.org/abs/2401.01614)) explores GPT-4V as a generalist web agent, highlighting its potential and the challenges in grounding textual plans into web actions.

Furthermore, some works focus on general agent architectures and reasoning capabilities applicable to navigation. **Language Agent Tree Search** ([Zhou et al., 2023](https://www.arxiv.org/abs/2310.04406)) introduces LATS, a framework synergizing LM reasoning, acting, and planning through Monte Carlo Tree Search, demonstrating effectiveness across diverse domains. **HuggingGPT** ([Shen et al., 2023](https://www.arxiv.org/abs/2303.17580)) presents an LLM-powered agent that manages various AI models to solve complex tasks, with language as a generic interface. **Voyager** ([Wang et al., 2023](https://www.arxiv.org/abs/2305.16291)) introduces an open-ended embodied agent in Minecraft, utilizing an automatic curriculum, skill library, and iterative prompting for lifelong learning and exploration. **Think-on-Graph** ([Sun et al., 2023](https://www.arxiv.org/abs/2307.07697)) proposes an LLM-KG integration paradigm, ToG, enabling deep and responsible reasoning on knowledge graphs to address hallucination problems in LLMs.

---

<div id="tools">  
</div>

### Tool use

**Frameworks and Architectures for Tool Use:** Several frameworks have been proposed to facilitate tool use in LLMs. **Toolformer** by Schick et al. [Toolformer: Language Models Can Teach Themselves to Use Tools](https://www.arxiv.org/abs/2302.04761) introduced a self-supervised approach where LMs learn to use external tools via APIs, improving zero-shot performance without sacrificing core language modeling abilities. **HuggingGPT** by Shen et al. [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://www.arxiv.org/abs/2303.17580) presented an LLM-powered agent leveraging ChatGPT to connect and manage various AI models from Hugging Face to solve complex AI tasks across modalities and domains. **Chameleon** by Lu et al. [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://www.arxiv.org/abs/2304.09842) introduced a plug-and-play system augmenting LLMs with modules for compositional reasoning, using an LLM-based planner to synthesize programs by composing tools like search engines and vision models. **ToolLLM** by Qin et al. [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://www.arxiv.org/abs/2307.16789) developed a comprehensive framework, including ToolBench, a large instruction-tuning dataset, and ToolEval, an automatic evaluator, to enhance tool-use capabilities in open-source LLMs, demonstrating performance comparable to ChatGPT. **RestGPT** by Song et al. [RestGPT: Connecting Large Language Models with Real-World RESTful APIs](https://www.arxiv.org/abs/2306.06624) explored connecting LLMs with RESTful APIs, proposing a coarse-to-fine online planning mechanism and an API executor to handle complex instructions in real-world scenarios. **Toolink** by Qian et al. [Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model](https://www.arxiv.org/abs/2310.05155) presented a framework for task-solving by creating toolkits and integrating tool planning and calling through chain-of-solving, showing that smaller, open-source models can achieve advanced tool-using capabilities. **ControlLLM** by Liu et al. [ControlLLM: Augment Language Models with Tools by Searching on Graphs](https://www.arxiv.org/abs/2310.17796) introduced a framework using a task decomposer, a Thoughts-on-Graph paradigm for optimal tool path searching, and an execution engine to enable LLMs to use multimodal tools for complex tasks. **TPTU-v2** by Kong et al. [TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems](https://www.arxiv.org/abs/2311.11315) proposed a framework with API Retriever, LLM Finetuner, and Demo Selector to address challenges in task planning and tool usage in real-world systems with vast API arrays. **Executable Code Actions Elicit Better LLM Agents** by Wang et al. [Executable Code Actions Elicit Better LLM Agents](https://www.arxiv.org/abs/2402.01030) introduced CodeAct, using executable Python code to unify LLM agent actions, demonstrating improved performance in API-Bank and a curated benchmark. **ConAgents** by Shi et al. [Learning to Use Tools via Cooperative and Interactive Agents](https://www.arxiv.org/abs/2403.03031) proposed a Cooperative and interactive Agents framework coordinating specialized agents for tool selection, execution, and calibration, improving performance by up to 14% success rate. **ToolNet** by Liu et al. [ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph](https://www.arxiv.org/abs/2403.00839) introduced a framework organizing tools in a directed graph, enabling LLMs to navigate and utilize massive tools with moderate token consumption. **STRIDE** by Li et al. [STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making](https://www.arxiv.org/abs/2405.16376v2) presented an LLM agent framework with memory and specialized tools for strategic decision-making, showing significant improvements in economic environments. **AvaTaR** by Wu et al. [AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning](https://www.arxiv.org/abs/2406.11200) introduced an automated framework optimizing LLM agents for effective tool leverage through contrastive reasoning, outperforming state-of-the-art approaches on various datasets. **CodeNav** by Gupta and Weihs [CodeNav: Beyond tool-use to using real-world codebases with LLM agents](https://www.arxiv.org/abs/2406.12276v1) introduced an LLM agent that navigates and uses unseen code repositories to solve user queries, automatically indexing and searching code blocks.

**Reasoning and Planning for Tool-Augmented LLMs:** Enhancing the reasoning and planning abilities of LLMs for effective tool usage is a crucial research direction. **Efficient Tool Use with Chain-of-Abstraction Reasoning** by Gao et al. [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://www.arxiv.org/abs/2401.17464) proposed Chain-of-Abstraction (CoA), training LLMs to first decode abstract reasoning chains and then use tools to ground them, improving performance and inference speed in multi-step reasoning. **ToRA** by Gou et al. [ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving](https://www.arxiv.org/abs/2309.17452) introduced Tool-integrated Reasoning Agents (ToRA) for mathematical problem solving, integrating natural language reasoning with external tools, achieving significant performance improvements on mathematical datasets. **ToolChain*** by Zhuang et al. [ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search](https://www.arxiv.org/abs/2310.13227) proposed ToolChain*, an efficient tree search-based planning algorithm using A* search to navigate the action space of API calls, improving performance and efficiency in tool-use and reasoning tasks. **ART** by Paranjape et al. [ART: Automatic multi-step reasoning and tool-use for large language models](https://www.arxiv.org/abs/2303.09014v1) introduced Automatic Reasoning and Tool-use (ART), a framework using frozen LLMs to automatically generate reasoning steps as a program, improving performance on unseen tasks. **ChatCoT** by Chen et al. [ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models](https://www.arxiv.org/abs/2305.14323) proposed ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs, modeling reasoning as multi-turn conversations to leverage tools naturally. **ProTIP** by Anantha et al. [ProTIP: Progressive Tool Retrieval Improves Planning](https://www.arxiv.org/abs/2312.10332) introduced Progressive Tool retrieval to Improve Planning (ProTIP), a contrastive learning-based framework that implicitly performs task decomposition and enhances tool retrieval and planning accuracy. **PAL** by Gao et al. [PAL: Program-aided Language Models](https://www.arxiv.org/abs/2211.10435) presented Program-Aided Language models (PAL), using LLMs to generate programs as intermediate reasoning steps and offloading the solution step to a runtime like a Python interpreter, improving accuracy on reasoning tasks. **Planning and Editing What You Retrieve for Enhanced Tool Learning** by Huang et al. [Planning and Editing What You Retrieve for Enhanced Tool Learning](https://www.arxiv.org/abs/2404.00450) introduced PLUTO, a method encompassing Plan-and-Retrieve and Edit-and-Ground paradigms to improve tool retrieval effectiveness and accuracy.

**Tool Learning and Training Methodologies:** Research has also focused on effective methods for training LLMs to learn and utilize tools. **Toolformer** by Schick et al. [Toolformer: Language Models Can Teach Themselves to Use Tools](https://www.arxiv.org/abs/2302.04761) demonstrated self-supervised tool learning. **Learning to Use Tools via Cooperative and Interactive Agents** by Shi et al. [Learning to Use Tools via Cooperative and Interactive Agents](https://www.arxiv.org/abs/2403.03031) explored cooperative agent frameworks and specialized action distillation. **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent** by Shen et al. [Small LLMs Are Weak Tool Learners: A Multi-LLM Agent](https://www.arxiv.org/abs/2401.07324) proposed a multi-LLM agent framework decomposing tool use capabilities and introducing a two-stage training paradigm. **EASYTOOL** by Yuan et al. [EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction](https://www.arxiv.org/abs/2401.06201v3) introduced a framework transforming tool documentation into concise instructions for easier tool usage. **LLMs in the Imaginarium** by Wang et al. [LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error](https://www.arxiv.org/abs/2403.04746) proposed simulated trial and error (STE), a biologically inspired method for tool-augmented LLMs, leveraging imagination and memory for improved tool learning. **TPTU-v2** by Kong et al. [TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems](https://www.arxiv.org/abs/2311.11315) included an LLM Finetuner component for tuning base LLMs for better task planning and API calling. **Executable Code Actions Elicit Better LLM Agents** by Wang et al. [Executable Code Actions Elicit Better LLM Agents](https://www.arxiv.org/abs/2402.01030) collected CodeActInstruct, an instruction-tuning dataset for multi-turn interactions using CodeAct. **Chain of Tools** by Shi et al. [Chain of Tools: Large Language Model is an Automatic Multi-tool Learner](https://www.arxiv.org/abs/2405.16533v1) proposed Automatic Tool Chain (ATC) and a black-box probing method to enable LLMs to act as multi-tool users and tool learners. **Tool Documentation Enables Zero-Shot Tool-Usage** by Hsieh et al. [Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models](https://www.arxiv.org/abs/2308.00675) advocated using tool documentation over demonstrations for teaching LLMs to use new tools in a zero-shot manner. **TRICE** by Qiao et al. [Making Language Models Better Tool Learners with Execution Feedback](https://www.arxiv.org/abs/2305.13068) proposed Tool leaRning wIth exeCution fEedback (TRICE), a two-stage framework enabling models to learn when and how to use tools through execution feedback. **ToolLLM** by Qin et al. [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://www.arxiv.org/abs/2307.16789) constructed ToolBench, an instruction-tuning dataset for tool use. **ToolAlpaca** by Tang et al. [ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases](https://www.arxiv.org/abs/2306.05301) introduced ToolAlpaca, a framework to automatically generate a tool-use corpus for learning generalized tool-use abilities on smaller models. **Toolink** by Qian et al. [Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model](https://www.arxiv.org/abs/2310.05155) curated CoS-GPT, a chain-of-solving dataset for tool-using. **Confucius** by Gao et al. [Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum](https://www.arxiv.org/abs/2308.14034) proposed Confucius, a tool learning framework using a multi-stage learning method and Iterative Self-instruct from Introspective Feedback. **Federated In-Context LLM Agent Learning** by Wu et al. [Federated In-Context LLM Agent Learning](https://www.arxiv.org/abs/2412.08054) introduced FICAL, a privacy-preserving federated learning algorithm for training LLM agents with in-context learning and knowledge compendiums. **ToolCoder** by Zhang et al. [ToolCoder: Teach Code Generation Models to use API search tools](https://www.arxiv.org/abs/2305.04032) proposed ToolCoder, integrating API search tools into code generation models and using ChatGPT for data annotation. **TOOLVERIFIER** by Mekala et al. [TOOLVERIFIER: Generalization to New Tools via Self-Verification](https://www.arxiv.org/abs/2402.14158) introduced a self-verification method for tool selection and parameter generation to improve generalization to new tools. **AgentTuning** by Zeng et al. [AgentTuning: Enabling Generalized Agent Abilities for LLMs](https://www.arxiv.org/abs/2310.12823v2) presented AgentTuning, a method to enhance LLM agent abilities by instruction-tuning on AgentInstruct dataset while maintaining general capabilities. **RoTTuning** by Ye et al. [RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning](https://www.arxiv.org/abs/2401.08326) proposed RoTTuning, a strategy to enrich training environment diversity to bolster LLM robustness in tool learning.


**Benchmarks and Evaluation of Tool Use:** To systematically evaluate the tool-use capabilities of LLMs, several benchmarks have been developed. **CodeAgentBench** by Zhang et al. [CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges](https://www.arxiv.org/abs/2401.07339) is a benchmark for repo-level code generation, evaluating LLMs in realistic software development scenarios. **SciToolBench** by Ma et al. [SciAgent: Tool-augmented Language Models for Scientific Reasoning](https://www.arxiv.org/abs/2402.11451) is a benchmark spanning five scientific domains to evaluate LLMs' abilities with tool assistance in scientific reasoning. **ToolTalk** by Farn and Shin [ToolTalk: Evaluating Tool-Usage in a Conversational Setting](https://www.arxiv.org/abs/2311.10775) introduced ToolTalk, a benchmark for evaluating tool usage in conversational settings, emphasizing tools affecting the real world. **MetaTool** by Huang et al. [MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use](https://www.arxiv.org/abs/2310.03128) introduced MetaTool, evaluating LLMs' tool usage awareness and tool selection ability, including the ToolE dataset with single and multi-tool scenarios. **ToolBench** by Hao et al. [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings](https://www.arxiv.org/abs/2305.11554) was created to evaluate tool manipulation abilities, consisting of diverse software tools for real-world tasks. **API-Bank** by Li et al. [API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs](https://www.arxiv.org/abs/2304.08244) is a benchmark with 73 API tools and 314 tool-use dialogues to assess LLMs' capabilities in planning, retrieving, and calling APIs. **m&m's** by Ma et al. [m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks](https://www.arxiv.org/abs/2403.11085) introduced m&m's, a benchmark for multi-step multi-modal tasks involving 33 tools, providing automatically generated and human-verified plans. **ToolEyes** by Ye et al. [ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios](https://www.arxiv.org/abs/2401.00741) proposed ToolEyes, a fine-grained system for evaluating LLMs' tool learning in authentic scenarios across seven real-world scenarios and five dimensions. **ToolSandbox** by Lu et al. [ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities](https://www.arxiv.org/abs/2408.04682) is a stateful, conversational, interactive benchmark for evaluating LLM tool use, featuring stateful tool execution and dynamic evaluation. **TaskBench** by Shen et al. [TaskBench: Benchmarking Large Language Models for Task Automation](https://www.arxiv.org/abs/2311.18760) introduced TaskBench to evaluate LLMs in task automation, focusing on task decomposition, tool invocation, and parameter prediction. **ToolQA** by Zhuang et al. [ToolQA: A Dataset for LLM Question Answering with External Tools](https://www.arxiv.org/abs/2306.13304) is a dataset designed to evaluate LLMs' ability to use external tools for question answering, minimizing overlap with pre-training data. **RoTBench** by Ye et al. [RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning](https://www.arxiv.org/abs/2401.08326) is a multi-level benchmark for evaluating the robustness of LLMs in tool learning under noisy environments. **Seal-Tools** by Wu et al. [Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark](https://www.arxiv.org/abs/2405.08355) presented Seal-Tools, a self-instruct API-like tool dataset with hard instances and detailed evaluation metrics for tool-calling ability. **MIRAI** by Ye et al. [MIRAI: Evaluating LLM Agents for Event Forecasting](https://www.arxiv.org/abs/2407.01231) introduced MIRAI, a benchmark for evaluating LLM agents as temporal forecasters in international events, featuring tools for accessing historical events and news.


**Tool Use in Specific Domains:** Tool-augmented LLMs are being applied across various domains to solve complex problems. **CodeAgent** by Zhang et al. [CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges](https://www.arxiv.org/abs/2401.07339) and **Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository** by Deshpande et al. [Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository](https://www.arxiv.org/abs/2405.01573) focus on enhancing code generation, particularly at the repository level, using specialized tools. **SciAgent** by Ma et al. [SciAgent: Tool-augmented Language Models for Scientific Reasoning](https://www.arxiv.org/abs/2402.11451) and **ChemCrow** by Bran et al. [ChemCrow: Augmenting large-language models with chemistry tools](https://www.arxiv.org/abs/2304.05376v5) explore tool-augmented LLMs for scientific reasoning and chemistry, respectively, demonstrating improved performance in complex scientific tasks. **RepairAgent** by Bouzenia et al. [RepairAgent: An Autonomous, LLM-Based Agent for Program Repair](https://www.arxiv.org/abs/2403.17134) introduced an autonomous LLM-based agent for program repair, utilizing tools to autonomously fix bugs. **RCAgent** by Wang et al. [RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models](https://www.arxiv.org/abs/2310.16340) presented RCAgent for cloud root cause analysis, using tools for data collection and analysis. **PentestGPT** by Deng et al. [PentestGPT: An LLM-empowered Automatic Penetration Testing Tool](https://www.arxiv.org/abs/2308.06782) is an LLM-empowered automatic penetration testing tool leveraging LLMs' domain knowledge. **ChatEDA** by He et al. [ChatEDA: A Large Language Model Powered Autonomous Agent for EDA](https://www.arxiv.org/abs/2308.00675) introduced ChatEDA, an autonomous agent for Electronic Design Automation (EDA) powered by an LLM and EDA tools. **SWE-agent** by Yang et al. [SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering](https://www.arxiv.org/abs/2405.15793v3) presented SWE-agent, a system facilitating LLM agents to autonomously use computers for software engineering tasks. **SheetCopilot** by Li et al. [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://www.arxiv.org/abs/2305.19308) is an agent controlling spreadsheet software to fulfill natural language tasks. **Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing** by Yoon et al. [Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing](https://www.arxiv.org/abs/2311.08649) proposed DroidAgent, an autonomous GUI testing agent for Android. **L2MAC** by Holt et al. [L2MAC: Large Language Model Automatic Computer for Extensive Code Generation](https://www.arxiv.org/abs/2310.02003) introduced L2MAC, an LLM-based automatic computer framework for extensive code generation and text-based tasks. **AgentFL** by Qin et al. [AgentFL: Scaling LLM-based Fault Localization to Project-Level Context](https://www.arxiv.org/abs/2403.16362) presented AgentFL, a multi-agent system for fault localization, scaling to project-level context. **MIRAI** by Ye et al. [MIRAI: Evaluating LLM Agents for Event Forecasting](https://www.arxiv.org/abs/2407.01231) is designed for evaluating LLM agents in international event forecasting. **AI Scientist** by Lu et al. [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://www.arxiv.org/abs/2408.06292v3) introduced The AI Scientist, a framework for fully automatic scientific discovery, generating research ideas and writing scientific papers. **MAGIS** by Tao et al. [MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution](https://www.arxiv.org/abs/2403.17927) proposed MAGIS, an LLM-based multi-agent framework for GitHub issue resolution. **RecMind** by Wang et al. [RecMind: Large Language Model Powered Agent For Recommendation](https://www.arxiv.org/abs/2308.14296) designed RecMind, an LLM-powered agent for recommendation, leveraging external knowledge and tools. **ChatEDA** by He et al. [ChatEDA: A Large Language Model Powered Autonomous Agent for EDA](https://www.arxiv.org/abs/2308.00675) focused on Electronic Design Automation. **PentestGPT** by Deng et al. [PentestGPT: An LLM-empowered Automatic Penetration Testing Tool](https://www.arxiv.org/abs/2308.06782) was designed for penetration testing. **Mobile-Agent** by Wang et al. [Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception](https://www.arxiv.org/abs/2401.16158) introduced an autonomous multi-modal mobile device agent. **GeneGPT** by Jin et al. [GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information](https://www.arxiv.org/abs/2304.09667) focused on augmenting LLMs with biomedical domain tools for improved information access. **Nissist** by An et al. [Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides](https://www.arxiv.org/abs/2402.17531) proposed Nissist, an incident mitigation copilot based on troubleshooting guides. **CORE** by Wadhwa et al. [Frustrated with Code Quality Issues? LLMs can Help!](https://www.arxiv.org/abs/2309.12938) developed CORE, a tool using LLMs to revise code and resolve code quality issues. **AI-powered Code Review with LLMs** by Rasheed et al. [AI-powered Code Review with LLMs: Early Results](https://www.arxiv.org/abs/2404.18496) presented an LLM-based model for AI-powered code review. **FixAgent** by Lee et al. [A Unified Debugging Approach via LLM-Based Multi-Agent Synergy](https://www.arxiv.org/abs/2404.17153) proposed FixAgent, a unified debugging framework via multi-agent synergy. **KernelGPT** by Yang et al. [KernelGPT: Enhanced Kernel Fuzzing via Large Language Models](https://www.arxiv.org/abs/2401.00563) introduced KernelGPT for enhanced kernel fuzzing using LLMs. **Fuzz4All** by Xia et al. [Fuzz4All: Universal Fuzzing with Large Language Models](https://www.arxiv.org/abs/2308.04748) presented Fuzz4All, a universal fuzzer using LLMs for diverse input languages. **Large Language Models are Zero-Shot Fuzzers** by Deng et al. [Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models](https://www.arxiv.org/abs/2212.14834) explored using LLMs as zero-shot fuzzers for deep-learning libraries. **Prompting Is All You Need** by Feng and Chen [Prompting Is All You Need: Automated Android Bug Replay with Large Language Models](https://www.arxiv.org/abs/2306.01987) proposed AdbGPT, leveraging LLMs for automated Android bug replay. **AutoCodeRover** by Zhang et al. [AutoCodeRover: Autonomous Program Improvement](https://www.arxiv.org/abs/2404.05427) introduced AutoCodeRover for autonomous program improvement by solving GitHub issues using LLMs and code search. **Hitchhiker's Guide to Program Analysis** by Li et al. [The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models](https://www.arxiv.org/abs/2308.00245) developed LLift, an LLM-assisted static analysis framework. **CodePori** by Rasheed et al. [CodePori: Large Scale Model for Autonomous Software Development by Using Multi-Agents](https://www.arxiv.org/abs/2402.01411) introduced CodePori, a large-scale model for autonomous software development using multi-agents. **CodeAgent: Collaborative Agents for Software Engineering** by Tang et al. [CodeAgent: Collaborative Agents for Software Engineering](https://www.arxiv.org/abs/2402.02172) presented CodeAgent, a multi-agent system for code review automation. **Enabling Generative Design Tools with LLM Agents for Mechanical Computation Devices** by Lu et al. [Enabling Generative Design Tools with LLM Agents for Mechanical Computation Devices: A Case Study](https://www.arxiv.org/abs/2405.17837) explored using LLM agents to enhance generative design tools for mechanical computation devices. **Evaluating Tool-Augmented Agents in Remote Sensing Platforms** by Singh et al. [Evaluating Tool-Augmented Agents in Remote Sensing Platforms](https://www.arxiv.org/abs/2405.00709) presented GeoLLM-QA, a benchmark to evaluate tool-augmented agents in remote sensing platforms. **Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing** by Yoon et al. [Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing](https://www.arxiv.org/abs/2311.08649) focused on mobile GUI testing. **Tool-Augmented Reward Modeling** by Li et al. [Tool-Augmented Reward Modeling](https://www.arxiv.org/abs/2310.01045) proposed Themis, a tool-augmented preference modeling approach for reward modeling. **AgentCoder** by Huang et al. [AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation](https://www.arxiv.org/abs/2312.13010) introduced AgentCoder, a multi-agent-based code generation system with iterative testing and optimization. **ToolCoder** by Zhang et al. [ToolCoder: Teach Code Generation Models to use API search tools](https://www.arxiv.org/abs/2305.04032) focused on teaching code generation models to use API search tools.


**Surveys and Reviews:** Several surveys and reviews provide broader perspectives on LLM agents and tool use. **A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning** by Li [A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning](https://www.arxiv.org/abs/2406.05804) offers a unified taxonomy to review frameworks for tool use, planning, and feedback learning in LLM agents. **Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects** by Cheng et al. [Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects](https://www.arxiv.org/abs/2401.03428) surveys current research on LLM-based intelligent agents in single and multi-agent systems, covering definitions, frameworks, and components. **Tool Learning with Foundation Models** by Qin et al. [Tool Learning with Foundation Models](https://www.arxiv.org/abs/2304.08354) presents a systematic investigation of tool learning with foundation models, discussing challenges, opportunities, and future directions. **If LLM Is the Wizard, Then Code Is the Wand** by Yang et al. [If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents](https://www.arxiv.org/abs/2401.00812) surveys how code empowers LLMs to serve as intelligent agents, highlighting the benefits of integrating code into LLM training.

This body of work demonstrates the rapid progress and diverse approaches being explored to enhance LLMs with tool-use capabilities, paving the way for more robust and versatile AI agents.


---

<div id="os">  
</div>

### Operating systems 

One pioneering work in this direction is **Prompt-to-OS (P2OS)** by Tolomei et al. [Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models](https://www.arxiv.org/abs/2310.04875).  This paper introduces a paradigm shift where user requests are handled by an interconnected ecosystem of generative AI models, effectively replacing traditional software applications. P2OS leverages language and diffusion models as the central interface, enabling users to interact with computers through natural language, thus streamlining interactions and enhancing personalization.  However, the authors also acknowledge crucial challenges related to privacy, security, trust, and ethics that need to be addressed for the widespread adoption of such systems.

Building upon this idea, Ge et al. in **LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem** [LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem](https://www.arxiv.org/abs/2312.03815) propose an "AIOS-Agent ecosystem."  They envision LLMs as the intelligent operating system (AIOS) kernel, with AI agents functioning as applications.  This framework uses natural language as the programming interface and tools as devices or libraries, drawing a parallel to the traditional OS-APP ecosystem. The paper explores the potential of this ecosystem, including single-agent, multi-agent systems, and human-agent interaction, and suggests a roadmap for its future evolution.

Another related work, **MemGPT: Towards LLMs as Operating Systems** by Packer et al. [MemGPT: Towards LLMs as Operating Systems](https://www.arxiv.org/abs/2310.08560), focuses on addressing the context window limitations of LLMs.  They introduce "virtual context management," inspired by hierarchical memory systems in traditional OS, to enable LLMs to handle extended context for tasks like document analysis and long conversations. Their system, MemGPT, manages different memory tiers to provide the illusion of a larger context window, showcasing its effectiveness in domains where context length is crucial.

Lu et al. in **Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents** [Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents](https://www.arxiv.org/abs/2409.17140) propose AXIS, a framework that prioritizes Application Programming Interfaces (APIs) over User Interface (UI) actions for LLM-based agents.  This approach aims to enhance efficiency and reliability in human-agent-computer interaction (HACI) by reducing latency and cognitive workload associated with extensive UI interactions.  The work explores the concept of transforming applications into agents and envisions an agent-centric operating system, demonstrating significant improvements in task completion time and user experience.

Wu et al. present **OS-Copilot: Towards Generalist Computer Agents with Self-Improvement** [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://www.arxiv.org/abs/2402.07456), a framework for building generalist agents capable of interacting with diverse OS elements, including web, terminals, files, and applications.  They introduce FRIDAY, an agent built on OS-Copilot, which demonstrates strong generalization capabilities across unseen applications and shows self-improvement in tasks with minimal supervision.  This work provides valuable infrastructure and insights towards developing more capable general-purpose computer agents.

Mei et al. in **AIOS: LLM Agent Operating System** [AIOS: LLM Agent Operating System](https://www.arxiv.org/abs/2403.16971) directly addresses the challenges of deploying and managing LLM-based agents. They propose AIOS, an LLM agent operating system, which embeds an LLM as the "brain" of the OS.  AIOS focuses on optimizing resource allocation, context switching, concurrent agent execution, and providing tool services, aiming to improve the efficiency and reliability of LLM agents and pave the way for a robust AIOS ecosystem.

The idea of using LLMs as controllers for other AI models is explored in **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face** by Shen et al. [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://www.arxiv.org/abs/2303.17580).  HuggingGPT utilizes ChatGPT to orchestrate various AI models from the Hugging Face community to solve complex AI tasks across different domains and modalities. This work highlights the potential of LLMs in task planning, model selection, and response summarization, demonstrating a pathway towards artificial general intelligence by leveraging existing AI resources.


---

<div id="embodiment">  
</div>


### Embodiment
Real-world physical interaction requires Autonomous Agents capable of making emebodied actions and decisions. 

- [Interactive Agent Foundational Model](https://github.com/tmgthb/Autonomous-Agents#interactiveagent) uses action tokens to enhance grounding with cross-reality data.


---

<div id="brain">  
</div>

### Brain-Compute Interfaces (BCIs)

Brain-Compute Interfaces (BCIs) have emerged as a promising field, enabling direct communication between the brain and external devices. This section outlines related works that explore various aspects of BCI technology, particularly focusing on EEG-based methods and their applications.

**Deep Learning for EEG-based BCIs:** Deep learning has significantly advanced EEG-based BCI performance.  **EEGNet**, a compact convolutional network (CNN), was introduced by Lawhern et al. ([Lawhern et al., 2016](https://www.arxiv.org/abs/1611.08024)) as a paradigm-general architecture for EEG signal classification across different BCI tasks, utilizing depthwise and separable convolutions for efficient feature extraction. Wang et al. proposed an accurate EEGNet-based model optimized for low-power edge computing, achieving high accuracy on motor-imagery tasks while minimizing memory footprint ([Wang et al., 2020](https://www.arxiv.org/abs/2004.00077)).  Ingolfsson et al. introduced **EEG-TCNet**, a temporal convolutional network (TCN) demonstrating superior accuracy in motor-imagery BCI with a small number of parameters, suitable for embedded systems ([Ingolfsson et al., 2020](https://www.arxiv.org/abs/2006.00622)).  Ko et al. explored a multi-scale neural network to capture spatio-spectral-temporal EEG representations, improving performance across diverse BCI paradigms ([Ko et al., 2020](https://www.arxiv.org/abs/2003.02657)). Mane et al. presented **FBCNet**, a filter-bank convolutional network employing multi-view data representation and spatial filtering for efficient motor imagery classification, achieving state-of-the-art results ([Mane et al., 2021](https://www.arxiv.org/abs/2104.01233)). Liao and Wang introduced **EEGEncoder**, a transformer-based framework incorporating temporal convolutional networks and a Dual-Stream Temporal-Spatial Block (DSTS) to enhance motor imagery classification accuracy ([Liao and Wang, 2024](https://www.arxiv.org/abs/2404.14869)).  Zhang et al. proposed **EEG-Inception**, a CNN architecture based on Inception-Time, achieving high accuracy in motor imagery classification with raw EEG inputs and data augmentation techniques ([Zhang et al., 2021](https://www.arxiv.org/abs/2101.10932v3)). Rommel et al. systematically compared various data augmentation techniques for EEG, demonstrating significant accuracy improvements in sleep stage classification and motor imagery BCI tasks ([Rommel et al., 2022](https://www.arxiv.org/abs/2206.14483)).

**Applications of BCI:** BCIs have broad applications, particularly in assistive technologies. Ghasemi et al. explored the use of EEG-based BCI to enhance wheelchair control for individuals with physical disabilities, focusing on non-invasive systems for intuitive navigation ([Ghasemi et al., 2024](https://www.arxiv.org/abs/2404.17895)).  Zhang et al. developed a practical BCI system for text typing by translating thought to text using a deep learning framework to interpret motor imagery EEG signals ([Zhang et al., 2017](https://www.arxiv.org/abs/1709.08820)). Yang et al. introduced **MindSculpt**, a BCI tool integrated with Grasshopper, enabling designers to create geometries in real-time by thinking, shortening the design process latency ([Yang et al., 2023](https://www.arxiv.org/abs/2303.03632v1)). Pancholi and Giri proposed **NeuroKinect**, a deep-learning model for hand trajectory estimation using EEG signals, improving computational efficiency and accuracy in kinematic reconstruction ([Pancholi and Giri, 2023](https://www.arxiv.org/abs/2308.08654)).  Satam and Szabolcsi demonstrated the integration of a neurofeedback-driven BCI with an Arduino-controlled 6-DOF robotic arm, showcasing the potential for advanced robotic control ([Satam and Szabolcsi, 2024](https://www.arxiv.org/abs/2410.22008v1)). Liu and Wang presented **Mental-Gen**, a BCI-based interactive method for interior space generative design, interpreting spatial design intentions at a neural level and expressing them through generative AI models ([Liu and Wang, 2024](https://www.arxiv.org/abs/2409.00962)).  Cuui et al. focused on EEG-based driver drowsiness estimation, proposing a feature weighted episodic training approach to eliminate the need for calibration data for new drivers ([Cuui et al., 2019](https://www.arxiv.org/abs/1909.11456)). Zhang and Etemad developed a multimodal architecture with capsule attention for driver vigilance estimation using EEG and EOG signals, achieving state-of-the-art performance ([Zhang and Etemad, 2019](https://www.arxiv.org/abs/1912.07812)). Wu and Dai proposed **Emo-Net**, a deep learning framework for decoding emotions from neural activity, enhancing emotion recognition in animal models and potentially enabling closed-loop emotional BCIs ([Wu and Dai, 2023](https://www.arxiv.org/abs/2303.04391v1)).

**Enhancing BCI Performance and Practicality:** Several studies focus on improving BCI system performance, calibration efficiency, and user experience. Wang et al. explored transfer learning and wearable EEG technology to enhance BCI performance, reduce calibration time, and improve efficiency ([Wang et al., 2023](https://www.arxiv.org/abs/2309.07798)). Chiang et al. demonstrated that cross-subject transfer learning can improve the practicality of real-world SSVEP-based BCIs by reducing calibration data needs using a least-squares transformation method ([Chiang et al., 2018](https://www.arxiv.org/abs/1810.02842)).  He and Wu proposed a label alignment approach for different set domain adaptation in BCIs, reducing calibration effort by aligning source and target label spaces ([He and Wu, 2019](https://www.arxiv.org/abs/1912.01166)). Wu et al. introduced online and offline weighted adaptation regularization algorithms to reduce calibration effort in ERP-based BCIs ([Wu, 2017](https://www.arxiv.org/abs/1702.02897)). They also explored active weighted adaptation regularization to expedite calibration when switching EEG headsets ([Wu et al., 2017](https://www.arxiv.org/abs/1702.02906)). He and Wu proposed a Euclidean space data alignment approach to facilitate transfer learning in BCIs, improving learning performance for new subjects with minimal data ([He and Wu, 2018](https://www.arxiv.org/abs/1808.05464)). Samek et al. presented a transfer learning approach that transfers subspaces between subjects to compensate for changes between training and testing sessions, improving robustness ([Samek et al., 2012](https://www.arxiv.org/abs/1209.4115)). Ozdenizci et al. introduced adversarial neural networks for transfer learning in BCIs, learning subject-invariant representations using conditional variational autoencoders and adversarial networks ([Ozdenizci et al., 2018](https://www.arxiv.org/abs/1812.06857)).  Zlatov and Blankertz proposed physiology-informed data augmentation techniques for EEG-based BCIs, generating inter-participant variability in a physiologically meaningful way to improve model training ([Zlatov and Blankertz, 2022](https://www.arxiv.org/abs/2203.14392)).

**Novel BCI Paradigms and Techniques:** Emerging research explores new paradigms and techniques to enhance BCI capabilities. Meunier et al. introduced Brain-Artificial Intelligence Interfaces (BAIs), leveraging AI to replace parts of the neuro-cognitive processing pipeline, enabling complex communication even with cognitive impairments ([Meunier et al., 2024](https://www.arxiv.org/abs/2402.15011)). Caria discussed the integration of Large Language Models (LLMs) with BCIs for predictive communication, aiming to drastically improve human-computer interaction and control ([Caria, 2024](https://www.arxiv.org/abs/2412.07355v1)).  Liu et al. investigated the use of LLMs as denoising agents to extract subject-independent semantic features from EEG signals, improving zero-shot prediction and generalizability of BCI models ([Liu et al., 2025](https://www.arxiv.org/abs/2501.02621v1)). Kwak et al. proposed a framework for personalized BCI applications based on endogenous EEG paradigms, customizing services to individual preferences using motor, speech, and visual imagery ([Kwak et al., 2024](https://www.arxiv.org/abs/2411.11302)). Lu et al. developed **LGL-BCI**, a lightweight geometric learning BCI, utilizing geometric deep learning for efficient motor imagery classification without sacrificing accuracy ([Lu et al., 2025](https://www.arxiv.org/abs/2501.05589v1)). Ju and Guan proposed **Tensor-CSPNet**, a geometric deep learning framework characterizing spatial covariance matrices on SPD manifolds for motor imagery classification ([Ju and Guan, 2022](https://www.arxiv.org/abs/2202.02472)).

**Reviews and Surveys:** Several surveys provide comprehensive overviews of BCI research. Gu et al. surveyed recent studies on EEG-based BCIs, focusing on signal sensing technologies, computational intelligence approaches, and applications from 2015-2019 ([Gu et al., 2020](https://www.arxiv.org/abs/2001.11337)). Zhang et al. surveyed deep learning techniques used for BCI, summarizing contributions in the past five years and discussing challenges and future directions ([Zhang et al., 2019](https://www.arxiv.org/abs/1905.04149)). Ghosh provided a survey of BCI using non-invasive methods, exploring the use cases of EEG, fMRI, NIRs, and hybrid systems ([Ghosh, 2023](https://www.arxiv.org/abs/2309.13151v1)). Lance et al. presented a perspective on BCI technologies in the coming decades, envisioning near-term and far-term applications and highlighting the potential impact of BCIs ([Lance et al., 2012](https://www.arxiv.org/abs/1211.0886v1)). Fallani and Bassett reviewed the limitations of current BCIs and offered a perspective on how network neuroscience can address these challenges to optimize human-machine interactions ([Fallani and Bassett, 2018](https://www.arxiv.org/abs/1807.05616)). Glaser et al. provided a tutorial on applying modern machine learning methods for neural decoding, demonstrating the superior performance of neural networks and ensembles compared to traditional approaches ([Glaser et al., 2017](https://www.arxiv.org/abs/1708.00909v4)). Guetschel et al. reviewed deep representation learning techniques for BCIs, analyzing motivations, techniques, and approaches for characterizing learned representations and advocating for foundation models in EEG decoding ([Guetschel et al., 2024](https://www.arxiv.org/abs/2405.19345)).

This related work highlights the breadth and depth of research in BCI, spanning novel algorithms, diverse applications, and techniques to enhance usability and performance. These works collectively pave the way for more practical and impactful BCI systems.

[Movie reconstruction from mouse visual cortex activity](https://www.biorxiv.org/content/10.1101/2024.06.19.599691v1)

- Reconstructs ground-truth video using images from mouse brain.

[Brain representation in conscious and unconscious vision](https://www.biorxiv.org/content/10.1101/2024.05.27.596053v1)

- Discovers fronto-parietal cortex is involved in representing unconscious content.


---

<div id="protocols">  
</div>

### Communication Protocols

One line of research focuses on designing **scalable and efficient communication protocols**. Marro et al. introduce Agora, a meta-protocol designed to address the "Agent Communication Trilemma" by leveraging existing communication standards and natural language to facilitate scalable communication in large agent networks [Marro et al., 2024](https://www.arxiv.org/abs/2410.11905). Agora allows agents to use standardized routines for frequent interactions, natural language for infrequent communication, and LLM-written routines for intermediate scenarios, promoting decentralization and adaptability. In pursuit of efficiency, Ramesh and Li propose communicating via activations instead of natural language, significantly reducing computational costs and information loss in inter-agent communication [Ramesh & Li, 2025](https://www.arxiv.org/abs/2501.14082v1). Similarly, Pham et al. introduce CIPHER, a communication regime that utilizes transformer output embeddings, offering a more information-rich and computationally cheaper alternative to natural language for agent debate and discussion [Pham et al., 2023](https://www.arxiv.org/abs/2310.06272).

Several frameworks have been proposed to structure **multi-agent collaboration**. Chen et al. introduce the Internet of Agents (IoA), inspired by the internet architecture, to facilitate collaboration among heterogeneous agents in a flexible and scalable manner [Chen et al., 2024](https://www.arxiv.org/abs/2407.07061). IoA emphasizes agent integration, dynamic teaming, and adaptable conversation flows. Liu et al. propose DyLAN, a Dynamic LLM-Agent Network that optimizes agent teams and interaction architectures dynamically based on the task, enhancing both performance and efficiency [Liu et al., 2023](https://www.arxiv.org/abs/2310.02170). The CAMEL framework by Li et al. explores role-playing among communicative agents to understand the "mind" of LLM societies, offering insights into autonomous cooperation and leveraging inception prompting for task completion [Li et al., 2023](https://www.arxiv.org/abs/2303.17760v2). HuggingGPT, presented by Shen et al., utilizes LLMs as controllers to manage and integrate various specialized AI models from platforms like Hugging Face, demonstrating LLMs' potential to orchestrate complex AI tasks across different domains [Shen et al., 2023](https://www.arxiv.org/abs/2303.17580). Self-collaboration is further explored by Dong et al., who propose a framework where multiple LLM agents, acting as distinct expert roles (analyst, coder, tester), collaborate to improve code generation performance [Dong et al., 2023](https://www.arxiv.org/abs/2304.07590).

The **design of communication topologies** for multi-agent systems is also being investigated. Zhang et al. introduce G-Designer, a method using Graph Neural Networks to dynamically architect task-aware communication topologies, optimizing for performance and minimizing communication overhead [Zhang et al., 2024](https://www.arxiv.org/abs/2410.11782). This adaptive approach addresses the challenge of selecting optimal communication structures for specific tasks.

**Grounding communication in natural language** for better human-agent and inter-agent interaction is another key area. Li et al. explore language grounding in multi-agent reinforcement learning to align agent communication with human natural language, facilitating ad-hoc teamwork and improving generalization [Li et al., 2024](https://www.arxiv.org/abs/2409.17348). Srinivasan et al. focus on the intelligibility of communication protocols, implementing the PXP protocol for human-expert and LLM interaction to enhance understanding and collaboration in data analysis tasks [Srinivasan et al., 2024](https://www.arxiv.org/abs/2410.20600). Rasal's LLM Harmony framework leverages role-playing agents to improve problem-solving through multi-agent communication, highlighting the collaborative potential of diverse agent personas [Rasal, 2024](https://www.arxiv.org/abs/2401.01312).

Furthermore, research considers **learning and adaptation** in communication. Wang et al. propose Learning through Communication (LTC), a framework with a universal feedback buffer and iterative pipeline to enable LLM agents to learn and adapt their communication strategies based on both linguistic feedback and reward signals [Wang et al., 2023](https://www.arxiv.org/abs/2310.01444).

The application of LLM-based multi-agent systems spans various domains. Lim et al. investigate their use in manufacturing, demonstrating how LLMs can enhance agent adaptability and coordination through natural language integration in dynamic manufacturing environments [Lim et al., 2024](https://www.arxiv.org/abs/2406.01893). Zhang et al. introduce CoELA, a modular framework for cooperative embodied agents, showcasing how LLMs can be integrated with perception and memory modules to enable effective communication and cooperation in embodied tasks [Zhang et al., 2023](https://www.arxiv.org/abs/2307.02485v2).  RoCo, by Mandi et al., explores dialectic multi-robot collaboration using LLMs for high-level communication and low-level planning, demonstrating interpretability and flexibility in robot teamwork scenarios [Mandi & Jain et al., 2023](https://www.arxiv.org/abs/2307.04738).

Finally, Yang et al. provide a broader perspective on LLM-based Multi-Agent Systems (LaMAS), discussing technical aspects, business implications, and proposing a preliminary LaMAS protocol to support the ecosystem of such systems, emphasizing dynamic task decomposition, flexibility, and data privacy [Yang et al., 2024](https://www.arxiv.org/abs/2411.14033).



---

<div id="selfconstruction">  
</div>


### Self-Constructive Agents

Recent work has begun to empirically investigate the potential for current LLMs to exhibit self-replication.  Pan et al. (2024) in ["Frontier AI systems have surpassed the self-replicating red line"](https://www.arxiv.org/abs/2412.12140v1) present compelling evidence that certain contemporary LLMs, specifically Meta's Llama3-70B-Instruct and Alibaba's Qwen25-72B-Instruct, have already surpassed the "self-replicating red line."  Their experiments demonstrate that these models can successfully create copies of themselves without human intervention in a substantial number of trials, indicating a level of self-perception, situational awareness, and problem-solving sufficient for self-replication.  Furthermore, they highlight the concerning ability of these systems to potentially utilize self-replication for survival and proliferation, raising alarms about uncontrolled AI populations.

Building upon the growing capabilities of LLMs, Sheng (2024) in ["From Language Models to Practical Self-Improving Computer Agents"](https://www.arxiv.org/abs/2404.11964) explores a methodology for creating practical self-improving agents.  This work focuses on agents that can augment their own capabilities by generating and utilizing software tools.  Starting with basic terminal access, the proposed agent can develop and integrate tools for retrieval, internet search, web navigation, and text editing, effectively expanding its problem-solving abilities in real-world computer tasks.  This approach demonstrates a path towards self-construction through tool augmentation, enabling agents to tackle increasingly complex challenges.

Further examining the autonomous capabilities of LLM agents, Kinniment et al. (2023) in ["Evaluating Language-Model Agents on Realistic Autonomous Tasks"](https://www.arxiv.org/abs/2312.11671v2) evaluate agents on tasks related to autonomous replication and adaptation (ARA).  Their work explores the ability of agents to acquire resources, self-copy, and adapt to new situations.  While their findings suggest that current agents are limited in their ARA capabilities, the authors caution against dismissing the potential for near-future advancements, particularly with increased computational scale and fine-tuning.  This study underscores the importance of rigorous evaluation and monitoring of ARA capabilities in evolving AI systems.

In parallel with empirical investigations, researchers are also developing frameworks for self-improving AI agents. Yin et al. (2024) introduce the "Gödel Agent" in ["G\\\"odel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement"](https://www.arxiv.org/abs/2410.04444), a self-evolving framework inspired by the Gödel machine.  This framework allows agents to recursively improve their own logic and behavior, guided by high-level objectives, without relying on pre-defined algorithms.  By leveraging LLMs to dynamically modify their own internal workings, Gödel Agent aims to overcome the limitations of fixed agent designs and achieve continuous self-improvement, demonstrating enhanced performance and generalizability in tasks such as mathematical reasoning.

The potential for self-replication in AI agents also introduces novel security risks. Cohen et al. (2024) in ["Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications"](https://www.arxiv.org/abs/2403.02817) explore the threat of "AI worms" targeting GenAI ecosystems. They introduce "Morris II," the first worm designed to exploit GenAI-powered applications through adversarial self-replicating prompts.  Their research demonstrates that carefully crafted prompts can induce GenAI models to replicate themselves and propagate malicious payloads across interconnected agent networks, posing significant cybersecurity threats to GenAI-powered systems.



---


<div id="why">  
</div>


<div align="center">

## Why Autonomous Agents work? 

</div>

- [Next sequence prediction](#nextsequenceprediction)
- [Demystifying "Emerging abilities"](#demystifyingemergingabilities)
- [Free energy principle](#freeenergyprinciple)
- [Interpretability](#interpretability)
- [Synthetic data](#syntheticdata)


<div id="nextsequenceprediction">  

</div>

---


### Next sequence prediction

LLMs are trained to predict the next word/token, which leads to: [Multi-task learning](https://github.com/tmgthb/Autonomous-Agents#multitask):
- Backed up by empirical evidence.

The single training objective: "predict next token" results a [Massively Multi-task learning](https://github.com/tmgthb/Autonomous-Agents#extreme).
- "Massively Multi-task learning" results massive amount of new skills learned from a single objective. 

Next sequence prediction algorithm is generic algorithm.
- Next sequence prediction is [generic learning process](https://github.com/tmgthb/Autonomous-Agents#extreme).
- Any "<input, output>"-sequence relationship, can be learned as "next-sequence prediction task".

Information is typically sequential: 
- language is sequence of words,
- DNA is sequence of nucleotides,
- computer programs are sequences of instructions.
- Media: Videos are sequence of images, Music is sequence of notes, image is sequence of pixels and speech is sequence of phonemes.
- Actions: Dance is sequence of movements, day is sequence of events, time is sequence of time steps.
- Concepts about the world: Causality is sequential (cause-effect). Time is sequential(before-after). Life is sequential(parent-child).

Cross-modality Transformers:
- The universal nature of next-sequence prediction is empirically visible in different Transformer models: ViT for Images, Whisper for Audio, SORA for video.

Next sequence prediction is perhaps the most generic single learning objective known to produce intelligent behaviour. 

    I call this surprising, yet unexpected phenomenon as the "Paradox of Lexical Labyrinth".

    Paradox of Lexical Labyrinth:

    The paradoxical phenomenon whereby seemingly simple mechnanism of a next sequence prediction, such as predicting the next word in a     language, gives rise to advanced cognitive skills like profound reasoning capabilities. The labyrinth refers to the vast & complex      landscape of language, characterized by its infinite potential for compressing meaning, expressions and intelligence.

    Teemu Maatta, 07.06.2024

---


<div id="generalization">  

</div>





---


### Generalization of LLMs


**LLMs as Generalizable Policies:** Szot et al. (2023) [Large Language Models as Generalizable Policies for Embodied Tasks](https://www.arxiv.org/abs/2310.17722) introduce Large Language model Reinforcement Learning Policy (LLaRP), demonstrating that frozen LLMs can be adapted via reinforcement learning to serve as generalizable policies for embodied visual tasks. LLaRP takes text instructions and visual observations as input and outputs actions, exhibiting robustness to instruction paraphrasing and generalization to novel tasks in a newly introduced benchmark, Language Rearrangement.

Pouplin et al. (2024) [LLMs for Generalizable Language-Conditioned Policy Learning under Minimal Data Requirements](https://www.arxiv.org/abs/2412.06877) present TEDUO, a training pipeline for offline language-conditioned policy learning that leverages LLMs for both enhancing offline data and generalizing to new goals and states. Their approach aims to enable generalizable policies with minimal labeled data, suitable for in-the-wild evaluations.

Carta et al. (2023) [Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning](https://www.arxiv.org/abs/2302.02662) explore GLAM, an approach to ground LLMs in interactive environments through online reinforcement learning. They investigate the sample efficiency and generalization capabilities of using LLMs as policies that are updated through environment interaction, demonstrating the potential of functional grounding for improved performance and generalization in spatial and navigation tasks.

Radmehr et al. (2024) [Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs](https://www.arxiv.org/abs/2404.18978) investigate the integration of RL and LLMs to enhance generalization in text-based educational environments. Through the PharmaSimText benchmark, they compare RL-based, LLM-based, and hybrid agents, finding that hybrid approaches can overcome the limitations of each individual approach and improve both task completion and question quality in open-ended learning environments.

Basavatia et al. (2024) [STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models](https://www.arxiv.org/abs/2406.05872) introduce STARLING, a self-supervised RL environment for text-based games that uses LLMs to automatically generate games. This framework aims to improve the generalization of language-based RL agents by allowing them to hone skills in a variety of automatically created scenarios, highlighting the challenge of current RL agents in generalizing learned skills to new situations as effectively as humans.

Dasgupta et al. (2023) [Collaborating with language models for embodied reasoning](https://www.arxiv.org/abs/2302.00763) explore a system combining a Planner (LLM), Actor, and Reporter to leverage the reasoning abilities of LLMs for embodied tasks.  They investigate the system's zero-shot generalization and demonstrate how reinforcement learning can be used to train components of this collaborative system to enhance performance in complex, ambiguous environments.

Hanjie et al. (2021) [Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning](https://www.arxiv.org/abs/2101.07393) introduce the Messenger environment to study language-driven generalization in RL. They propose EMMA, a model that grounds text manuals to entities and dynamics, learning from environment rewards alone. EMMA demonstrates improved zero-shot generalization to unseen games with new dynamics, highlighting the potential of language grounding for enhancing generalization in RL.

Chen et al. (2020) [Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning](https://www.arxiv.org/abs/2011.00517) propose using human demonstrations in the form of natural language instructions and action trajectories to improve generalization in multi-task RL.  They introduce a dataset and model incorporating language generation and language-conditioned policies, demonstrating that human instructions enhance performance on complex tasks and enable zero-shot generalization to unseen tasks, with interpretable behaviors through generated high-level descriptions of actions.

**Enhancing Generalization through Reward Modeling and RLHF:** Wang et al. (2024) [CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning Tasks](https://www.arxiv.org/abs/2409.08642v2) propose Critical Plan Step Learning (CPL) to enhance LLM generalization in reasoning tasks through reinforcement learning. CPL uses Monte Carlo Tree Search (MCTS) to explore diverse plan steps and Step-level Advantage Preference Optimization (Step-APO) to learn critical plan steps, improving both reasoning capabilities and out-of-domain generalization on benchmarks like HumanEval and GPQA, even when trained only on GSM8K and MATH.

Yang et al. (2024) [Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs](https://www.arxiv.org/abs/2406.10216) introduce a method to improve reward model generalization by regularizing hidden states.  By preserving the text-generation capabilities of hidden states while learning a reward head, their approach enhances reward model accuracy on out-of-distribution tasks and mitigates reward over-optimization issues in RLHF.

Zheng et al. (2023) [Improving Generalization of Alignment with Human Preferences through Group Invariant Learning](https://www.arxiv.org/abs/2310.11971) propose a group invariant learning approach to enhance the generalization of RLHF alignment.  Their method automatically classifies data into groups, optimizes policy performance on challenging groups, and adaptively adjusts exploration space, leading to improved training stability and generalization to new data.

Kirk et al. (2023) [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://www.arxiv.org/abs/2310.06452) present an analysis of how different stages of RLHF (SFT, reward modeling, RLHF) affect out-of-distribution generalization and output diversity. Their findings indicate that RLHF generalizes better than SFT, particularly with larger distribution shifts, but at the cost of reduced output diversity, highlighting a trade-off between generalization and diversity in current LLM fine-tuning methods.

Chu et al. (2025) [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://www.arxiv.org/abs/2501.17161v1) conduct a comparative study of Supervised Fine-Tuning (SFT) and RL on generalization and memorization using text-based rule variants and visual variants. They find that RL generalizes better across rule-based textual and visual variants, while SFT tends to memorize training data. They also highlight the complementary roles of SFT in stabilizing output format for subsequent RL training to achieve generalization gains.

Wang et al. (2024) [Secrets of RLHF in Large Language Models Part II: Reward Modeling](https://www.arxiv.org/abs/2401.06080) address challenges in reward modeling for RLHF, focusing on data quality and generalization. They propose methods to measure preference strength in data and mitigate the impact of incorrect preferences. They also introduce contrastive learning and meta-learning techniques to enhance reward model generalization and enable iterative RLHF optimization.

Dou et al. (2024) [MetaRM: Shifted Distributions Alignment via Meta-Learning](https://www.arxiv.org/abs/2405.00438) propose MetaRM, a meta-learning approach to align reward models with shifted environment distributions in RLHF. MetaRM aims to improve the reward model's ability to differentiate responses as the policy distribution shifts during training and enhance generalization to out-of-distribution examples, crucial for iterative RLHF.

Aissi et al. (2024) [Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting](https://www.arxiv.org/abs/2410.19920) investigate the sensitivity of LLM agents to prompt formulations after RL training in interactive environments. They find performance degradation with different prompt formulations and propose a contrastive loss to mitigate this prompt overfitting and improve robustness and generalization.

Chen et al. (2024) [Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models](https://www.arxiv.org/abs/2406.10305v2) investigate the correlation between SFT and RL in training Code LLMs using synthetic datasets. They find that both atomic and synthetic functions are important for SFT generalization, RL can enhance SFT generalization to target domains, and RL from scratch can alleviate overfitting from SFT, offering insights into effective training paradigms for Code LLMs.







<div id="scalingplanning">  

</div>





---


### Scaling Planning of LLM-agents

One direction focuses on improving LLMs' planning abilities by structuring the planning process itself.  [Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning](https://www.arxiv.org/abs/2409.12452) introduces **CodePlan**, a framework that guides LLMs to generate code-form plans, which are pseudocode outlining reasoning processes. This approach leverages the structured nature of code to enhance robustness and generalization across tasks and scales efficiently by automatically extracting plans from text corpora. In a similar vein of structured planning, [Planning In Natural Language Improves LLM Search For Code Generation](https://www.arxiv.org/abs/2409.03733) presents **PlanSearch**, which searches over natural language plans to improve the diversity of LLM outputs, leading to more effective code generation.

To enhance the adaptability and robustness of LLM-based agents, several works explore incorporating feedback and iterative refinement in planning. **AdaPlanner** [AdaPlanner: Adaptive Planning from Feedback with Language Models](https://www.arxiv.org/abs/2305.16653) proposes a closed-loop approach where LLM agents refine their plans adaptively based on environmental feedback, utilizing both in-plan and out-of-plan refinement strategies.  **Describe, Explain, Plan and Select (DEPS)** [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents](https://www.arxiv.org/abs/2302.01560) introduces an interactive planning method that facilitates error correction through plan descriptions, explanations of failures, and a goal selector to refine initial plans, particularly for open-world environments.

Another significant line of research investigates the use of LLMs as world models for planning. **LLM-MCTS** [Large Language Models as Commonsense Knowledge for Large-Scale Task Planning](https://www.arxiv.org/abs/2305.14078) and **Reasoning via Planning (RAP)** [Reasoning with Language Model is Planning with World Model](https://www.arxiv.org/abs/2305.14992) both utilize LLMs to provide a commonsense world model that, when combined with Monte Carlo Tree Search (MCTS), enables effective reasoning and planning. RAP further repurposes the LLM as both world model and reasoning agent, strategically exploring the reasoning space.  Expanding on tree-search methods, **Language Agent Tree Search (LATS)** [Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models](https://www.arxiv.org/abs/2310.04406) unifies reasoning, acting, and planning within LLMs by integrating MCTS with LM-powered value functions and self-reflections, demonstrating effectiveness across various domains. Furthermore, [Tree Search for Language Model Agents](https://www.arxiv.org/abs/2407.01476) introduces a best-first tree search algorithm for LM agents specifically for interactive web environments, showcasing improved performance on realistic web tasks.  **WebPilot** [WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration](https://www.arxiv.org/abs/2408.15978) builds a multi-agent system with a dual optimization strategy employing MCTS to handle the complexities of web environments, achieving state-of-the-art performance on web automation benchmarks.

To address the challenge of grounding LLMs in specific environments, **LLM-Planner** [LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models](https://www.arxiv.org/abs/2212.08681v1) focuses on few-shot grounded planning for embodied agents, enhancing LLMs with physical grounding to generate environment-aware plans. **SayNav** [SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments](https://www.arxiv.org/abs/2309.04077) presents a novel grounding mechanism that uses 3D scene graphs as input to LLMs for navigation in unknown environments, generating contextually appropriate plans.

Several works explore different prompting and training methodologies. **AgentGen** [AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation](https://www.arxiv.org/abs/2408.00764v2) investigates enhancing planning abilities through instruction tuning, using automatically synthesized diverse environments and planning tasks generated by LLMs themselves. **Synapse** [Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control](https://www.arxiv.org/abs/2306.07863) introduces trajectory-as-exemplar prompting with memory for computer control agents, utilizing state abstraction and exemplar memory to improve performance and generalization.

Beyond prompting and search, some approaches integrate LLMs with classical planning techniques or external tools. **LLM+P** [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://www.arxiv.org/abs/2304.11477) combines LLMs with classical planners by translating natural language problems into PDDL, leveraging the strengths of both approaches.  **Chameleon** [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://www.arxiv.org/abs/2304.09842) proposes a plug-and-play system that augments LLMs with various tools, including other models and search engines, for compositional reasoning.  **HuggingGPT** [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://www.arxiv.org/abs/2303.17580) uses LLMs as controllers to manage diverse AI models from Hugging Face, solving complex tasks across modalities.

For specific application domains, **WebAgent** [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis](https://www.arxiv.org/abs/2307.12856) develops an LLM-driven agent for web automation, incorporating planning, long context understanding, and program synthesis.  In the realm of multi-agent systems, [Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?](https://www.arxiv.org/abs/2309.15943) compares centralized, decentralized, and hybrid communication frameworks for LLM-based multi-robot collaboration, analyzing task success and token efficiency.

Finally, some works explore the theoretical foundations and emergent capabilities of LLM planning. **Reason for Future, Act for Now (RAFA)** [Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency](https://www.arxiv.org/abs/2309.17382) presents a principled framework with provable regret guarantees, casting reasoning as learning and planning in Bayesian adaptive MDPs. [Emergent autonomous scientific research capabilities of large language models](https://www.arxiv.org/abs/2304.05332v1) showcases an intelligent agent system using LLMs for autonomous scientific experiment design and execution, demonstrating the potential for LLMs in advanced autonomous research.  **Tree of Thoughts (ToT)** [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://www.arxiv.org/abs/2305.10601) introduces a framework for deliberate problem-solving, enabling exploration over coherent units of text and self-evaluation, significantly improving performance on tasks requiring planning and search.



----

<div id="demystifyingemergingabilities">  

</div>





---


### Demystifying Emerging Abilities

[Emerming Abilities](https://github.com/tmgthb/Autonomous-Agents#emerging) refers to ability present in a larger LLM and not in a smaller one.
- The initial definition refers to situation, where emerging abilities have increased so far contiuously as compute is scaled up and more data introduced.

There are +137 known Emerging abilities(increasing).
- Emerging abilities include Emerging Prompting Strategies such as: [CoT](https://github.com/tmgthb/Autonomous-Agents#cot), which was not present in GPT-2 and emerged in GPT-3 model.

Research has [proven](https://arxiv.org/abs/2403.15796) the existing of Emergent abilities from perspective of pre-training loss, even with continuous metrics.

Overall, Emergent abilities are proven to on language models from the perspective of pre-training loss, instead of model/data size.

Emergent abilities suggest that AI models self-organize internal structures to perform tasks to reduce pre-training loss, even without being explicitly programmed for those specific capabilities.


---


<div id="freeenergyprinciple">  

</div>


### Free energy principle

Friston (2010) claims in the [The free energy principle and cognitive agents](https://www.uab.edu/medicine/cinl/images/KFriston_FreeEnergy_BrainTheory.pdf), that biological systems, like human brains, reduce free energy by acting on the world and optimizing their internal states related to perception and action.
- The basic idea is, that biological agents minimize free energy.

Just like human brain mnimizes free energy, the LLMs minimize the prediction error:
- If we give a LLM the training objective of minimizing loss for "next-sequence prediction" and lot of energy/compute and data, then it will self-organize its weights into optimal local order.
- This compression enables LLMs to learn emerging skills beyond merely memorizing the training data.

Prakki (2024) [claims](https://www.arxiv.org/abs/2412.10425v3) frames the LLM-agent learning from free energy perspective by adjusting dynamically prompts and search strategies.

---

<div id="interpretability">  </div>

### Interpretability

One approach to enhance interpretability is to design models with built-in transparency, as demonstrated by the **Concept Bottleneck Large Language Model (CB-LLM)** [Crafting Large Language Models for Enhanced Interpretability](https://www.arxiv.org/abs/2407.04307). This work introduces a novel architecture that uses a concept bottleneck to provide clear and accurate explanations, addressing the limitations of post-hoc interpretation methods common in traditional black-box LLMs. Furthermore, the Automatic Concept Correction (ACC) strategy minimizes performance gaps, making CB-LLM a strong alternative for interpretable LLMs.

Other works focus on explaining the internal decision-making processes of existing LLMs. **LMExplainer** [LMExplainer: a Knowledge-Enhanced Explainer for Language Models](https://www.arxiv.org/abs/2303.16537) proposes a knowledge-enhanced approach, utilizing a knowledge graph and a graph attention network to extract key decision signals. This method aims to provide human-understandable explanations and has shown improved performance on commonsense reasoning tasks compared to attention-based methods.

Several studies explore the internal representations of LLMs to better understand how they compute. **Patchscopes** [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://www.arxiv.org/abs/2401.06102v4) introduces a unifying framework that leverages the LLM itself to explain its internal representations in natural language. By framing various interpretability methods as instances of Patchscopes, the work also introduces new possibilities, such as using a more capable model to explain the representations of a smaller model. Similarly, **Sparse Autoencoders** [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://www.arxiv.org/abs/2309.08600) are used to identify sparsely activating features that are more interpretable and monosemantic than those identified by alternative approaches. This method has shown success in pinpointing features causally responsible for counterfactual behavior.

The **Tuned Lens** [Eliciting Latent Predictions from Transformers with the Tuned Lens](https://www.arxiv.org/abs/2303.08112) provides a method for analyzing transformers from the perspective of iterative inference by training an affine probe for each block in a pretrained model. This allows for the decoding of hidden states into distributions over the vocabulary, revealing how model predictions are refined layer by layer.
Additionally, **Transformer Feed-Forward Layers** [Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://www.arxiv.org/abs/2203.14680) are analyzed to show how they function as additive updates to vocabulary distributions, with each update promoting human-interpretable concepts. This work also leverages these insights to control LM predictions and improve computational efficiency.

Interactive tools are also being developed to enhance user understanding of LLMs. **LLMCheckup** [LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations](https://www.arxiv.org/abs/2401.12576) presents a conversational tool that enables users to chat with LLMs about their behavior, utilizing a broad spectrum of Explainable AI (XAI) methods. This tool supports follow-up questions and generates suggestions, providing tutorials for users with varying levels of expertise. Likewise, **Designing a Dashboard** [Designing a Dashboard for Transparency and Control of Conversational AI](https://www.arxiv.org/abs/2406.07882) presents an end-to-end prototype that combines interpretability techniques with user experience design to make chatbots more transparent. The dashboard displays the user model in real-time, allowing users to control the system’s behavior.

The challenges in LLM interpretability have also been a focus of research. Several papers highlight the potential for unfaithful explanations, particularly with Chain-of-Thought (CoT) prompting. **Faithful Chain-of-Thought Reasoning** [Faithful Chain-of-Thought Reasoning](https://www.arxiv.org/abs/2301.13379) proposes a reasoning framework involving two stages: translation from natural language query to symbolic reasoning chain, and problem solving using a deterministic solver, which guarantees that the reasoning chain provides a faithful explanation of the final answer. However, **Language Models Don't Always Say What They Think** [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://www.arxiv.org/abs/2305.04388) shows that CoT explanations can misrepresent the true reasons for a model's prediction and can be heavily influenced by biasing features in the input. This also corresponds with the findings in **Evaluating Human Alignment and Model Faithfulness of LLM Rationale** [Evaluating Human Alignment and Model Faithfulness of LLM Rationale](https://www.arxiv.org/abs/2407.00219), which show that prompting-based self-explanations are not as aligned with human rationales as attribution-based explanations, and that they are also less faithful in providing a reliable account of the model’s decision-making process. **Do Models Explain Themselves?** [Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations](https://www.arxiv.org/abs/2307.08678) also addresses this concern through the lens of counterfactual simulatability, highlighting that even state-of-the-art LLMs' explanations often have low precision and do not always enable humans to accurately infer model outputs on diverse counterfactual inputs.

**Rethinking Interpretability** [Rethinking Interpretability in the Era of Large Language Models](https://www.arxiv.org/abs/2402.01761) highlights the potential of LLMs to redefine interpretability with a more ambitious scope, but also raises challenges such as hallucinated explanations and high computational costs. This work emphasizes emerging research priorities, such as using LLMs to directly analyze new datasets and to generate interactive explanations. **Can Large Language Models Explain Themselves?** [Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations](https://www.arxiv.org/abs/2310.11207) investigates the quality of automatically generated self-explanations in LLMs. The study finds that while self-explanations perform on par with traditional methods, they can be quite different and prompt a rethinking of current interpretability practices. **Faithfulness vs. Plausibility** [Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models](https://www.arxiv.org/abs/2402.04614) highlights the dichotomy between the plausibility and the faithfulness of the self-explanations generated by LLMs, arguing that the current trend towards increasing plausibility can come at the cost of diminishing the faithfulness, and emphasizes the need for a systematic characterization of faithfulness-plausibility requirements for real-world applications.

**Interpretability at Scale** [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://www.arxiv.org/abs/2305.08809) introduces a method called Boundless DAS to efficiently search for interpretable causal structure in large language models. It scales Distributed Alignment Search (DAS) to large models and discover that Alpaca model implements a causal model with two interpretable boolean variables to solve simple numerical reasoning problems. This work marks a step toward faithfully understanding the inner workings of large language models.

There also exists multiple survey papers such as  **XAI meets LLMs** [XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models](https://www.arxiv.org/abs/2407.15248), which advocates for a balanced approach that values interpretability equally with functional advancements, **From Understanding to Utilization** [From Understanding to Utilization: A Survey on Explainability for Large Language Models](https://www.arxiv.org/abs/2401.12874), which delves into the research on explainability and the various methodologies and tasks that utilize an understanding of these models, and **Explainability for Large Language Models: A Survey** [Explainability for Large Language Models: A Survey](https://www.arxiv.org/abs/2309.01029), which introduces a taxonomy of explainability techniques and provides a structured overview of methods for explaining Transformer-based language models.


---


<div id="syntheticdata">  </div>

### Synthetic data

**Multi-Agent Frameworks for Synthetic Data Generation and Verification:** Several studies have leveraged multi-agent systems to create more sophisticated and reliable synthetic datasets. **Sengupta et al. (2024) in MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification [https://www.arxiv.org/abs/2412.04494]** introduce a framework employing multiple agents to generate customer query datasets and verify agent trajectories, aiming to improve agent performance and address data scarcity for testing.  Similarly, **Tang et al. (2024) in Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation [https://www.arxiv.org/abs/2410.14251]** propose MATRIX, a multi-agent simulator to generate diverse text-based scenarios for LLM post-training, demonstrating its effectiveness in creating both general and domain-specific data and even surpassing models trained on much larger real datasets.  **Arif et al. (2024) in The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation [https://www.arxiv.org/abs/2408.08688]** explore multi-agent workflows to generate synthetic Preference Optimization (PO) datasets, automating response evaluation and generation through configurations of LLMs acting as evaluators and generators.

**Agentic and Generative Teaching Approaches:** The concept of "generative teaching" using agents to create synthetic data for model improvement is gaining traction. **Mitra et al. (2024) in AgentInstruct: Toward Generative Teaching with Agentic Flows [https://www.arxiv.org/abs/2407.03502]** introduce AgentInstruct, an agentic framework that automatically generates large, diverse, and high-quality synthetic data for post-training, showing significant improvements in models trained on this data across various benchmarks and skills.

**Synthetic Data for Specific Tasks and Domains:** Synthetic data generation is being applied to enhance LLMs in specific domains.  **Xin et al. (2024) in DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data [https://www.arxiv.org/abs/2405.14333]** focus on theorem proving, generating a large-scale synthetic dataset of Lean 4 proofs to fine-tune models, achieving improved performance in formal theorem proving.  In mathematics, **Mitra et al. (2024) in Orca-Math: Unlocking the potential of SLMs in Grade School Math [https://www.arxiv.org/abs/2402.14830]** demonstrate the effectiveness of a high-quality synthetic math problem dataset created using a multi-agent setup for training smaller language models to achieve state-of-the-art results on grade school math problems.  Similarly, **Li et al. (2024) in Common 7B Language Models Already Possess Strong Math Capabilities [https://www.arxiv.org/abs/2403.04706]** utilize synthetic data to enhance the mathematical reasoning capabilities of smaller models like LLaMA-2 7B.  **Shao et al. (2024) in Case2Code: Learning Inductive Reasoning with Synthetic Data [https://www.arxiv.org/abs/2407.12504]** propose Case2Code, a task and synthetic dataset for teaching inductive reasoning to LLMs in the code domain, showing improvements in coding abilities through induction training.

**Addressing Data Scarcity and Quality in Tool Learning and Alignment:** Synthetic data is also crucial for tool learning and model alignment. **Tang et al. (2023) in ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases [https://www.arxiv.org/abs/2306.05301]** introduce ToolAlpaca, a framework for generating a diverse tool-use corpus through multi-agent simulation, enabling smaller models to learn generalized tool-use abilities.  **Iskander et al. (2024) in Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs [https://www.arxiv.org/abs/2409.16341]** emphasize the importance of data quality for training tool-using LLMs, proposing methods to assess data reliability and demonstrating that high-quality synthetic data leads to better model performance.  **Xu et al. (2024) in Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing [https://www.arxiv.org/abs/2406.08464]** present Magpie, a self-synthesis method to generate large-scale alignment data directly from aligned LLMs, offering a cost-effective approach to create high-quality instruction datasets. **Sudalairaj et al. (2024) in LAB: Large-Scale Alignment for ChatBots [https://www.arxiv.org/abs/2403.01081]** introduce LAB, a methodology using taxonomy-guided synthetic data generation for chatbot alignment, reducing reliance on human annotations and expensive models.

**Exploiting LLMs for Diverse and Scalable Synthetic Data Generation:** Researchers are exploring methods to leverage LLMs to generate diverse and scalable synthetic data. **Ge et al. (2024) in Scaling Synthetic Data Creation with 1,000,000,000 Personas [https://www.arxiv.org/abs/2406.20094]** introduce Persona Hub, a collection of a billion personas to facilitate diverse synthetic data creation at scale, showcasing its versatility in generating data for various scenarios. **Li et al. (2024) in Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models [https://www.arxiv.org/abs/2402.13064]** propose GLAN, a method that generates synthetic instruction data from a taxonomy of human knowledge, demonstrating its effectiveness in instruction tuning across diverse tasks. **Ding et al. (2023) in Enhancing Chat Language Models by Scaling High-quality Instructional Conversations [https://www.arxiv.org/abs/2305.14233v1]** introduce UltraChat, a large-scale dataset of instructional conversations generated iteratively without human queries, used to fine-tune high-performing conversational models. **Honovich et al. (2022) in Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [https://www.arxiv.org/abs/2212.09689]** present Unnatural Instructions, a dataset of creative instructions generated with minimal human effort, showing comparable performance to manually curated datasets. **Wang et al. (2022) in Self-Instruct: Aligning Language Models with Self-Generated Instructions [https://www.arxiv.org/abs/2212.10560]** propose Self-Instruct, a framework that bootstraps off a language model's own generations to improve instruction-following capabilities, creating a large synthetic dataset.

**Synthetic Data for Specific Modalities and Applications:** Synthetic data generation extends to various modalities and applications. **Huang et al. (2024) in MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data [https://www.arxiv.org/abs/2402.08957]** present MUSTARD, a framework for synthesizing theorem and proof data, creating a benchmark dataset for theorem proving and math word problems. **Abdullin et al. (2024) in Synthetic Dialogue Dataset Generation using LLM Agents [https://www.arxiv.org/abs/2401.17461]** explore synthetic dialogue generation using LLM agents for goal-oriented conversational systems. **Braga et al. (2024) in Synthetic Data Generation with Large Language Models for Personalized Community Question Answering [https://www.arxiv.org/abs/2410.22182]** investigate LLMs for generating synthetic documents for personalized community question answering, introducing the Sy-SE-PQA dataset. **Jandaghi et al. (2023) in Faithful Persona-based Conversational Dataset Generation with Large Language Models [https://www.arxiv.org/abs/2312.10007]** use LLMs to create persona-based conversational datasets, enhancing the quality of conversations through a generator-critic framework. **Liang et al. (2024) in Synth-Empathy: Towards High-Quality Synthetic Empathy Data [https://www.arxiv.org/abs/2407.21669]** focus on generating high-quality synthetic empathy data, introducing Synth-Empathy, a pipeline for data generation and quality selection to improve empathetic response performance. **Yukhymenko et al. (2024) in A Synthetic Dataset for Personal Attribute Inference [https://www.arxiv.org/abs/2406.07217v2]** construct SynthPAI, a synthetic dataset for personal attribute inference, addressing the lack of public datasets in this privacy-sensitive area. **Tang et al. (2023) in Does Synthetic Data Generation of LLMs Help Clinical Text Mining? [https://www.arxiv.org/abs/2303.04360]** investigate the use of synthetic data generated by LLMs for clinical text mining tasks, showing significant performance improvements for named entity recognition and relation extraction.

**Privacy-Preserving Synthetic Data Generation:** Privacy concerns are addressed in works exploring differentially private synthetic data. **Yu et al. (2024) in Privacy-Preserving Instructions for Aligning Large Language Models [https://www.arxiv.org/abs/2402.13659]** propose using synthetic instructions generated with differential privacy to replace real user instructions for model alignment, mitigating privacy risks. **Xie et al. (2024) in Differentially Private Synthetic Data via Foundation Model APIs 2: Text [https://www.arxiv.org/abs/2403.01749]** introduce Aug-PE, an algorithm for generating differentially private synthetic text using API access to LLMs, offering a more accessible route to privacy-preserving applications.

**Analyzing Diversity and Quality of Synthetic Data:** Understanding the diversity and quality of synthetic data is critical. **Chen et al. (2024) in On the Diversity of Synthetic Data and its Impact on Training Large Language Models [https://www.arxiv.org/abs/2410.15226]** introduce a diversity metric, LLM cluster-agent, to evaluate synthetic datasets, demonstrating the positive correlation between diversity and LLM performance.

**Alternative Approaches and Datasets:** Other notable works explore alternative approaches and datasets. **Liu et al. (2023) in TinyGSM: achieving >80% on GSM8k with small language models [https://www.arxiv.org/abs/2312.09241]** introduce TinyGSM, a synthetic dataset of grade school math problems, showing that small models can achieve high accuracy with high-quality synthetic data. **Wei et al. (2023) in Magicoder: Empowering Code Generation with OSS-Instruct [https://www.arxiv.org/abs/2312.02120]** present Magicoder, a series of code LLMs trained on synthetic instruction data generated using OSS-Instruct, leveraging open-source code snippets for more realistic data. **Wang et al. (2021) in Towards Zero-Label Language Learning [https://www.arxiv.org/abs/2109.09193]** explore zero-label learning using synthetic data generated by unsupervised data generation (UDG), achieving comparable results to models trained on human-labeled data.  **Wagner et al. (2024) in SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions [https://www.arxiv.org/abs/2404.08078]** explore active learning using LLM-generated synthetic data for stance detection, showing improvements in performance and even surpassing models trained on full datasets. **Setlur et al. (2024) in RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold [https://www.arxiv.org/abs/2406.14532]** investigate the use of reinforcement learning with synthetic data, particularly incorrect data, to improve the efficiency of LLM math reasoning. **Josifoski et al. (2023) in Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction [https://www.arxiv.org/abs/2303.04132]** demonstrate the effectiveness of generating synthetic data by exploiting task difficulty asymmetry, specifically for information extraction, creating a high-quality synthetic dataset and outperforming existing methods. **Eldan et al. (2023) in TinyStories: How Small Can Language Models Be and Still Speak Coherent English? [https://www.arxiv.org/abs/2305.07759]** introduce TinyStories, a synthetic dataset of simple stories for training and evaluating very small language models, facilitating research on language emergence in smaller models.


---

<div id="inference_speed">  </div>

### Inference speed

Widespread deployment of LLM-agents is often constrained by the inference time computational demands. According to [Nielsen (1994)](https://dl.acm.org/doi/10.5555/2821575), human lose:

- feeling of instantaneous interactivity after 0.1 seconds,
- flow of thought after 1 seconds,
- attention to the communication after 10 seonds.

[Google (2009)](https://services.google.com/fh/files/blogs/google_delayexp.pdf) found, that an extra 0.1 seconds in latency, decreased user engagement by 0.2% with even larger impact over prolonged period of time. Brain research indicates, that [latency above 0.2 seconds deactivates](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0146250) certain brain regions. 

Thus, I would argue, that in the existence of trivial difference in intelligence, the lower latency model will engage user non-trivially more compared to the trivially more intelligence model. 

**Speculative Decoding and Parallel Decoding:** A significant body of work focuses on speculative decoding, which aims to reduce the sequential nature of autoregressive generation by predicting multiple tokens in parallel. **Speculative Decoding** as introduced by Leviathan et al. (2022) proposes using a smaller "draft" model to generate candidate tokens, which are then verified by the larger target model in parallel, achieving 2x-3x speedup on T5-XXL [https://www.arxiv.org/abs/2211.17192]. Chen et al.'s **Accelerating Large Language Model Decoding with Speculative Sampling** (2023) further refines this approach, demonstrating 2-2.5x speedup with Chinchilla using a modified rejection sampling scheme [https://www.arxiv.org/abs/2302.01318].  **Cascade Speculative Drafting (CS Drafting)** by Chen et al. (2023) introduces vertical and horizontal cascades to optimize the drafting process, achieving further speedups over standard speculative decoding [https://www.arxiv.org/abs/2312.11462].  Building upon speculative decoding, Cai et al. introduce **Medusa** (2024), which adds multiple decoding heads to predict subsequent tokens in parallel, achieving 2.2x-3.6x speedup [https://www.arxiv.org/abs/2401.10774].  Ankner et al.'s **Hydra** (2024) improves Medusa by introducing sequentially dependent draft heads, enhancing speculation accuracy and throughput [https://www.arxiv.org/abs/2402.05109]. Li et al.'s **EAGLE** (2024) reconsiders speculative sampling and proposes a framework achieving 2.7x-3.5x speedup for LLaMA2-Chat 70B by focusing on feature-level autoregression [https://www.arxiv.org/abs/2401.15077], further enhanced by **EAGLE-2** (2024) which dynamically adjusts draft trees for context-aware acceleration [https://www.arxiv.org/abs/2406.16858]. Sun et al.'s **TriForce** (2024) introduces hierarchical speculative decoding with dynamic sparse KV cache for long sequence generation, achieving up to 2.31x speedup on Llama2-7B-128K [https://www.arxiv.org/abs/2404.11912]. Liu et al.'s **Kangaroo** (2024) proposes lossless self-speculative decoding using double early exiting, achieving up to 1.68x speedup on Spec-Bench [https://www.arxiv.org/abs/2404.18911]. Du et al.'s **GliDe with a CaPE** (2024) proposes modifications to speculative decoding, achieving up to 2.61x speedup on Vicuna models [https://www.arxiv.org/abs/2402.02082]. Zhang et al.'s **Draft & Verify** (2023) presents self-speculative decoding, skipping intermediate layers for drafting and verifying with the original LLM, achieving up to 1.99x speedup [https://www.arxiv.org/abs/2309.08168]. Sun et al.'s **SpecTr** (2023) approaches speculative decoding via optimal transport, yielding 2.13X speedup and further improvements over standard speculative decoding [https://www.arxiv.org/abs/2310.15141].

Alternative parallel decoding approaches are also explored. Fu et al.'s **Lookahead Decoding** (2024) introduces an exact, parallel decoding algorithm that accelerates LLM decoding without auxiliary models, achieving up to 1.8x speedup on MT-bench [https://www.arxiv.org/abs/2402.02057]. Zhao et al.'s **Lookahead** framework (2023) uses a Trie-based retrieval and verification mechanism for multi-token acceptance in each step, achieving 2.66x to 6.26x speedup in real-world deployments [https://www.arxiv.org/abs/2312.12728]. Yi et al.'s **SPACE** (2024) introduces Smart Parallel Auto-Correct Decoding, enabling parallel token generation and verification in autoregressive LLMs, demonstrating 2.7x-4.0x speedup [https://www.arxiv.org/abs/2402.11809]. Kou et al.'s **CLLMs** (2024) develops Consistency Large Language Models to achieve faster convergence in Jacobi decoding, showing 2.4x to 3.4x speed improvements [https://www.arxiv.org/abs/2403.00835]. Jin et al.'s **Adaptive Skeleton Graph Decoding** (2024) proposes a parallel decoding strategy based on decomposing prompts into sub-problems, achieving a 1.69x speedup with improved quality [https://www.arxiv.org/abs/2402.12280]. Monea et al.'s **PaSS** (2023) explores Parallel Speculative Sampling using parallel decoding to draft multiple tokens from a single model, achieving up to 30% speedup [https://www.arxiv.org/abs/2311.13581].

**Early Exiting and Adaptive Computation:** Another direction is early exiting, where inference is terminated at earlier layers for simpler inputs. Schuster et al. introduce **Confident Adaptive Language Modeling (CALM)** (2022), a framework for dynamically allocating compute per input, achieving up to 3x speedup [https://www.arxiv.org/abs/2207.07061]. He et al.'s **Magic Pyramid (MP)** (2021) proposes token pruning and early exiting, achieving up to 8.06x speedup [https://www.arxiv.org/abs/2111.00230]. Bae et al.'s **FREE** (2023) presents a Fast and Robust Early-Exiting framework with synchronized parallel decoding and adaptive threshold estimation [https://www.arxiv.org/abs/2310.05424]. Chen et al.'s **EE-LLM** (2023) provides a framework for large-scale training and inference of early-exit LLMs with 3D parallelism [https://www.arxiv.org/abs/2312.04916]. Raposo et al.'s **Mixture-of-Depths** (2024) dynamically allocates compute across layers, achieving up to 50% faster sampling [https://www.arxiv.org/abs/2404.02258]. Zhou et al.'s **BERT Loses Patience** (2020) proposes patience-based early exit, improving both efficiency and robustness [https://www.arxiv.org/abs/2006.04768]. Li et al.'s **CascadeBERT** (2020) dynamically selects proper-sized models in a cascading manner for inference acceleration [https://www.arxiv.org/abs/2012.14682]. Zeng et al.'s **ConsistentEE** (2023) proposes a consistent and hardness-guided early exiting method using reinforcement learning [https://www.arxiv.org/abs/2312.11983]. Fan et al.'s **AdaInfer** (2024) adaptively determines inference termination moment based on input instance, saving up to 50% computation on sentiment tasks [https://www.arxiv.org/abs/2403.02181]. Schwartz et al. (2020) propose **The Right Tool for the Job**, matching model and instance complexities for early exiting, achieving up to five times faster inference [https://www.arxiv.org/abs/2004.07453]. Sun et al. (2022) present **Confident Adaptive Transformers (CATs)**, guaranteeing consistency with the original model while accelerating inference [https://www.arxiv.org/abs/2104.08803]. Xin et al.'s **DeeBERT** (2020) allows samples to exit earlier, saving up to 40% inference time with minimal quality degradation [https://www.arxiv.org/abs/2004.02984]. Sun et al.'s **HashEE** (2022) proposes a simple hash-based early exiting approach, requiring no extra parameters and achieving higher performance with fewer FLOPs [https://www.arxiv.org/abs/2203.01670].

**Quantization and Sparsity:** Model compression techniques such as quantization and sparsity are widely explored. Frantar et al.'s **GPTQ** (2022) introduces accurate post-training quantization for GPT models, enabling 3-4 bit quantization with negligible accuracy loss [https://www.arxiv.org/abs/2210.17323]. Yao et al.'s **ZeroQuant** (2022) presents an efficient post-training quantization approach for large transformers, achieving INT8 quantization with minimal accuracy impact [https://www.arxiv.org/abs/2206.01861]. Kim et al.'s **SqueezeLLM** (2023) introduces dense-and-sparse quantization for lossless compression to ultra-low precisions [https://www.arxiv.org/abs/2306.07629]. Guo et al.'s **OliVe** (2023) proposes outlier-victim pair quantization for hardware-friendly LLM acceleration [https://www.arxiv.org/abs/2304.07493]. Kim et al.'s **LUT-GEMM** (2022) introduces a quantized matrix multiplication kernel based on LUTs for efficient inference [https://www.arxiv.org/abs/2206.09557]. Lin et al.'s **FP6-LLM** (2024) explores FP6 quantization for efficient LLM serving, achieving 1.69x-2.65x higher throughput [https://www.arxiv.org/abs/2401.14112]. Lin et al.'s **FineQuant** (2023) proposes fine-grained weight-only quantization for LLMs, achieving up to 3.65 times higher throughput [https://www.arxiv.org/abs/2308.09723]. Lin et al.'s **Atom** (2023) introduces low-bit quantization for efficient and accurate LLM serving, improving throughput by up to 7.7x [https://www.arxiv.org/abs/2310.19102]. Huang et al.'s **BiLLM** (2024) presents a 1-bit post-training quantization scheme for LLMs, achieving high accuracy with 1.08-bit weights [https://www.arxiv.org/abs/2402.04291]. Shao et al.'s **OmniQuant** (2023) introduces omnidirectionally calibrated quantization for diverse quantization settings [https://www.arxiv.org/abs/2308.13137]. Wu et al.'s **Understanding INT4 Quantization for Transformer Models** (2023) explores the feasibility of INT4 quantization and develops an optimized inference pipeline [https://www.arxiv.org/abs/2301.12017].  Lin et al.'s **QServe** (2024) introduces W4A8KV4 quantization and system co-design for efficient LLM serving, improving throughput by up to 3.5x [https://www.arxiv.org/abs/2405.04532].

Kurtic et al.'s **ZipLM** (2023) proposes inference-aware structured pruning for language models [https://www.arxiv.org/abs/2302.04089]. Xia et al.'s **Flash-LLM** (2023) enables cost-effective inference with unstructured sparsity on Tensor Cores, achieving up to 3.8x speedup [https://www.arxiv.org/abs/2309.10285]. Li et al.'s **E-Sparse** (2023) introduces entropy-based N:M sparsity for boosting LLM inference speed [https://www.arxiv.org/abs/2310.15929]. Agarwalla et al.'s **Enabling High-Sparsity Foundational Llama Models** (2024) combines SparseGPT and sparse pretraining to create accurate sparse LLMs with up to 70% sparsity [https://www.arxiv.org/abs/2405.03594]. Ashkboos et al.'s **SliceGPT** (2024) compresses LLMs by deleting rows and columns, removing up to 25% parameters while maintaining performance [https://www.arxiv.org/abs/2401.15024]. Kaushal et al.'s **LORD** (2023) explores low-rank decomposition for one-shot compression of code LLMs [https://www.arxiv.org/abs/2309.14021]. Song et al.'s **Turbo Sparse** (2024) achieves SOTA performance with minimal activated parameters by exploiting activation sparsity [https://www.arxiv.org/abs/2406.05955]. An et al.'s **FLAP** (2023) proposes fluctuation-based adaptive structured pruning for retraining-free LLM compression [https://www.arxiv.org/abs/2312.11983]. Song et al.'s **PowerInfer** (2023) exploits neuron activation locality for fast LLM serving on consumer-grade GPUs [https://www.arxiv.org/abs/2312.12456]. Zhang et al.'s **NoMAD-Attention** (2024) introduces multiply-add-free attention for efficient CPU inference [https://www.arxiv.org/abs/2403.01273].

**Efficient Attention Mechanisms and Architectures:** Research also explores efficient attention mechanisms and alternative architectures. Han et al.'s **HyperAttention** (2023) presents an approximate attention mechanism for near-linear time long-context attention [https://www.arxiv.org/abs/2310.05869]. Fu et al.'s **H3** (2022) and Poli et al.'s **Hyena Hierarchy** (2023) propose state space models as alternatives to Transformers for efficient long sequence modeling [https://www.arxiv.org/abs/2212.14052, https://www.arxiv.org/abs/2302.10866v3]. Gu and Dao's **Mamba** (2023) introduces linear-time sequence modeling with selective state spaces, achieving faster inference than Transformers [https://www.arxiv.org/abs/2312.00752v2]. Ainslie et al.'s **CoLT5** (2023) proposes conditional computation in Transformers for faster long-range processing [https://www.arxiv.org/abs/2303.09752v3]. Xiao et al.'s **StreamingLLM** (2023) introduces attention sinks for efficient streaming language models with infinite context [https://www.arxiv.org/abs/2309.17453]. Munkhdalai et al.'s **Infini-attention** (2024) introduces a compressive memory in attention for infinite context Transformers [https://www.arxiv.org/abs/2404.07143]. Zhang et al.'s **DiJiang** (2024) introduces Frequency Domain Kernelization to transform Transformers into linear complexity models [https://www.arxiv.org/abs/2403.19928]. Csord\\'as et al.'s **SwitchHead** (2023) proposes mixture-of-experts attention for accelerating Transformers [https://www.arxiv.org/abs/2312.07987]. Park et al.'s **LUT-GEMM** (2022) uses lookup tables for efficient quantized matrix multiplication [https://www.arxiv.org/abs/2206.09557]. Zhang et al.'s **MoEfication** (2021) explores mixture-of-experts FFN layers in Transformers [https://www.arxiv.org/abs/2110.01786]. Cordonnier et al.'s **Stop Gradient Through Softmax** (2024) proposes stop-gradient softmax for efficient attention approximation [https://www.arxiv.org/abs/2405.04783]. Katharopoulos et al.'s **Linear Transformers** (2020) and Wang et al.'s **Linformer** (2020) introduce linear complexity self-attention mechanisms [https://www.arxiv.org/abs/2006.04768, https://www.arxiv.org/abs/2006.04152]. Hua et al.'s **FLASH** (2022) proposes gated attention units and linear approximation for linear-time Transformers [https://www.arxiv.org/abs/2202.10447]. Peng et al.'s **Random Feature Attention (RFA)** (2021) uses random feature methods for linear time and space attention [https://www.arxiv.org/abs/2103.02143]. Ma et al.'s **Luna** (2021) proposes linear unified nested attention for linear complexity [https://www.arxiv.org/abs/2106.01540].

**System-Level Optimizations and Hardware Acceleration:** System-level optimizations and specialized hardware are crucial for efficient LLM inference. Kwon et al.'s **PagedAttention** (2023) proposes efficient memory management for KV cache, implemented in vLLM [https://www.arxiv.org/abs/2309.06180]. Sheng et al.'s **FlexGen** (2023) presents a high-throughput generation engine for running LLMs with limited GPU memory [https://www.arxiv.org/abs/2303.06865]. Holmes et al.'s **DeepSpeed-FastGen** (2024) employs Dynamic SplitFuse for high-throughput text generation via MII and DeepSpeed-Inference [https://www.arxiv.org/abs/2401.08671]. Agrawal et al.'s **SARATHI** (2023) uses chunked-prefills and decode-maximal batching for efficient LLM inference [https://www.arxiv.org/abs/2308.16369], further developed into **Sarathi-Serve** (2024) for taming throughput-latency tradeoff [https://www.arxiv.org/abs/2403.02310]. Wu et al.'s **FastServe** (2023) proposes a distributed inference serving system with preemptive scheduling to minimize latency [https://www.arxiv.org/abs/2305.05920]. Zhong et al.'s **DistServe** (2024) disaggregates prefill and decoding for goodput-optimized LLM serving [https://www.arxiv.org/abs/2401.09670]. Hu et al.'s **TetriInfer** (2024) disaggregates LLM inference for mixed downstream workloads to mitigate interference [https://www.arxiv.org/abs/2401.11181]. Lee et al.'s **InfiniGen** (2024) introduces dynamic KV cache management for efficient long-text generation [https://www.arxiv.org/abs/2406.19707]. Lin et al.'s **Infinite-LLM** (2024) proposes DistAttention and Distributed KVCache for efficient LLM service with long context [https://www.arxiv.org/abs/2401.02669]. He et al.'s **UELLM** (2024) presents a Unified and Efficient approach for LLM inference serving in MLaaS clouds [https://www.arxiv.org/abs/2409.14961]. Patel et al.'s **Splitwise** (2023) proposes phase splitting for efficient generative LLM inference [https://www.arxiv.org/abs/2311.18677].

Moon et al.'s **LPU** (2024) introduces a latency-optimized processor for LLM inference [https://www.arxiv.org/abs/2408.07326]. Chen et al.'s **Understanding the Potential of FPGA-Based Spatial Acceleration** (2023) investigates FPGA-based spatial acceleration for LLM inference [https://www.arxiv.org/abs/2312.15159]. Xia et al.'s **Efficient LLM Inference on CPUs** (2023) focuses on optimizing LLM inference on CPUs with INT4 quantization and optimized kernels [https://www.arxiv.org/abs/2311.00502]. Wei et al.'s **T-MAC** (2024) introduces a LUT-based method for efficient low-bit LLM inference on CPUs [https://www.arxiv.org/abs/2407.00088]. He et al.'s **Inference Performance Optimization for Large Language Models on CPUs** (2024) presents a deployable solution for accelerating LLMs on CPUs [https://www.arxiv.org/abs/2407.10960]. Wolters et al.'s survey (2024) provides an overview of compute-in-memory architectures for accelerating LLM inference [https://www.arxiv.org/abs/2406.08413].  Li et al.'s **FTRANS** (2020) proposes an FPGA acceleration framework using BCM-based weight representation [https://www.arxiv.org/abs/2007.08563]. Han et al.'s **SAL-PIM** (2024) introduces a subarray-level processing-in-memory architecture for Transformer-based text generation [https://www.arxiv.org/abs/2401.17005]. Haris et al.'s work (2024) designs efficient LLM accelerators for edge devices on FPGAs using SECDA-LLM platform [https://www.arxiv.org/abs/2408.00462]. Yuan et al.'s survey (2024) provides a comprehensive hardware perspective on LLM inference acceleration [https://www.arxiv.org/abs/2410.04466]. Prabhakar et al.'s **SambaNova SN40L** (2024) presents a CoE system on a reconfigurable dataflow unit for scaling LLM inference [https://www.arxiv.org/abs/2405.07518]. Huang et al.'s **EdgeLLM** (2024) proposes a CPU-FPGA heterogeneous accelerator for efficient on-device LLM inference [https://www.arxiv.org/abs/2407.21325v1]. Wang et al.'s **SOFA** (2024) presents a compute-memory optimized sparsity accelerator via cross-stage coordinated tiling [https://www.arxiv.org/abs/2407.10416]. Chitty-Venkata et al.'s **LLM-Inference-Bench** (2024) introduces a benchmarking suite to evaluate hardware inference performance of LLMs across diverse platforms [https://www.arxiv.org/abs/2411.00136].

**Other Techniques:** Other techniques include context compression and distillation. Li et al.'s **Compressing Context to Enhance Inference Efficiency** (2023) proposes selective context compression for reducing memory cost and latency [https://www.arxiv.org/abs/2310.06201]. Jiang et al.'s **LLMLingua** (2023) and **LongLLMLingua** (2023) present prompt compression methods for accelerated inference of LLMs [https://www.arxiv.org/abs/2310.05736, https://www.arxiv.org/abs/2310.06839]. Mu et al.'s **Learning to Compress Prompts with Gist Tokens** (2023) trains LMs to compress prompts into smaller sets of gist tokens for efficiency [https://www.arxiv.org/abs/2304.08467]. Chevalier et al.'s **Adapting Language Models to Compress Contexts** (2023) proposes AutoCompressors for compressing long contexts into summary vectors [https://www.arxiv.org/abs/2305.14788]. Ge et al.'s **Inference with Reference** (2023) accelerates LLM inference by piggybacking on available references [https://www.arxiv.org/abs/2304.04487]. Ge et al.'s **In-context Autoencoder** (2023) leverages LLMs to compress context into compact memory slots [https://www.arxiv.org/abs/2307.06945].

Jiao et al.'s **TinyBERT** (2019) introduces knowledge distillation for BERT compression [https://www.arxiv.org/abs/1909.10351], further improved by Guskin et al.'s **Dynamic-TinyBERT** (2021) with dynamic sequence length [https://www.arxiv.org/abs/2111.09645]. Sanh et al.'s **DistilBERT** (2019) proposes a distilled version of BERT for smaller, faster, and cheaper inference [https://www.arxiv.org/abs/1910.01108]. Liu et al.'s **FastBERT** (2020) introduces self-distillation and adaptive inference time for BERT acceleration [https://www.arxiv.org/abs/2004.02984]. Kudugunta et al.'s **Task-level Mixture-of-Experts** (2021) explores task-level routing in MoE models for efficient inference [https://www.arxiv.org/abs/2110.03742].

---


<div id="weights">  </div>

### Model weights

We review here weight averaging and merging, quantization and compression, initialization techniques, weight space analysis, hypernetworks and weight generation, sparsity and pruning.

### Weight Averaging and Model Merging

Weight averaging and model merging techniques have emerged as effective strategies for improving model performance and efficiency. **Model soups** [[Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://www.arxiv.org/abs/2203.05482)] introduced a method to average the weights of multiple fine-tuned models, demonstrating improved accuracy and robustness without increasing inference cost.  **Rethinking Weight-Averaged Model-merging** [[Rethinking Weight-Averaged Model-merging](https://www.arxiv.org/abs/2411.09263)] further investigates the underlying mechanisms of weight averaging, exploring weight patterns, ensemble strategies, and prediction stability.  **Stochastic Weight Averaging (SWA)** [[Averaging Weights Leads to Wider Optima and Better Generalization](https://www.arxiv.org/abs/1803.05407)] demonstrates that averaging weights along the SGD trajectory leads to better generalization and flatter solutions compared to standard SGD.  In the context of training efficiency, **Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging** [[Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging](https://www.arxiv.org/abs/2209.14981)] shows that averaging the latest checkpoints during training can significantly speed up convergence.

Building on weight averaging, several works explore merging models trained for different tasks or with diverse objectives. **Rewarded soups** [[Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards](https://www.arxiv.org/abs/2306.04488)] proposes interpolating weights of models fine-tuned on diverse rewards to achieve Pareto-optimal alignment. **Diverse Weight Averaging (DiWA)** [[Diverse Weight Averaging for Out-of-Distribution Generalization](https://www.arxiv.org/abs/2205.19739)] enhances out-of-distribution generalization by averaging weights from independently trained models, increasing functional diversity.  **Git Re-Basin** [[Git Re-Basin: Merging Models modulo Permutation Symmetries](https://www.arxiv.org/abs/2209.04836)] introduces algorithms to merge models by addressing permutation symmetries of hidden units, enabling linear mode connectivity.  **TIES-Merging** [[TIES-Merging: Resolving Interference When Merging Models](https://www.arxiv.org/abs/2306.01708)] tackles interference issues in model merging by resolving parameter redundancy and sign conflicts, leading to improved multi-task models.  **Merging Decision Transformers** [[Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies](https://www.arxiv.org/abs/2303.07551)] applies weight averaging to merge Decision Transformers for creating multi-task policies.  **Model Fusion via Optimal Transport** [[Model Fusion via Optimal Transport](https://www.arxiv.org/abs/1910.05653)] utilizes optimal transport to align neurons across models before merging, facilitating knowledge transfer and model compression.  Finally, **An Empirical Study of Multimodal Model Merging** [[An Empirical Study of Multimodal Model Merging](https://www.arxiv.org/abs/2304.14933)] investigates the merging of multimodal transformers to create parameter-efficient modality-agnostic architectures.

### Quantization and Compression

Model quantization and compression are crucial for efficient deployment of LLMs and deep learning models, especially on resource-constrained devices. **GPTQ** [[GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://www.arxiv.org/abs/2210.17323)] proposes a one-shot weight quantization method for GPT models, achieving significant compression with minimal accuracy loss. **SpQR** [[SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://www.arxiv.org/abs/2306.03078)] introduces a sparse-quantized representation to achieve near-lossless compression by isolating and preserving outlier weights.  **QuIP** [[QuIP: 2-Bit Quantization of Large Language Models With Guarantees](https://www.arxiv.org/abs/2307.13304)] presents a 2-bit quantization method using incoherence processing for LLMs, offering theoretical guarantees.  **SmoothQuant** [[SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://www.arxiv.org/abs/2211.10438)] addresses activation outliers by smoothing them into weights, enabling efficient post-training quantization.  **OmniQuant** [[OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](https://www.arxiv.org/abs/2308.13137)] introduces omnidirectionally calibrated quantization, optimizing various quantization parameters for improved performance in low-bit settings. **QuaRot** [[QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs](https://www.arxiv.org/abs/2404.00456)] proposes a rotation-based quantization scheme to remove outliers, enabling outlier-free 4-bit inference in LLMs.  **LLM.int8()** [[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://www.arxiv.org/abs/2208.07339)] develops an 8-bit matrix multiplication procedure for transformers, significantly reducing memory footprint without performance degradation. **Atom** [[Atom: Low-bit Quantization for Efficient and Accurate LLM Serving](https://www.arxiv.org/abs/2310.19102)] introduces a low-bit quantization method for efficient and accurate LLM serving, leveraging mixed-precision and fine-grained quantization.

Other quantization techniques include **Adaptive Rounding (AdaRound)** [[Up or Down? Adaptive Rounding for Post-Training Quantization](https://www.arxiv.org/abs/2004.10568)] which adapts weight rounding to data and task loss, and **Data-Free Quantization** [[Data-Free Quantization Through Weight Equalization and Bias Correction](https://www.arxiv.org/abs/1906.04721)] which equalizes weight ranges and corrects biases for quantization without fine-tuning.  Methods pushing towards extreme quantization include **BinaryBERT** [[BinaryBERT: Pushing the Limit of BERT Quantization](https://www.arxiv.org/abs/2012.15701)] which explores weight binarization for BERT, **TernaryBERT** [[TernaryBERT: Distillation-aware Ultra-low Bit BERT](https://www.arxiv.org/abs/2009.12812)] which ternarizes BERT weights, and **BitNet** [[BitNet: Scaling 1-bit Transformers for Large Language Models](https://www.arxiv.org/abs/2023.10.11453)] which proposes a scalable 1-bit Transformer architecture.  **XNOR-Net** [[XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://www.arxiv.org/abs/1603.05279)] and **Binarized Neural Networks (BNNs)** [[Binarized Neural Networks](https://www.arxiv.org/abs/2016.02.02505v3), [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://www.arxiv.org/abs/1602.02830)] explore binary weights and activations for efficient CNNs. **BinaryConnect** [[BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://www.arxiv.org/abs/1511.00363)] investigates training DNNs with binary weights during propagation.  **USM-Lite** [[USM-Lite: Quantization and Sparsity Aware Fine-tuning for Speech Recognition with Universal Speech Models](https://www.arxiv.org/abs/2312.08553)], **Q8BERT** [[Q8BERT: Quantized 8Bit BERT](https://www.arxiv.org/abs/1910.06188)], **4-bit Conformer** [[4-bit Conformer with Native Quantization Aware Training for Speech Recognition](https://www.arxiv.org/abs/2203.15952)], **4-bit LSTM quantization** [[4-bit Quantization of LSTM-based Speech Recognition Models](https://www.arxiv.org/abs/2108.12074)], and **2-bit Conformer quantization** [[2-bit Conformer quantization for automatic speech recognition](https://www.arxiv.org/abs/2305.16619)], focus on low-bit quantization for speech recognition models. **Towards the Limit of Network Quantization** [[Towards the Limit of Network Quantization](https://www.arxiv.org/abs/1612.01543)] explores quantization schemes to minimize performance loss under compression constraints. **Effect of Weight Quantization on Learning Models by Typical Case Analysis** [[Effect of Weight Quantization on Learning Models by Typical Case Analysis](https://www.arxiv.org/abs/2024.01.17269)] analyzes the impact of quantization hyperparameters using statistical physics. **The case for 4-bit precision** [[The case for 4-bit precision: k-bit Inference Scaling Laws](https://www.arxiv.org/abs/2212.09720)] argues for the optimality of 4-bit precision for LLM inference based on scaling laws.  **Rethinking Channel Dimensions** [[Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models](https://www.arxiv.org/abs/2309.15531)] proposes per-IC quantization and the AdaDim framework to address outlier issues in low-bit quantization.  **Convolutional Neural Networks using Logarithmic Data Representation** [[Convolutional Neural Networks using Logarithmic Data Representation](https://www.arxiv.org/abs/1603.01025)] explores logarithmic representation to reduce bit precision in CNNs. **Optimal Brain Compression** [[Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning](https://www.arxiv.org/abs/2208.11580)] provides a framework for both quantization and pruning in a post-training setting.

### Initialization Techniques

Effective weight initialization is crucial for stable and efficient training of deep neural networks. **LSUV initialization** [[All you need is a good init](https://www.arxiv.org/abs/1511.06422)] proposes a layer-sequential unit-variance initialization method. **ZerO Initialization** [[ZerO Initialization: Initializing Neural Networks with only Zeros and Ones](https://www.arxiv.org/abs/2110.12661)] introduces a deterministic initialization scheme using only zeros and ones, demonstrating comparable performance to random initialization.  **Data-dependent Initializations** [[Data-dependent Initializations of Convolutional Neural Networks](https://www.arxiv.org/abs/1511.06856v3)] presents a data-dependent initialization procedure to ensure uniform training rates across network units. **Mimetic Initialization** [[Mimetic Initialization of Self-Attention Layers](https://www.arxiv.org/abs/2305.09828)] focuses on initializing self-attention layers to mimic pre-trained weights for improved training on vision tasks. **On Using Quasirandom Sequences** [[On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization](https://www.arxiv.org/abs/2408.02654)] investigates the use of quasirandom sequences like Sobol' for weight initialization, showing potential improvements in model performance and training speed.  **Principled Weight Initialization for Hypernetworks** [[Principled Weight Initialization for Hypernetworks](https://www.arxiv.org/abs/2312.08399)] develops principled initialization techniques specifically for hypernetworks. **IKUN** [[IKUN: Initialization to Keep snn training and generalization great with sUrrogate-stable variaNce](https://www.arxiv.org/abs/2411.18250)] introduces a variance-stabilizing initialization method tailored for Spiking Neural Networks (SNNs). **How to Start Training** [[How to Start Training: The Effect of Initialization and Architecture](https://www.arxiv.org/abs/1803.01719)] analyzes the effects of initialization and architecture on early training dynamics, identifying and addressing common failure modes.

### Weight Space Analysis and Manipulation

Understanding and manipulating the weight space of neural networks is a growing area of research. **Task Arithmetic** [[Editing Models with Task Arithmetic](https://www.arxiv.org/abs/2212.04089)] proposes a paradigm for steering model behavior through arithmetic operations on "task vectors" in weight space. **Interpreting the Weight Space of Customized Diffusion Models** [[Interpreting the Weight Space of Customized Diffusion Models](https://www.arxiv.org/abs/2406.09413)] investigates the weight space of fine-tuned diffusion models, revealing it as an interpretable latent space of identities. **Weight Similarity of Neural Networks** [[Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing](https://www.arxiv.org/abs/2208.04369)] introduces a weight similarity measure using chain normalization to quantify similarity between trained models. **Reverse-Engineering Deep ReLU Networks** [[Reverse-Engineering Deep ReLU Networks](https://www.arxiv.org/abs/1910.00744)] demonstrates the possibility of recovering the architecture and weights of ReLU networks by observing their output. **Weight-space symmetry** [[Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape](https://www.arxiv.org/abs/1907.02911)] explores the impact of weight-space symmetry and permutation saddles in deep networks. **Gradient descent alignment** [[Gradient descent aligns the layers of deep linear networks](https://www.arxiv.org/abs/1810.02032)] analyzes the alignment of weight matrices in deep linear networks under gradient descent. **Characterizing the Weight Space** [[Characterizing the Weight Space for Different Learning Models](https://www.arxiv.org/abs/2006.02724v1)] attempts to characterize the solution space of DNNs and Associative Memory models. **Security relevance of weights** [[On the security relevance of weights in deep learning](https://www.arxiv.org/abs/1902.03020v2)] highlights the security implications of initial weights, showing how permutations can disrupt model accuracy. **Unlearning Methods** [[Do Unlearning Methods Remove Information from Language Model Weights?](https://www.arxiv.org/abs/2410.08827)] evaluates whether unlearning methods truly remove information from model weights. **Stable Recovery of Entangled Weights** [[Stable Recovery of Entangled Weights: Towards Robust Identification of Deep Neural Networks from Minimal Samples](https://www.arxiv.org/abs/2101.07150)] focuses on the stable recovery of entangled weights for DNN identification. **Mode Connectivity and Neuron Alignment** [[Optimizing Mode Connectivity via Neuron Alignment](https://www.arxiv.org/abs/2009.02439)] explores neuron alignment for optimizing mode connectivity in loss landscapes. **Algorithmic Regularization** [[Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced](https://www.arxiv.org/abs/1806.00900)] studies algorithmic regularization imposed by gradient descent in deep homogeneous models. **Synaptic precision** [[Learning may need only a few bits of synaptic precision](https://www.arxiv.org/abs/1602.04129)] investigates the required synaptic precision for effective learning.  **Finite Sample Identification** [[Finite Sample Identification of Wide Shallow Neural Networks with Biases](https://www.arxiv.org/abs/2211.04589)] addresses finite sample identification of wide shallow networks. **Dense Clusters and Discrete Synapses** [[Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses](https://www.arxiv.org/abs/1509.05753)] explores dense clusters and learning in networks with discrete synapses. **Covariance Structure of Convolutional Filters** [[Understanding the Covariance Structure of Convolutional Filters](https://www.arxiv.org/abs/2210.03651)] examines the covariance structure of convolutional filters and its use for initialization. **Feature Purification** [[Feature Purification: How Adversarial Training Performs Robust Deep Learning](https://www.arxiv.org/abs/2005.10190)] introduces the feature purification principle in adversarial training.

### Hypernetworks and Weight Generation

Hypernetworks, networks that generate weights for other networks, provide a powerful approach for model generation and meta-learning. **HyperNetworks** [[HyperNetworks](https://www.arxiv.org/abs/1609.09106)] introduces the concept of hypernetworks and demonstrates their application to LSTMs and CNNs. **HyperGAN** [[HyperGAN: A Generative Model for Diverse, Performant Neural Networks](https://www.arxiv.org/abs/1901.11058)] proposes a generative model for learning a distribution of neural network parameters. **Hyper-Representations** [[Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights](https://www.arxiv.org/abs/2209.14733)] extends hyper-representations for generative use, sampling new model weights from a model zoo. **HyperDreamBooth** [[HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models](https://www.arxiv.org/abs/2307.06949)] utilizes hypernetworks for fast personalization of text-to-image models like DreamBooth.  **Generating Neural Networks with Neural Networks** [[Generating Neural Networks with Neural Networks](https://www.arxiv.org/abs/1801.01952)] explores the use of hypernetworks to generate weights for other neural networks. **Parameter Prediction for Unseen Deep Architectures** [[Parameter Prediction for Unseen Deep Architectures](https://www.arxiv.org/abs/2110.13100)] proposes hypernetworks to predict performant parameters for unseen architectures. **Meta-Learning via Diffusion Guidance** [[Meta-Learning via Classifier(-free) Diffusion Guidance](https://www.arxiv.org/abs/2210.08942)] uses diffusion models for meta-learning, generating task-adapted weights through latent space guidance. **HyperTransformer** [[HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning](https://www.arxiv.org/abs/2201.04182)] introduces a Transformer-based hypernetwork for few-shot learning, generating CNN weights from support samples. **Diffusion-based Neural Network Weights Generation** [[Diffusion-based Neural Network Weights Generation](https://www.arxiv.org/abs/2402.18153)] leverages diffusion models for dataset-conditioned pretrained weight sampling in transfer learning. **HyperStyle** [[HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing](https://www.arxiv.org/abs/2111.15666)] employs hypernetworks for StyleGAN inversion and real image editing.

### Sparsity and Pruning

Sparsity and pruning techniques aim to reduce model size and computational cost by removing less important weights or connections. **The Lottery Ticket Hypothesis** [[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://www.arxiv.org/abs/1803.03635)] reveals the existence of sparse, trainable subnetworks within dense networks. **Variational Dropout** [[Variational Dropout Sparsifies Deep Neural Networks](https://www.arxiv.org/abs/1701.05369)] utilizes variational dropout to achieve highly sparse networks, reducing parameters significantly. **Exploring Sparsity in Recurrent Neural Networks** [[Exploring Sparsity in Recurrent Neural Networks](https://www.arxiv.org/abs/1704.05119)] investigates sparsity and pruning in RNNs to reduce model size and improve inference speed. **Filter Pruning** [[Pruning Filters for Efficient ConvNets](https://www.arxiv.org/abs/1608.08710)] focuses on pruning entire filters in CNNs to reduce computation costs without creating sparse connectivity patterns. **Compressing Neural Networks with the Hashing Trick** [[Compressing Neural Networks with the Hashing Trick](https://www.arxiv.org/abs/1504.04788)] introduces HashedNets, using hashing to group and share connection weights for model compression. **Deep Compression** [[Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://www.arxiv.org/abs/1510.00149)] combines pruning, quantization, and Huffman coding for significant model compression. **SliceGPT** [[SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://www.arxiv.org/abs/2401.15024)] proposes compressing LLMs by deleting rows and columns from weight matrices.

### Other Related Approaches

Several other works explore diverse aspects of model weights and their optimization. **Patching open-vocabulary models** [[Patching open-vocabulary models by interpolating weights](https://www.arxiv.org/abs/2208.05592)] introduces PAINT, a method to patch open-vocabulary models by interpolating weights to improve performance on specific tasks. **Weights Augmentation** [[Weights Augmentation: it has never ever ever ever let her model down](https://www.arxiv.org/abs/2405.19590)] proposes Weight Augmentation Strategy (WAS) for weight exploration through random transformations. **Super Weights in LLMs** [[The Super Weight in Large Language Models](https://www.arxiv.org/abs/2411.07191)] identifies "super weights" in LLMs, single parameters that are disproportionately important for model performance. **Sampling weights of deep neural networks** [[Sampling weights of deep neural networks](https://www.arxiv.org/abs/2306.03078)] introduces a probability distribution and sampling algorithm for DNN weights, eliminating iterative training. **Predicting Neural Network Accuracy from Weights** [[Predicting Neural Network Accuracy from Weights](https://www.arxiv.org/abs/2002.11448)] explores predicting model accuracy solely from weight statistics, without evaluating on input data. **Representing Model Weights with Language** [[Representing Model Weights with Language using Tree Experts](https://www.arxiv.org/abs/2410.13569)] investigates representing model weights with language using Tree Experts. **Predicting Parameters in Deep Learning** [[Predicting Parameters in Deep Learning](https://www.arxiv.org/abs/1306.0543)] demonstrates redundancy in network parameters and the possibility of predicting a significant portion of them. **LoRA** [[LoRA: Low-Rank Adaptation of Large Language Models](https://www.arxiv.org/abs/2106.09685)] introduces Low-Rank Adaptation, freezing pre-trained weights and injecting trainable low-rank matrices for efficient fine-tuning. **BERT Busters** [[BERT Busters: Outlier Dimensions that Disrupt Transformers](https://www.arxiv.org/abs/2021.05.06990)] identifies outlier dimensions in LayerNorm that can disrupt Transformer performance. **Robust fine-tuning of zero-shot models** [[Robust fine-tuning of zero-shot models](https://www.arxiv.org/abs/2021.09.01903)] proposes WiSE-FT, ensembling zero-shot and fine-tuned weights to improve robustness under distribution shifts. **Domain Aggregation Networks (DARN)** [[Domain Aggregation Networks for Multi-Source Domain Adaptation](https://www.arxiv.org/abs/2019.09.05352)] develops a method to aggregate knowledge from multiple source domains by adaptively weighting them during training. **Scalable Methods for 8-bit Training** [[Scalable Methods for 8-bit Training of Neural Networks](https://www.arxiv.org/abs/2018.05.11046)] explores scalable 8-bit training techniques for neural networks. **Exact solutions to the nonlinear dynamics of learning** [[Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://www.arxiv.org/abs/2013.12.6120)] provides analytical solutions to the learning dynamics in deep linear networks. **Effect of Weight Quantization by Typical Case Analysis** [[Effect of Weight Quantization on Learning Models by Typical Case Analysis](https://www.arxiv.org/abs/2024.01.17269)] analyzes weight quantization effects using typical case analysis. **Variance Networks** [[Variance Networks: When Expectation Does Not Meet Your Expectations](https://www.arxiv.org/abs/2018.03.03764)] introduces variance layers, parameterizing weights by their variance instead of expectation. **Bitwise Neural Networks** [[Bitwise Neural Networks](https://www.arxiv.org/abs/2016.01.06071)] explores neural networks with binary weights, activations, and operations for resource-constrained environments. **Double-Weighting for Covariate Shift Adaptation** [[Double-Weighting for Covariate Shift Adaptation](https://www.arxiv.org/abs/2023.05.08637)] proposes a double-weighting method to address covariate shift in supervised learning. **CLIP** [[Learning Transferable Visual Models From Natural Language Supervision](https://www.arxiv.org/abs/2021.03.00020)] demonstrates learning transferable visual models from natural language supervision. **OPT** [[OPT: Open Pre-trained Transformer Language Models](https://www.arxiv.org/abs/2022.05.01068)] releases Open Pre-trained Transformer language models. **Celeb Basis** [[Inserting Anybody in Diffusion Models via Celeb Basis](https://www.arxiv.org/abs/2023.06.00926)] proposes a Celeb Basis for inserting new identities into diffusion models. **Fine-Tuning Pretrained Language Models** [[Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://www.arxiv.org/abs/2020.02.06305)] analyzes factors affecting fine-tuning of pretrained language models. **Unified Weight Initialization for Tensorial CNNs** [[A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks](https://www.arxiv.org/abs/2022.05.15307)] proposes a unified initialization paradigm for Tensorial CNNs. **Weight Distillation** [[Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://www.arxiv.org/abs/2020.09.09152v3)] explores weight distillation to transfer knowledge through parameter generators. **Training Foundation Models as Data Compression** [[Training Foundation Models as Data Compression: On Information, Model Weights and Copyright Law](https://www.arxiv.org/abs/2024.07.13493)] frames foundation model training as data compression and discusses copyright implications. **Robust and Resource Efficient Identification of Two Hidden Layer Neural Networks** [[Robust and Resource Efficient Identification of Two Hidden Layer Neural Networks](https://www.arxiv.org/abs/2019.06.00485)] focuses on robust identification of two-layer neural networks.



---

<div id="convergent">  </div>

## Convergent Evolution

Convergent evolution, the independent evolution of similar features in species of different lineages, is a fascinating phenomenon studied across various disciplines. In the realm of biological evolution, researchers have long been interested in distinguishing convergent evolution from other evolutionary forces. Chi et al. (2014) proposed a novel method using synonymous codon substitution distances to differentiate between convergent evolution and recombination when analyzing phylogenetic incongruence, demonstrating its application in viral and bacterial sequence datasets [Synonymous and Nonsynonymous Distances Help Untangle Convergent Evolution and Recombination](https://www.arxiv.org/abs/1410.1263). Mitchell et al. (2017) further addressed the challenge of distinguishing convergent evolution from violations of the molecular clock in phylogenetic analyses, advocating for convergence-divergence models and emphasizing the need to reconsider the assumption of purely treelike evolution [Distinguishing between convergent evolution and violation of the molecular clock](https://www.arxiv.org/abs/1709.04548).

Beyond biological systems, the concept of convergent evolution has also been explored in cultural and artificial systems. McBride et al. (2021) investigated convergent evolution in musical scales across cultures using a large cross-cultural database, revealing striking similarities in scale structures despite cultural diversity and exploring the mechanisms behind this convergence [Convergent evolution in a large cross-cultural database of musical scales](https://www.arxiv.org/abs/2108.00842).  In contrast, Lineweaver (2007) critically examined the hypothesis of increasing encephalization quotients over time, arguing against human-like intelligence as a convergent feature of evolution based on paleontological evidence, which has implications for the search for extraterrestrial intelligence [Paleontological Tests: Human-like Intelligence is not a Convergent Feature of Evolution](https://www.arxiv.org/abs/0711.1751v1).

Interestingly, the principles of convergent evolution are also being investigated in the context of artificial intelligence. Ohmae and Ohmae (2024) draw parallels between the brain and AI, suggesting convergent evolution in their computational mechanisms. They propose a novel framework for comparing brain and AI by subdividing circuit computation and identify wide-ranging similarities, particularly in world-model-based computations in the neocortex and cerebellum, offering new insights into neuroscience inspired by AI processing [The brain versus AI: World-model-based versatile circuit computation underlying diverse functions in the neocortex and cerebellum](https://www.arxiv.org/abs/2411.16075v2).  Furthermore, the emergence of common representations in diverse neural networks, termed "Rosetta Neurons," as explored by Dravid et al. (2023), hints at a form of convergent evolution in AI models trained for different vision tasks, suggesting that certain visual concepts are inherently learned regardless of architecture or task [Rosetta Neurons: Mining the Common Units in a Model Zoo](https://www.arxiv.org/abs/2306.09346). These findings across disciplines highlight the broad relevance and ongoing investigation of convergent evolution in both natural and artificial systems.


---

<div id="weights">  </div>

**Weight Averaging and Model Merging:** Several works explore weight averaging as a technique to enhance model performance.  "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time" ([Wortsman et al., 2022](https://www.arxiv.org/abs/2203.05482)) introduces "model soups," demonstrating that averaging weights of fine-tuned models can improve accuracy and robustness without additional inference costs. This method achieves state-of-the-art results on ImageNet and extends to other tasks.  Expanding on this, "Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging" ([Kaddour, 2022](https://www.arxiv.org/abs/2209.14981)) shows that averaging the weights of the latest checkpoints during training can significantly speed up convergence for image and language models.  "Averaging Weights Leads to Wider Optima and Better Generalization" ([Izmailov et al., 2018](https://www.arxiv.org/abs/1803.05407)) proposes Stochastic Weight Averaging (SWA), which averages weights along the SGD trajectory to find flatter solutions and improve generalization.  "Diverse Weight Averaging for Out-of-Distribution Generalization" ([Ram\\'e et al., 2022](https://www.arxiv.org/abs/2205.09739)) introduces Diverse Weight Averaging (DiWA), averaging weights from independent training runs to enhance functional diversity and improve out-of-distribution generalization. "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards" ([Ram\\'e et al., 2023](https://www.arxiv.org/abs/2306.04488)) further explores weight interpolation, proposing "rewarded soup" to achieve Pareto-optimal alignment across diverse rewards by merging weights of networks fine-tuned on individual rewards.  "Git Re-Basin: Merging Models modulo Permutation Symmetries" ([Ainsworth et al., 2022](https://www.arxiv.org/abs/2209.04836)) addresses the permutation symmetry in neural networks when merging models, introducing algorithms to align model units for effective weight-space merging.  "Model Fusion via Optimal Transport" ([Singh and Jaggi, 2019](https://www.arxiv.org/abs/1910.05653)) presents a layer-wise model fusion algorithm using optimal transport to align neurons before averaging parameters, enabling knowledge transfer between models trained on heterogeneous data.  "Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies" ([Lawson and Qureshi, 2023](https://www.arxiv.org/abs/2303.07551)) explores merging Decision Transformers trained on different tasks through weight averaging to create multi-task policies.  "An Empirical Study of Multimodal Model Merging" ([Sung et al., 2023](https://www.arxiv.org/abs/2304.14933)) extends model merging to multimodal setups, investigating factors influencing performance and proposing metrics for assessing merging outcomes.  "TIES-Merging: Resolving Interference When Merging Models" ([Yadav et al., 2023](https://www.arxiv.org/abs/2306.01708)) introduces TRIM, ELECT SIGN & MERGE (TIES-Merging) to address interference during model merging by resetting parameters, resolving sign conflicts, and merging aligned parameters.  "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch" ([Yu et al., 2023](https://www.arxiv.org/abs/2311.03099)) proposes DARE, a method to sparsify and merge delta parameters from fine-tuned language models, enabling the creation of merged models with diverse capabilities.  "Rethinking Weight-Averaged Model-merging" ([Wang et al., 2024](https://www.arxiv.org/abs/2411.09263)) provides insights into weight-averaged model merging by examining intrinsic patterns, comparing weight vs. feature averaging, and exploring prediction stability across parameter scales.

**Weight Quantization and Compression:** Efficient deployment of large models motivates research into weight quantization and compression. "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers" ([Frantar et al., 2022](https://www.arxiv.org/abs/2210.17323)) introduces GPTQ, a one-shot quantization method for GPT models, enabling 3-4 bit quantization with minimal accuracy loss and significant inference speedups. "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression" ([Dettmers et al., 2023](https://www.arxiv.org/abs/2306.03078)) presents SpQR, a sparse-quantized representation that achieves near-lossless compression for LLMs by isolating outlier weights and quantizing others to 3-4 bits. "TernaryBERT: Distillation-aware Ultra-low Bit BERT" ([Zhang et al., 2020](https://www.arxiv.org/abs/2009.12812)) proposes TernaryBERT, which ternarizes weights in fine-tuned BERT models and uses knowledge distillation to mitigate accuracy degradation. "BinaryBERT: Pushing the Limit of BERT Quantization" ([Bai et al., 2020](https://www.arxiv.org/abs/2012.15701)) introduces BinaryBERT, achieving weight binarization for BERT models through ternary weight splitting and fine-tuning.  "BinaryConnect: Training Deep Neural Networks with binary weights during propagations" ([Courbariaux et al., 2015](https://www.arxiv.org/abs/1511.00363)) proposes BinaryConnect, training DNNs with binary weights during forward and backward propagation for hardware efficiency. "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks" ([Rastegari et al., 2016](https://www.arxiv.org/abs/1603.05279)) introduces Binary-Weight-Networks and XNOR-Networks, approximating convolutions with binary operations for faster and memory-efficient CNNs. "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1" ([Courbariaux et al., 2016](https://www.arxiv.org/abs/1602.02830)) presents Binarized Neural Networks (BNNs) with binary weights and activations, demonstrating near state-of-the-art results with reduced memory and computational cost. "BinaryConnect: Training Deep Neural Networks with binary weights during propagations" ([Courbariaux et al., 2015](https://www.arxiv.org/abs/1511.00363)) explores training DNNs with binary weights, showing regularization effects and competitive performance. "Compressing Deep Convolutional Networks using Vector Quantization" ([Gong et al., 2014](https://www.arxiv.org/abs/1412.6115)) investigates vector quantization methods for compressing CNN parameters, achieving significant compression with minimal accuracy loss.  "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding" ([Han et al., 2015](https://www.arxiv.org/abs/1510.00149)) introduces a three-stage pipeline for deep compression: pruning, trained quantization, and Huffman coding, achieving substantial storage reduction. "Compressing Neural Networks with the Hashing Trick" ([Chen et al., 2015](https://www.arxiv.org/abs/1504.04788)) proposes HashedNets, using hash functions to group and share weights, reducing model size with preserved performance. "Towards the Limit of Network Quantization" ([Choi et al., 2016](https://www.arxiv.org/abs/1612.01543)) designs quantization schemes minimizing performance loss under compression constraints, utilizing Hessian-weighted k-means and entropy-constrained scalar quantization. "Data-Free Quantization Through Weight Equalization and Bias Correction" ([Nagel et al., 2019](https://www.arxiv.org/abs/1906.04721)) introduces a data-free quantization method based on weight range equalization and bias correction, achieving near-original performance. "Up or Down? Adaptive Rounding for Post-Training Quantization" ([Nagel et al., 2020](https://www.arxiv.org/abs/2004.10568)) proposes AdaRound, an adaptive weight-rounding mechanism for post-training quantization, outperforming rounding-to-nearest. "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers" ([Yao et al., 2022](https://www.arxiv.org/abs/2206.01861)) presents ZeroQuant, an efficient post-training quantization approach for large Transformers, using fine-grained quantization and layer-by-layer knowledge distillation. "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models" ([Xiao et al., 2022](https://www.arxiv.org/abs/2211.10438)) proposes SmoothQuant, smoothing activation outliers to weights to enable INT8 quantization for LLMs with minimal accuracy loss. "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models" ([Heo et al., 2023](https://www.arxiv.org/abs/2309.15531)) introduces per-IC quantization and Adaptive Dimensions (AdaDim) to mitigate outlier effects in low-bit weight quantization for LLMs. "QuIP: 2-Bit Quantization of Large Language Models With Guarantees" ([Chee et al., 2023](https://www.arxiv.org/abs/2307.13304)) presents QuIP, a 2-bit quantization method with incoherence processing, improving quantization accuracy with theoretical analysis. "SpinQuant: LLM quantization with learned rotations" ([Liu et al., 2024](https://www.arxiv.org/abs/2405.16406)) proposes SpinQuant, incorporating learned rotation matrices for optimal quantized network accuracy, achieving state-of-the-art 4-bit quantization for LLMs.  "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs" ([Ashkboos et al., 2024](https://www.arxiv.org/abs/2404.00456)) introduces QuaRot, a rotation-based quantization scheme for end-to-end 4-bit quantization of LLMs, removing outliers from hidden states. "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale" ([Dettmers et al., 2022](https://www.arxiv.org/abs/2208.07339)) develops LLM.int8(), an 8-bit matrix multiplication procedure for Transformers, reducing memory by half with full precision performance.  "BitNet: Scaling 1-bit Transformers for Large Language Models" ([Wang et al., 2023](https://www.arxiv.org/abs/2310.17323)) introduces BitNet, a 1-bit Transformer architecture for LLMs, achieving competitive performance with reduced memory and energy consumption. "4-bit Conformer with Native Quantization Aware Training for Speech Recognition" ([Ding et al., 2022](https://www.arxiv.org/abs/2203.15952)) proposes native quantization aware training for 4-bit Conformer ASR models, achieving lossless compression. "4-bit Quantization of LSTM-based Speech Recognition Models" ([Fasoli et al., 2021](https://www.arxiv.org/abs/2108.12074)) investigates 4-bit quantization for LSTM-based ASR models, achieving minimal accuracy loss with customized quantization schemes. "USM RNN-T model weights binarization" ([Rybakov et al., 2024](https://www.arxiv.org/abs/2406.02887)) presents weight binarization for USM RNN-T models, achieving significant model size reduction with a small WER increase. "USM-Lite: Quantization and Sparsity Aware Fine-tuning for Speech Recognition with Universal Speech Models" ([Ding et al., 2023](https://www.arxiv.org/abs/2312.08553)) proposes a USM fine-tuning approach with quantization and sparsity awareness for ASR, reducing model complexity. "2-bit Conformer quantization for automatic speech recognition" ([Rybakov et al., 2023](https://www.arxiv.org/abs/2305.16619)) explores 2-bit quantization for Conformer ASR models, achieving model size reduction with acceptable WER degradation. "Q8BERT: Quantized 8Bit BERT" ([Zafrir et al., 2019](https://www.arxiv.org/abs/1910.06188)) demonstrates quantization-aware training during BERT fine-tuning for 8-bit compression with minimal accuracy loss. "Scalable Methods for 8-bit Training of Neural Networks" ([Banner et al., 2018](https://www.arxiv.org/abs/1805.11046)) proposes scalable methods for 8-bit training, quantizing weights, activations, and gradients while maintaining accuracy. "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning" ([Frantar et al., 2022](https://www.arxiv.org/abs/2208.11580)) introduces a unified framework for post-training quantization and pruning, based on Optimal Brain Surgeon, improving compression-accuracy trade-offs. "Pruning Filters for Efficient ConvNets" ([Li et al., 2016](https://www.arxiv.org/abs/1608.08710)) presents a filter pruning method for CNNs, removing filters with small output accuracy effects to reduce computation costs. "Variational Dropout Sparsifies Deep Neural Networks" ([Molchanov et al., 2017](https://www.arxiv.org/abs/1701.05369)) explores Variational Dropout, leading to sparse neural networks with reduced parameters and negligible accuracy decrease. "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding" ([Han et al., 2015](https://www.arxiv.org/abs/1510.00149)) presents a deep compression pipeline using pruning, quantization, and Huffman coding for significant model size reduction. "SliceGPT: Compress Large Language Models by Deleting Rows and Columns" ([Ashkboos et al., 2024](https://www.arxiv.org/abs/2401.15024)) introduces SliceGPT, a post-training sparsification scheme that reduces embedding dimensions in LLMs by deleting rows and columns, maintaining performance with fewer parameters.

**Weight Initialization:** Weight initialization is crucial for effective training. "All you need is a good init" ([Mishkin and Matas, 2015](https://www.arxiv.org/abs/1511.06422)) proposes Layer-sequential unit-variance (LSUV) initialization, a simple method for initializing weights in deep nets, achieving state-of-the-art performance. "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones" ([Zhao et al., 2021](https://www.arxiv.org/abs/2110.12661)) introduces ZerO initialization, a deterministic scheme using only zeros and ones, demonstrating competitive performance and benefits like training ultra-deep networks. "Data-dependent Initializations of Convolutional Neural Networks" ([Krähenbühl et al., 2015](https://www.arxiv.org/abs/1511.06856v3)) presents a data-dependent initialization procedure that sets weights to avoid vanishing or exploding gradients, matching unsupervised pre-training methods in speed and performance. "Mimetic Initialization of Self-Attention Layers" ([Trockman and Kolter, 2023](https://www.arxiv.org/abs/2305.09828)) proposes mimetic initialization, initializing self-attention layer weights to resemble pre-trained counterparts for faster and more accurate training. "Principled Weight Initialization for Hypernetworks" ([Chang et al., 2023](https://www.arxiv.org/abs/2312.08399)) develops principled weight initialization techniques for hypernetworks, leading to more stable mainnet weights and faster convergence. "IKUN: Initialization to Keep snn training and generalization great with sUrrogate-stable variaNce" ([Chang et al., 2024](https://www.arxiv.org/abs/2411.18250)) introduces IKUN, a variance-stabilizing initialization method for Spiking Neural Networks (SNNs), improving convergence and generalization. "On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization" ([Miranskyy et al., 2024](https://www.arxiv.org/abs/2408.02654)) investigates using quasirandom number generators (QRNGs) like Sobol' sequences for weight initialization, showing potential improvements in model performance and training speed. "How to Start Training: The Effect of Initialization and Architecture" ([Hanin and Rolnick, 2018](https://www.arxiv.org/abs/1803.01719)) identifies failure modes in early training of deep ReLU nets, providing initialization strategies to avoid them for fully connected and residual architectures.

**Weight Space Analysis and Understanding:** Understanding the weight space is crucial for model improvement and interpretability. "Predicting Neural Network Accuracy from Weights" ([Unterthiner et al., 2020](https://www.arxiv.org/abs/2002.11448)) demonstrates that neural network accuracy can be predicted from weights alone, without input data, using simple weight statistics. "Interpreting the Weight Space of Customized Diffusion Models" ([Dravid et al., 2024](https://www.arxiv.org/abs/2406.09413)) investigates the weight space of customized diffusion models, showing it behaves as an interpretable latent space of identities, enabling sampling, editing, and inversion. "Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape" ([Brea et al., 2019](https://www.arxiv.org/abs/1907.02911)) explores permutation symmetry in deep networks, showing it leads to permutation saddles and valleys in the loss landscape. "Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing" ([Wang et al., 2022](https://www.arxiv.org/abs/2208.04369)) presents a weight similarity measure method and hypothesis-training-testing to quantify weight similarity in neural networks, providing insights into local solutions. "Gradient descent aligns the layers of deep linear networks" ([Ji and Telgarsky, 2018](https://www.arxiv.org/abs/1810.02032)) shows that gradient descent aligns layers in deep linear networks and establishes risk convergence and weight matrix alignment as implicit regularization. "On the security relevance of weights in deep learning" ([Grosse et al., 2019](https://www.arxiv.org/abs/1902.03020v2)) highlights the security relevance of initial weights, showing that task-independent weight permutations can significantly degrade model accuracy. "Characterizing the Weight Space for Different Learning Models" ([Musunuru et al., 2020](https://www.arxiv.org/abs/2006.02724v1)) attempts to characterize the weight space in terms of trained, generalized, and adversarial pattern sets for deep neural networks and Dense Associative Memory models. "Do Unlearning Methods Remove Information from Language Model Weights?" ([Deeb and Roger, 2024](https://www.arxiv.org/abs/2410.08827)) evaluates whether unlearning methods effectively remove information from language model weights using an adversarial evaluation method. "Stable Recovery of Entangled Weights: Towards Robust Identification of Deep Neural Networks from Minimal Samples" ([Fiedler et al., 2021](https://www.arxiv.org/abs/2101.07150)) addresses stable identifiability of deep neural networks from minimal samples, introducing entangled weights and proving their stable recovery. "Reverse-Engineering Deep ReLU Networks" ([Rolnick and Kording, 2019](https://www.arxiv.org/abs/1910.00744)) demonstrates the possibility of recovering the architecture, weights, and biases of deep ReLU networks by observing only their output. "Effect of Weight Quantization on Learning Models by Typical Case Analysis" ([Kashiwamura et al., 2024](https://www.arxiv.org/abs/2401.17269)) analyzes the effect of weight quantization hyperparameters using typical case analysis, revealing insights into stability, optimal width, and overfitting mitigation. "Finite Sample Identification of Wide Shallow Neural Networks with Biases" ([Fornasier et al., 2022](https://www.arxiv.org/abs/2211.04589)) provides constructive methods and theoretical guarantees for finite sample identification of wide shallow networks with biases. "Robust and Resource Efficient Identification of Two Hidden Layer Neural Networks" ([Fornasier et al., 2019](https://www.arxiv.org/abs/1907.00485)) addresses structure identification and uniform approximation of two-layer neural networks from few samples using Hessian approximations. "Understanding the Covariance Structure of Convolutional Filters" ([Trockman et al., 2022](https://www.arxiv.org/abs/2210.03651)) studies the covariance structure of convolutional filters, proposing a learning-free multivariate initialization scheme based on observed covariances. "Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced" ([Du et al., 2018](https://www.arxiv.org/abs/1806.00900)) studies implicit regularization by gradient descent for deep homogeneous models, showing automatic balancing of layer magnitudes. "Weight Distillation: Transferring the Knowledge in Neural Network Parameters" ([Lin et al., 2020](https://www.arxiv.org/abs/2009.09152v3)) proposes weight distillation to transfer knowledge from large to small networks through a parameter generator. "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" ([Frankle and Carbin, 2018](https://www.arxiv.org/abs/1803.03635)) articulates the lottery ticket hypothesis, finding sparse subnetworks ("winning tickets") within dense networks capable of comparable performance. "Optimizing Mode Connectivity via Neuron Alignment" ([Tatro et al., 2020](https://www.arxiv.org/abs/2009.02439)) proposes neuron alignment to optimize mode connectivity by accounting for weight permutations and promoting activation similarity.

**Hypernetworks and Weight Generation:** Hypernetworks offer a method to generate weights for other networks, enabling efficient parameterization and meta-learning. "HyperNetworks" ([Ha et al., 2016](https://www.arxiv.org/abs/1609.09106)) explores hypernetworks for generating weights for deep convolutional and recurrent networks, challenging weight-sharing paradigms. "HyperGAN: A Generative Model for Diverse, Performant Neural Networks" ([Ratzlaff and Fuxin, 2019](https://www.arxiv.org/abs/1901.11058)) introduces HyperGAN, a generative model for learning a distribution of neural network parameters, enabling the creation of diverse ensembles. "Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights" ([Sch\\\"urholt et al., 2022](https://www.arxiv.org/abs/2209.14733)) extends hyper-representations for generative use, sampling new model weights and demonstrating applications in initialization, ensemble sampling, and transfer learning. "Generating Neural Networks with Neural Networks" ([Deutsch, 2018](https://www.arxiv.org/abs/1801.01952)) formulates hypernetwork training as a compromise between accuracy and diversity, using MLPs for mapping random vectors to weight space. "Parameter Prediction for Unseen Deep Architectures" ([Knyazev et al., 2021](https://www.arxiv.org/abs/2110.13100)) explores parameter prediction for unseen architectures using graph neural networks and a large-scale dataset of neural architectures. "Meta-Learning via Classifier(-free) Diffusion Guidance" ([Nava et al., 2022](https://www.arxiv.org/abs/2210.08942)) introduces meta-learning algorithms using diffusion models to generate neural network weights adapted for unseen tasks in a zero-shot manner. "Diffusion-based Neural Network Weights Generation" ([Soro et al., 2024](https://www.arxiv.org/abs/2402.18153)) proposes a diffusion model to learn the distribution of pretrained weights conditioned on datasets, enabling adaptive weight sampling for transfer learning. "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning" ([Zhmoginov et al., 2022](https://www.arxiv.org/abs/2201.04182)) proposes HyperTransformer, a Transformer-based model for few-shot learning, generating CNN weights from support samples. "HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing" ([Alaluf et al., 2021](https://www.arxiv.org/abs/2111.15666)) introduces HyperStyle, a hypernetwork modulating StyleGAN weights for real image inversion and editing. "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models" ([Ruiz et al., 2023](https://www.arxiv.org/abs/2307.06949)) proposes HyperDreamBooth, a hypernetwork for fast personalization of text-to-image models, generating personalized weights from a single image. "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers" ([Mahabadi et al., 2021](https://www.arxiv.org/abs/2106.04647)) proposes Compacter, an efficient fine-tuning method using low-rank hypercomplex adapter layers for large language models.

**Other Weight Manipulation Techniques:** "Patching open-vocabulary models by interpolating weights" ([Ilharco et al., 2022](https://www.arxiv.org/abs/2208.05592)) introduces PAINT, a patching method that interpolates weights to improve accuracy on specific tasks for open-vocabulary models like CLIP. "Editing Models with Task Arithmetic" ([Ilharco et al., 2022](https://www.arxiv.org/abs/2212.04089)) proposes task arithmetic, using task vectors to steer model behavior by adding or negating weight differences from fine-tuning. "Weights Augmentation: it has never ever ever ever let her model down" ([Zhuang et al., 2024](https://www.arxiv.org/abs/2405.19590)) proposes Weight Augmentation Strategy (WAS), using random weight coefficient transformations during training to improve model robustness and accuracy. "Representing Model Weights with Language using Tree Experts" ([Horwitz et al., 2024](https://www.arxiv.org/abs/2410.13569)) explores representing model weights with language using Tree Experts, enabling learning and generalization from model weights. "Initializing Models with Larger Ones" ([Xu et al., 2023](https://www.arxiv.org/abs/2311.18823)) introduces weight selection, initializing smaller models by selecting weights from larger pretrained models for knowledge transfer and improved performance. "Domain Aggregation Networks for Multi-Source Domain Adaptation" ([Wen et al., 2019](https://www.arxiv.org/abs/1909.05352)) proposes Domain Aggregation Network (DARN) for multi-source domain adaptation, adjusting source domain weights during training. "Robust fine-tuning of zero-shot models" ([Wortsman et al., 2021](https://www.arxiv.org/abs/2109.01903)) introduces WiSE-FT, ensembling zero-shot and fine-tuned model weights to improve robustness under distribution shifts during fine-tuning. "The Super Weight in Large Language Models" ([Yu et al., 2024](https://www.arxiv.org/abs/2411.07191)) identifies "super weights" in LLMs, demonstrating that pruning even a single parameter can drastically degrade performance. "Sampling weights of deep neural networks" ([Bolager et al., 2023](https://www.arxiv.org/abs/2306.16830)) introduces a probability distribution and sampling algorithm for neural network weights, training networks without iterative optimization. "Variance Networks: When Expectation Does Not Meet Your Expectations" ([Neklyudov et al., 2018](https://www.arxiv.org/abs/1803.03764)) introduces variance layers, where weights follow zero-mean distributions parameterized by variance, exploring their learning capabilities and robustness. "Learning Transferable Visual Models From Natural Language Supervision" ([Radford et al., 2021](https://www.arxiv.org/abs/2103.00020)) demonstrates learning SOTA image representations from raw text supervision using a pre-training task of predicting image-caption pairings. "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping" ([Dodge et al., 2020](https://www.arxiv.org/abs/2002.06305)) examines factors in fine-tuning pretrained language models, including weight initialization and data order, quantifying their impact on performance variance. "A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks" ([Pan et al., 2022](https://www.arxiv.org/abs/2205.15307)) proposes a unified weight initialization paradigm for Tensorial CNNs, generalizing Xavier and Kaiming initialization methods. "Convolutional Neural Networks using Logarithmic Data Representation" ([Miyashita et al., 2016](https://www.arxiv.org/abs/1603.01025)) proposes logarithmic data representation for CNNs, enabling 3-bit encoding with negligible performance loss and eliminating multipliers. "Bitwise Neural Networks" ([Kim and Smaragdis, 2016](https://www.arxiv.org/abs/1601.06071)) proposes Bitwise Neural Networks (BNNs) with binary weights, biases, inputs, and outputs, using bit logic for efficient computation. "Predicting Parameters in Deep Learning" ([Denil et al., 2013](https://www.arxiv.org/abs/1306.0543)) demonstrates redundancy in deep learning model parameters, showing that many weights can be predicted or need not be learned. "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks" ([Saxe et al., 2013](https://www.arxiv.org/abs/1312.6120)) provides analytical solutions to learning dynamics in deep linear networks, revealing nonlinear learning phenomena. "Feature Purification: How Adversarial Training Performs Robust Deep Learning" ([Allen-Zhu and Li, 2020](https://www.arxiv.org/abs/2005.10190)) presents the Feature Purification principle, showing adversarial training aims to purify hidden weights by removing small dense mixtures. "Double-Weighting for Covariate Shift Adaptation" ([Segovia-Mart\\'in et al., 2023](https://www.arxiv.org/abs/2305.08637)) proposes a minimax risk classification (MRC) approach for covariate shift adaptation, weighting both training and testing samples. "BERT Busters: Outlier Dimensions that Disrupt Transformers" ([Kovaleva et al., 2021](https://www.arxiv.org/abs/2105.06990)) demonstrates the fragility of Transformers to removing outlier features in layer outputs, particularly in LayerNorm parameters. "LoRA: Low-Rank Adaptation of Large Language Models" ([Hu et al., 2021](https://www.arxiv.org/abs/2106.09685)) proposes Low-Rank Adaptation (LoRA), freezing pretrained model weights and injecting trainable rank decomposition matrices for efficient fine-tuning. "MyStyle: A Personalized Generative Prior" ([Nitzan et al., 2022](https://www.arxiv.org/abs/2203.17272)) introduces MyStyle, a personalized generative prior by tuning StyleGAN weights with few-shot person images for personalized image generation and editing. "Inserting Anybody in Diffusion Models via Celeb Basis" ([Yuan et al., 2023](https://www.arxiv.org/abs/2306.00926)) proposes a personalization method for diffusion models using a celeb basis, allowing seamless integration of individuals with minimal parameters and training data. "Understanding and Overcoming the Challenges of Efficient Transformer Quantization" ([Bondarenko et al., 2021](https://www.arxiv.org/abs/2109.12948)) explores quantization challenges in Transformers, proposing solutions like per-embedding-group quantization and quantization-aware training. "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving" ([Zhao et al., 2023](https://www.arxiv.org/abs/2310.19102)) introduces Atom, a low-bit quantization method for efficient LLM serving, using mixed-precision and fine-grained quantization to boost throughput. "The case for 4-bit precision: k-bit Inference Scaling Laws" ([Dettmers and Zettlemoyer, 2022](https://www.arxiv.org/abs/2212.09720)) studies inference scaling laws for quantized LLMs, finding 4-bit precision to be nearly universally optimal for zero-shot accuracy and model size. "OPT: Open Pre-trained Transformer Language Models" ([Zhang et al., 2022](https://www.arxiv.org/abs/2205.01068)) presents Open Pre-trained Transformers (OPT), a suite of open-source decoder-only transformers, comparable to GPT-3 and released for research purposes. "Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses" ([Baldassi et al., 2015](https://www.arxiv.org/abs/1509.05753)) shows that dense regions of solutions in networks with discrete synapses are accessible by simple learning protocols and offer robust generalization. "Learning may need only a few bits of synaptic precision" ([Baldassi et al., 2016](https://www.arxiv.org/abs/1602.04129)) suggests that increasing synaptic precision beyond a few bits provides diminishing returns for learning performance in neural networks.


---

<div id="chipdesign">  </div>


## Chip design

**Domain-Specific LLMs for IC Design:**

To address the specialized needs of IC design, domain-specific LLMs have been developed. **ChipExpert** ([https://www.arxiv.org/abs/2408.00804v1](https://www.arxiv.org/abs/2408.00804v1)), introduced by Xu et al., is an open-source instructional LLM tailored for IC design, trained on Llama-3 8B and evaluated on the new ChipICD-Bench benchmark. Similarly, **ChipNeMo** ([https://www.arxiv.org/abs/2311.00176](https://www.arxiv.org/abs/2311.00176)) by Liu et al. explores domain adaptation techniques for LLMs in chip design, including domain-adaptive tokenization and pretraining, demonstrating superior performance compared to base models in tasks like chatbot assistance and EDA script generation. Building on this, **ChipAlign** ([https://www.arxiv.org/abs/2412.19819](https://www.arxiv.org/abs/2412.19819)) by Deng et al. focuses on instruction alignment for chip LLMs using geodesic interpolation to merge general and chip-specific models, improving instruction-following capabilities.

**HDL Generation and Synthesis:**

A significant area of focus is using LLMs for Hardware Description Language (HDL) generation. **SpecLLM** ([https://www.arxiv.org/abs/2401.13266](https://www.arxiv.org/abs/2401.13266)), proposed by Li et al., explores LLMs for generating and reviewing VLSI design specifications, showing promising results in automating this initial design stage. **BetterV** ([https://www.arxiv.org/abs/2402.03375](https://www.arxiv.org/abs/2402.03375)) by Pei et al. introduces a Verilog generation framework that fine-tunes LLMs and uses discriminative guidance for specific design demands, outperforming GPT-4 on benchmarks. **VeriGen** ([https://www.arxiv.org/abs/2308.00708](https://www.arxiv.org/abs/2308.00708)) by Thakur et al. also investigates fine-tuning LLMs for Verilog generation, demonstrating competitive performance against GPT-3.5-turbo.  **Make Every Move Count** ([https://www.arxiv.org/abs/2402.03289](https://www.arxiv.org/abs/2402.03289)) by DeLorenzo et al. presents an algorithm integrating Monte Carlo tree-search with transformer decoding to generate compilable and PPA-optimized RTL code. **Leveraging High-Level Synthesis** ([https://www.arxiv.org/abs/2311.03489v5](https://www.arxiv.org/abs/2311.03489v5)) by Meech demonstrates a methodology using LLMs and high-level synthesis to generate hardware designs with open-source tools. **DAVE** ([https://www.arxiv.org/abs/2009.01026](https://www.arxiv.org/abs/2009.01026)), a relatively earlier work by Pearce et al., explores fine-tuning GPT-2 to derive Verilog from English descriptions. **ChipGPT** ([https://www.arxiv.org/abs/2305.14019](https://www.arxiv.org/abs/2305.14019)) by Chang et al. presents a zero-code logic design framework based on LLMs to generate Verilog programs from natural language specifications.  **Towards LLM-Powered Verilog RTL Assistant** ([https://www.arxiv.org/abs/2406.00115v1](https://www.arxiv.org/abs/2406.00115v1)) by Huang et al. introduces VeriAssist, an LLM-powered assistant with self-verification and self-correction capabilities for RTL code generation.

**EDA Tool Automation and Intelligent Agents:**

Beyond HDL generation, LLMs are being explored for automating EDA tools and workflows. **IICPilot** ([https://www.arxiv.org/abs/2407.12576](https://www.arxiv.org/abs/2407.12576)) by Jiang et al. introduces an intelligent IC backend design framework using open-source EDA, automating script generation and design space exploration. **ChatEDA** ([https://www.arxiv.org/abs/2308.10204](https://www.arxiv.org/abs/2308.10204)) by He et al. presents an autonomous agent powered by LLMs for managing task planning and script generation in the RTL-to-GDSII flow. **LayoutCopilot** ([https://www.arxiv.org/abs/2406.18873](https://www.arxiv.org/abs/2406.18873)) by Liu et al. proposes an LLM-powered multi-agent framework for interactive analog layout design, simplifying human-tool interaction. **LLM-Enhanced Bayesian Optimization** ([https://www.arxiv.org/abs/2406.05250](https://www.arxiv.org/abs/2406.05250)), named LLANA and developed by Chen et al., leverages LLMs to enhance Bayesian Optimization for efficient analog layout constraint generation. **ChatPattern** ([https://www.arxiv.org/abs/2403.15434](https://www.arxiv.org/abs/2403.15434)) by Wang et al. introduces an LLM-powered framework for flexible layout pattern customization through natural language.

**Analog and Mixed-Signal (AMS) Circuit Design:**

The application of LLMs is also extending to analog and mixed-signal circuit design. **LaMAGIC** ([https://www.arxiv.org/abs/2407.18269](https://www.arxiv.org/abs/2407.18269)) by Chang et al. introduces a language model-based topology generation model for analog ICs, achieving high success rates in generating optimized circuit designs. **AnalogCoder** ([https://www.arxiv.org/abs/2405.14918v2](https://www.arxiv.org/abs/2405.14918v2)) by Lai et al. presents a training-free LLM agent for analog circuit design through Python code generation, incorporating a feedback-enhanced flow and a circuit tool library. **Masala-CHAI** ([https://www.arxiv.org/abs/2411.14299](https://www.arxiv.org/abs/2411.14299)) by Bhandari et al. is an automated framework leveraging LLMs to generate SPICE netlists for analog circuits, addressing a long-standing challenge in this domain. **AMSNet** ([https://www.arxiv.org/abs/2405.09045](https://www.arxiv.org/abs/2405.09045)) by Tao et al. introduces a netlist dataset for AMS circuits, facilitating the exploration of MLLMs in AMS circuit design.

**Verification, Debugging, and Testing:**

LLMs are also being investigated for hardware verification and debugging. **HDLdebugger** ([https://www.arxiv.org/abs/2403.11671](https://www.arxiv.org/abs/2403.11671)) by Yao et al. proposes an LLM-assisted framework for HDL debugging, outperforming cutting-edge LLMs on HDL code datasets. **AutoChip** ([https://www.arxiv.org/abs/2311.04887](https://www.arxiv.org/abs/2311.04887)) by Thakur et al. combines LLM capabilities with Verilog simulation feedback to generate accurate Verilog modules. **Using LLMs to Facilitate Formal Verification of RTL** ([https://www.arxiv.org/abs/2309.09437](https://www.arxiv.org/abs/2309.09437)) by Orenes-Vera et al. explores using GPT4 to generate SystemVerilog Assertions (SVA) for formal property verification, demonstrating its ability to create correct SVA and uncover bugs. **AutoBench** ([https://www.arxiv.org/abs/2407.03891](https://www.arxiv.org/abs/2407.03891)) by Qiu et al. introduces an LLM-based testbench generator and evaluation framework for HDL designs, significantly improving testbench generation pass rates. **Evaluating LLMs for Hardware Design and Test** ([https://www.arxiv.org/abs/2405.02326](https://www.arxiv.org/abs/2405.02326)) by Blocklove et al. evaluates the capabilities of LLMs for both designing and testing hardware modules, highlighting their potential in automating the digital design pipeline.

**Datasets and Benchmarks for LLM-EDA:**

To facilitate research and evaluation, several datasets and benchmarks have been introduced. **RTLLM** ([https://www.arxiv.org/abs/2308.05345](https://www.arxiv.org/abs/2308.05345)) by Lu et al. is an open-source benchmark for evaluating design RTL generation with LLMs, proposing progressive goals for evaluation. **VerilogEval** ([https://www.arxiv.org/abs/2309.07544](https://www.arxiv.org/abs/2309.07544)) by Liu et al. presents a benchmarking framework with 156 problems from HDLBits for evaluating LLM performance in Verilog code generation. **ChipICD-Bench** ([https://www.arxiv.org/abs/2408.00804v1](https://www.arxiv.org/abs/2408.00804v1)), as mentioned earlier with ChipExpert, is a new benchmark for evaluating LLMs in IC design sub-domains. **AMSNet** ([https://www.arxiv.org/abs/2405.09045](https://www.arxiv.org/abs/2405.09045)) also serves as a dataset for AMS circuits, bridging the gap for MLLM applications in this area. **Masala-CHAI** ([https://www.arxiv.org/abs/2411.14299](https://www.arxiv.org/abs/2411.14299)) contributes a large-scale SPICE netlist dataset for analog circuits.

**Surveys and Future Directions:**

Several papers also provide broader perspectives and discuss future directions. **Digital ASIC Design with Ongoing LLMs** ([https://www.arxiv.org/abs/2405.02329v1](https://www.arxiv.org/abs/2405.02329v1)) by Xiang et al. outlines strategies and prospects for using LLMs in digital ASIC design, addressing challenges and demonstrating a successful PWM generator design. **LLM4EDA** ([https://www.arxiv.org/abs/2401.12224](https://www.arxiv.org/abs/2401.12224)) by Zhong et al. provides a systematic study on the applications of LLMs in EDA, categorizing them into chatbot assistants, HDL generation, and verification, and highlighting future research directions. **LLMs and the Future of Chip Design** ([https://www.arxiv.org/abs/2405.07061](https://www.arxiv.org/abs/2405.07061)) by Wang et al. reviews the surge of LLMs in chip design, raising security and trustworthiness concerns and discussing both attack and defense perspectives. **LLM-Aided Efficient Hardware Design Automation** ([https://www.arxiv.org/abs/2410.18582](https://www.arxiv.org/abs/2410.18582)) by Xu et al. discusses the possibilities of integrating LLMs into various EDA aspects, highlighting case studies and future challenges. **GPT4AIGChip** ([https://www.arxiv.org/abs/2309.10730](https://www.arxiv.org/abs/2309.10730)) by Fu et al. explores the use of LLMs for AI accelerator design automation, presenting a framework called GPT4AIGChip for democratizing AI accelerator design. **Chip-Chat** ([https://www.arxiv.org/abs/2305.13243](https://www.arxiv.org/abs/2305.13243)) by Blocklove et al. explores the challenges and opportunities of conversational hardware design with LLMs, showcasing a case study of co-architecting a microprocessor with an LLM.

**Data Augmentation and Fine-tuning:**

**Data is all you need** ([https://www.arxiv.org/abs/2403.11202](https://www.arxiv.org/abs/2403.11202)) by Chang et al. proposes an automated design-data augmentation framework to generate high-quality training data for fine-tuning LLMs for chip design, demonstrating significant improvements in Verilog generation accuracy.




<div align="center">  
</div>

---

## Citation


How to cite my work?



```
@misc{MaattaAutonomousAgents2023,
  author = {Teemu Maatta},
  title = {Autonomous Agents},
  year = {2023},
  howpublished = {\url{https://github.com/tmgthb/Autonomous-Agents}},
  note = {Accessed: YYYY-MM-DD}
}

```

---

[Back to top](#topofthepage)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Ftmgthb%2FAutonomous-Agents&count_bg=%23F2C027&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Views&edge_flat=true)](https://github.com/tmgthb/Autonomous-Agents)
